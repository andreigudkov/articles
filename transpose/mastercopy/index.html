<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.12">
<title>What it takes to transpose a matrix</title>
<style>
/*! normalize.css v2.1.2 | MIT License | git.io/normalize */article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}audio,canvas,video{display:inline-block}audio:not([controls]){display:none;height:0}[hidden],template{display:none}script{display:none !important}html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}a{background:transparent}a:focus{outline:thin dotted}a:active,a:hover{outline:0}h1{font-size:2em;margin:.67em 0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}mark{background:#ff0;color:#000}code,kbd,pre,samp{font-family:monospace,serif;font-size:1em}pre{white-space:pre-wrap}q{quotes:"\201C" "\201D" "\2018" "\2019"}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}svg:not(:root){overflow:hidden}figure{margin:0}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{border:0;padding:0}button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}button,input{line-height:normal}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="search"]{-webkit-appearance:textfield;-moz-box-sizing:content-box;-webkit-box-sizing:content-box;box-sizing:content-box}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}textarea{overflow:auto;vertical-align:top}table{border-collapse:collapse;border-spacing:0}*,*::before,*::after{box-sizing:border-box;margin:0}body{font-family:Helvetica,Arial,sans-serif;font-size:16px;color:#222;line-height:1.5;max-width:55em;margin:0 auto}#content,#footnotes{padding-left:.5em;padding-right:.5em}strong{font-weight:bold}em{font-style:italic}:not(pre)>code{font-family:Courier,monospace;line-height:1.0}a{color:#0061c5;text-decoration:none}a:hover{text-decoration:underline}hr{border-width:0 0 1px 0;border-style:solid;border-color:#678}ul,ol{list-style-position:outside;padding-left:0;margin-left:2em}ul li ul,ul li ol,ol li ul,ol li ol{margin-left:1.414em}ul>li{list-style-type:square;font-size:80%}ul>li>*{font-size:125%}ol>li{font-weight:bold}ol>li>*{font-weight:normal}ol.arabic{list-style-type:decimal}ol.decimal{list-style-type:decimal-leading-zero}ol.loweralpha{list-style-type:lower-alpha}ol.upperalpha{list-style-type:upper-alpha}ol.lowerroman{list-style-type:lower-roman}ol.upperroman{list-style-type:upper-roman}ol.lowergreek{list-style-type:lower-greek}.dlist dt{color:#325d72;font-weight:bold}.dlist dt:not(:first-child){margin-top:1em}.dlist dd{margin-left:2em}td.hdlist1{color:#325d72;padding-right:.5em;vertical-align:top}td.hdlist2{padding-bottom:.5em}h1{font-size:28px;font-weight:normal;letter-spacing:-1px;color:white;background-color:#325d72;text-align:center;margin:0 0 .5em 0;padding:.05em .5em}@media print{h1{color:#325d72;background-color:white;font-weight:bold}}h1::after{content:':';width:0;overflow:hidden;display:inline-block;vertical-align:middle}.author{color:#325d72}.email::before{content:"<";color:#325d72}.email::after{content:">";color:#325d72}.author+br,.email+br{display:none}#author{padding-left:.5em}#toc{margin:1em 0 2em 0;padding-left:.5em}#toctitle{font-size:19px;font-weight:bold;color:#325d72;margin:.5em 0}#toc>ul{line-height:1.4;font-size:15px;margin:0 0 0 .5em}#toc ul li{list-style-type:none}#toc li{margin:0}.big{font-size:120%}.small{font-size:75%}.underline{text-decoration:underline}.overline{text-decoration:overline}.line-through{text-decoration:line-through}.aqua{color:#00bfbf}.aqua-background{background-color:#00fafa;border-radius:2px;padding:0 3px}.black{color:"black"}.black-background{background-color:"black";border-radius:2px;padding:0 3px}.blue{color:#0000bf}.blue-background{background-color:#0000fa;border-radius:2px;padding:0 3px}.fuchsia{color:#bf00bf}.fuchsia-background{background-color:#fa00fa;border-radius:2px;padding:0 3px}.gray{color:#606060}.gray-background{background-color:#7d7d7d;border-radius:2px;padding:0 3px}.green{color:#006000}.green-background{background-color:#007d00;border-radius:2px;padding:0 3px}.lime{color:#00bf00}.lime-background{background-color:#00fa00;border-radius:2px;padding:0 3px}.maroon{color:#600000}.maroon-background{background-color:#7d0000;border-radius:2px;padding:0 3px}.navy{color:#000060}.navy-background{background-color:#00007d;border-radius:2px;padding:0 3px}.olive{color:#606000}.olive-background{background-color:#7d7d00;border-radius:2px;padding:0 3px}.purple{color:#600060}.purple-background{background-color:#7d007d;border-radius:2px;padding:0 3px}.red{color:#bf0000}.red-background{background-color:#fa0000;border-radius:2px;padding:0 3px}.silver{color:#909090}.silver-background{background-color:#bcbcbc;border-radius:2px;padding:0 3px}.teal{color:#006060}.teal-background{background-color:#007d7d;border-radius:2px;padding:0 3px}.white{color:#bfbfbf}.white-background{background-color:#fafafa;border-radius:2px;padding:0 3px}.yellow{color:#bfbf00}.yellow-background{background-color:#fafa00;border-radius:2px;padding:0 3px}table.tableblock{border:1px solid #91a7b3;margin-left:auto;margin-right:auto}table.tableblock>caption.title{text-align:left;margin-bottom:.5em}table.tableblock>colgroup>col{width:inherit !important}table.tableblock>tbody>tr>td{border-style:solid;border-color:#91a7b3;border-width:0 1px;padding:0 5px 2px 5px}table.tableblock>tbody>tr:nth-of-type(2n){background-color:#f8f8f8}p.tableblock{text-align:inherit}table.tableblock>thead>tr>td,table.tableblock>thead>tr>th,table.tableblock>tfoot>tr>td,table.tableblock>tfoot>tr>th{color:#325d72;font-weight:bold;line-height:1.35;padding:2px 5px;border:1px solid #91a7b3}table.tableblock>thead>tr>th,table.tableblock>thead>tr>td{border-bottom-width:2px}table.tableblock>tfoot>tr>th,table.tableblock>tfoot>tr>td{border-top-width:2px}th.halign-left,td.halign-left{text-align:left}th.halign-right,td.halign-right{text-align:right}th.halign-center,td.halign-center{text-align:center}th.valign-top,td.valign-top{vertical-align:top}th.valign-bottom,td.valign-bottom{vertical-align:bottom}th.valign-middle,td.valign-middle{vertical-align:middle}div.listingblock{padding:.25em;background-color:#f8f8f8;overflow:auto}div.listingblock .title{text-align:right}div.listingblock pre{font-family:Menlo,Consolas,Monaco,"Lucida Console",monospace;font-size:14px;white-space:pre;background-color:#f8f8f8 !important;margin:0}div.listingblock td.linenos{border-right:2px solid #91a7b3}div.listingblock td.code{padding-left:.25em}div.imageblock>div.content>img{max-width:98%}.text-indent{padding-left:2em}img.inlinemath{image-rendering:optimizequality;margin-top:.5ex}h2,h3,h4{font-weight:normal;color:#325d72;margin:0}h2{font-size:27px;letter-spacing:-1px;border-bottom:1px solid #91a7b3}h3{font-size:24px;letter-spacing:-0.75px}h4{font-size:21px;letter-spacing:-0.5px}.title{color:#325d72;font-weight:bold}#footer{font-size:80%;color:white;background-color:#325d72}@media print{#footer{color:#325d72;background-color:white;font-weight:bold}}#footer-text{text-align:center;padding:.5em}#footer-badges{display:none}span.footnote{vertical-align:super;font-size:80%}#footnotes>hr{display:none}#footnotes::before{display:block;border-bottom:1px solid #678;margin:.5em 0;content:"Notes";font-size:19px;font-weight:bold;color:#325d72}#footnotes .footnote{margin-left:.5em;font-size:15px}hr:not(:first-child){margin-top:1.5em}hr:not(:last-child){margin-bottom:1.5em}.imageblock:not(:last-child),.listingblock:not(:last-child),.tableblock:not(:last-child){margin-bottom:1em}p+*{margin-top:1em}.paragraph+*{margin-top:1em}p+.ulist,p+.olist,p+.dlist,p+.hdlist,.paragraph+.ulist,.paragraph+.olist,.paragraph+.dlist,.paragraph+.hdlist,.paragraph+.listingblock{margin-top:.5em !important}li *+.ulist,li *+.olist,li *+.dlist,li *+.hdlist{margin-top:.1em !important}.title:not(:first-child){margin-top:1.5em}.content+.title{margin-top:.5em !important}.title+*{margin-top:1.5em}.title+p,.title+.paragraph,.title+.ulist,.title+.olist,.title+.dlist,.title+.hdlist{margin-top:.5em !important}.ulist:not(:last-child){margin-bottom:1em}.olist:not(:last-child){margin-bottom:1em}li:not(:first-child){margin-top:.1em}.dlist:not(:last-child){margin-bottom:1em}.dlist:not(:first-child){margin-top:1em}.sect3:not(:last-child){margin-bottom:18px}.sect3:not(:first-child){margin-top:18px}h4:not(:last-child){margin-bottom:9px}.sect2:not(:last-child){margin-bottom:22px}.sect2:not(:first-child){margin-top:22px}h3:not(:last-child){margin-bottom:11px}.sect1:not(:last-child){margin-bottom:40px}.sect1:not(:first-child){margin-top:40px}#preamble:not(:last-child){margin-bottom:40px}h2:not(:last-child){margin-bottom:13px}#header:not(:last-child),#content:not(:last-child),#footnotes:not(:last-child){margin-bottom:2em}.text-left{text-align:left}.text-center{text-align:center}.text-right{text-align:right}
</style>
<style>
pre { line-height: 125%; margin: 0; }
td.linenos pre { color: #000000; background-color: #f0f0f0; padding: 0 5px 0 5px; }
span.linenos { color: #000000; background-color: #f0f0f0; padding: 0 5px 0 5px; }
td.linenos pre.special { color: #000000; background-color: #ffffc0; padding: 0 5px 0 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding: 0 5px 0 5px; }
pre.pygments .hll { background-color: #ffffcc }
pre.pygments { background: #f8f8f8; }
pre.pygments .tok-c { color: #408080; font-style: italic } /* Comment */
pre.pygments .tok-err { border: 1px solid #FF0000 } /* Error */
pre.pygments .tok-k { color: #008000; font-weight: bold } /* Keyword */
pre.pygments .tok-o { color: #666666 } /* Operator */
pre.pygments .tok-ch { color: #408080; font-style: italic } /* Comment.Hashbang */
pre.pygments .tok-cm { color: #408080; font-style: italic } /* Comment.Multiline */
pre.pygments .tok-cp { color: #BC7A00 } /* Comment.Preproc */
pre.pygments .tok-cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
pre.pygments .tok-c1 { color: #408080; font-style: italic } /* Comment.Single */
pre.pygments .tok-cs { color: #408080; font-style: italic } /* Comment.Special */
pre.pygments .tok-gd { color: #A00000 } /* Generic.Deleted */
pre.pygments .tok-ge { font-style: italic } /* Generic.Emph */
pre.pygments .tok-gr { color: #FF0000 } /* Generic.Error */
pre.pygments .tok-gh { color: #000080; font-weight: bold } /* Generic.Heading */
pre.pygments .tok-gi { color: #00A000 } /* Generic.Inserted */
pre.pygments .tok-go { color: #888888 } /* Generic.Output */
pre.pygments .tok-gp { color: #000080; font-weight: bold } /* Generic.Prompt */
pre.pygments .tok-gs { font-weight: bold } /* Generic.Strong */
pre.pygments .tok-gu { color: #800080; font-weight: bold } /* Generic.Subheading */
pre.pygments .tok-gt { color: #0044DD } /* Generic.Traceback */
pre.pygments .tok-kc { color: #008000; font-weight: bold } /* Keyword.Constant */
pre.pygments .tok-kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
pre.pygments .tok-kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
pre.pygments .tok-kp { color: #008000 } /* Keyword.Pseudo */
pre.pygments .tok-kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
pre.pygments .tok-kt { color: #B00040 } /* Keyword.Type */
pre.pygments .tok-m { color: #666666 } /* Literal.Number */
pre.pygments .tok-s { color: #BA2121 } /* Literal.String */
pre.pygments .tok-na { color: #7D9029 } /* Name.Attribute */
pre.pygments .tok-nb { color: #008000 } /* Name.Builtin */
pre.pygments .tok-nc { color: #0000FF; font-weight: bold } /* Name.Class */
pre.pygments .tok-no { color: #880000 } /* Name.Constant */
pre.pygments .tok-nd { color: #AA22FF } /* Name.Decorator */
pre.pygments .tok-ni { color: #999999; font-weight: bold } /* Name.Entity */
pre.pygments .tok-ne { color: #D2413A; font-weight: bold } /* Name.Exception */
pre.pygments .tok-nf { color: #0000FF } /* Name.Function */
pre.pygments .tok-nl { color: #A0A000 } /* Name.Label */
pre.pygments .tok-nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
pre.pygments .tok-nt { color: #008000; font-weight: bold } /* Name.Tag */
pre.pygments .tok-nv { color: #19177C } /* Name.Variable */
pre.pygments .tok-ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
pre.pygments .tok-w { color: #bbbbbb } /* Text.Whitespace */
pre.pygments .tok-mb { color: #666666 } /* Literal.Number.Bin */
pre.pygments .tok-mf { color: #666666 } /* Literal.Number.Float */
pre.pygments .tok-mh { color: #666666 } /* Literal.Number.Hex */
pre.pygments .tok-mi { color: #666666 } /* Literal.Number.Integer */
pre.pygments .tok-mo { color: #666666 } /* Literal.Number.Oct */
pre.pygments .tok-sa { color: #BA2121 } /* Literal.String.Affix */
pre.pygments .tok-sb { color: #BA2121 } /* Literal.String.Backtick */
pre.pygments .tok-sc { color: #BA2121 } /* Literal.String.Char */
pre.pygments .tok-dl { color: #BA2121 } /* Literal.String.Delimiter */
pre.pygments .tok-sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
pre.pygments .tok-s2 { color: #BA2121 } /* Literal.String.Double */
pre.pygments .tok-se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
pre.pygments .tok-sh { color: #BA2121 } /* Literal.String.Heredoc */
pre.pygments .tok-si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
pre.pygments .tok-sx { color: #008000 } /* Literal.String.Other */
pre.pygments .tok-sr { color: #BB6688 } /* Literal.String.Regex */
pre.pygments .tok-s1 { color: #BA2121 } /* Literal.String.Single */
pre.pygments .tok-ss { color: #19177C } /* Literal.String.Symbol */
pre.pygments .tok-bp { color: #008000 } /* Name.Builtin.Pseudo */
pre.pygments .tok-fm { color: #0000FF } /* Name.Function.Magic */
pre.pygments .tok-vc { color: #19177C } /* Name.Variable.Class */
pre.pygments .tok-vg { color: #19177C } /* Name.Variable.Global */
pre.pygments .tok-vi { color: #19177C } /* Name.Variable.Instance */
pre.pygments .tok-vm { color: #19177C } /* Name.Variable.Magic */
pre.pygments .tok-il { color: #666666 } /* Literal.Number.Integer.Long */
</style>
</head>
<body class="article">
<div id="header">
<h1>What it takes to transpose a matrix</h1>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_introduction">Introduction</a></li>
<li><a href="#_setting">Setting</a></li>
<li><a href="#_naive_implementation_3_90">Naive implementation [3.90]</a>
<ul class="sectlevel2">
<li><a href="#_read_stream">Read stream</a></li>
<li><a href="#_write_stream">Write stream</a></li>
<li><a href="#_combining_read_and_write_streams_together">Combining read and write streams together</a></li>
</ul>
</li>
<li><a href="#_dealing_with_cache_aliasing">Dealing with cache aliasing</a></li>
<li><a href="#_reversing_the_order_2_61">Reversing the order [2.61]</a></li>
<li><a href="#_exploring_block_structure_1_46">Exploring block structure [1.46]</a></li>
<li><a href="#_software_prefetching_1_35">Software prefetching [1.35]</a></li>
<li><a href="#_64_bit_simd_0_74">64-bit SIMD [0.74]</a></li>
<li><a href="#_256_bit_simd_0_49">256-bit SIMD [0.49]</a></li>
<li><a href="#_buffering_output_0_35">Buffering output [0.35]</a></li>
<li><a href="#_conclusion">Conclusion</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Andrei Gudkov &lt;<a href="mailto:gudokk@gmail.com">gudokk@gmail.com</a>&gt;</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_introduction">Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Classical CPU architecture is a poor choice for performing matrix-oriented computations.
Developers have to spend much effort to create efficient algorithms even for the problems appearing trivial on the surface.
Matrix transpose is probably the most striking representative of such problems.
It exposes numerous performance-related issues, such as high memory latency,
shortcomings of cache organization, and failure of the compiler to automatically vectorize the code.
Performance of more mathematically complex matrix operations is usually bounded by slow math instructions
(such as <em>div</em> or <em>sqrt</em>), which mask the majority of other issues.
Matrix transpose, on the other hand, being stripped of any slow instructions,
is open to all kinds of inefficiencies.
In this article we are going to gradually build a sequence of progressively more efficient implementations of matrix transpose,
with the most sophisticated implementation being up to x25 times faster than the naive one.
During each step we will locate the bottleneck, figure out what has caused it, and think of a solution to overcome it.
This article is intended to serve as an introduction to optimizing matrix algorithms for x86_64,
presented from the perspective of a real-world problem.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_setting">Setting</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We will work with the following formal problem statement: given NxN <code>src</code> matrix consisting of 1-byte elements,
and <code>dst</code> matrix of the same shape (non-overlapping with <code>src</code>), copy each <code>src[r,c]</code> element into <code>dst[c,r]</code>.
Below is the straightforward C++ prototype:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><table class="linenotable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20</pre></div></td><td class="code"><pre><span></span><span class="tok-k">class</span> <span class="tok-nc">Mat</span> <span class="tok-p">{</span>
<span class="tok-k">public</span><span class="tok-o">:</span>
  <span class="tok-kt">int64_t</span> <span class="tok-n">n</span><span class="tok-p">()</span> <span class="tok-k">const</span> <span class="tok-p">{</span> <span class="tok-k">return</span> <span class="tok-n">_n</span><span class="tok-p">;</span> <span class="tok-p">}</span>

  <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">data</span><span class="tok-p">()</span> <span class="tok-k">const</span> <span class="tok-p">{</span> <span class="tok-k">return</span> <span class="tok-n">_data</span><span class="tok-p">;</span> <span class="tok-p">}</span>
  <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">data</span><span class="tok-p">()</span> <span class="tok-p">{</span> <span class="tok-k">return</span> <span class="tok-n">_data</span><span class="tok-p">;</span> <span class="tok-p">}</span>

  <span class="tok-kt">uint8_t</span> <span class="tok-n">at</span><span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">row</span><span class="tok-p">,</span> <span class="tok-kt">int64_t</span> <span class="tok-n">col</span><span class="tok-p">)</span> <span class="tok-k">const</span> <span class="tok-p">{</span> <span class="tok-k">return</span> <span class="tok-n">_data</span><span class="tok-p">[</span><span class="tok-n">row</span> <span class="tok-o">*</span> <span class="tok-n">_n</span> <span class="tok-o">+</span> <span class="tok-n">col</span><span class="tok-p">];</span> <span class="tok-p">}</span>
  <span class="tok-kt">uint8_t</span><span class="tok-o">&amp;</span> <span class="tok-n">at</span><span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">row</span><span class="tok-p">,</span> <span class="tok-kt">int64_t</span> <span class="tok-n">col</span><span class="tok-p">)</span> <span class="tok-p">{</span> <span class="tok-k">return</span> <span class="tok-n">_data</span><span class="tok-p">[</span><span class="tok-n">row</span> <span class="tok-o">*</span> <span class="tok-n">_n</span> <span class="tok-o">+</span> <span class="tok-n">col</span><span class="tok-p">];</span> <span class="tok-p">}</span>

  <span class="tok-cm">/* ... */</span>

<span class="tok-k">private</span><span class="tok-o">:</span>
  <span class="tok-kt">int64_t</span> <span class="tok-n">_n</span><span class="tok-p">;</span> <span class="tok-c1">// matrix size</span>
  <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">_data</span><span class="tok-p">;</span> <span class="tok-c1">// array of size n*n</span>
<span class="tok-p">};</span>

<span class="tok-kt">void</span> <span class="tok-nf">transpose</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-c1">// copy each src[row,col] to dst[col,row]</span>
<span class="tok-p">}</span>
</pre></td></tr></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>Here you can see that I imposed some restrictions:
matrices are of square shape, they are filled with one-byte elements,
and transpose happens out-of-place.
The most important restriction is that element size is one byte long.
The reason behind it is that it will make things more challenging than with larger element sizes.
Increasing element size to, let&#8217;s say, <code>int64_t</code> will create more pressure on memory throughput,
making it primary bottleneck and therefore hiding other issues.
In general, all imposed restrictions can be lifted without loss in performance.
However, to create a truly "universal" transposer would require too much
boilerplate code not fitted for the purpose of this article.</p>
</div>
<div class="paragraph">
<p>Numbers in square brackets after section names will indicate performance of the corresponding algorithm,
expressed as average number of CPU cycles per single matrix element.
They were measured for a matrix of size 2112x2112, selected to represent medium size.
For larger matrices the difference in algorithm performance will be even more prominent.
The hardware used for testing is Skylake 7700HQ CPU with Turbo Boost off.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_naive_implementation_3_90">Naive implementation [3.90]</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Naive implementation is trivial and can be coded in less than a minute.
Just scan <code>src</code> row by row, column by column, and write elements to the corresponding location in <code>dst</code> matrix.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><table class="linenotable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8</pre></div></td><td class="code"><pre><span></span><span class="tok-kt">void</span> <span class="tok-nf">transpose_Naive</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">n</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">n</span><span class="tok-p">();</span>
  <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">r</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">r</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-p">;</span> <span class="tok-n">r</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
    <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">c</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">c</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-p">;</span> <span class="tok-n">c</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
      <span class="tok-n">dst</span><span class="tok-o">-&gt;</span><span class="tok-n">data</span><span class="tok-p">()[</span><span class="tok-n">n</span> <span class="tok-o">*</span> <span class="tok-n">c</span> <span class="tok-o">+</span> <span class="tok-n">r</span><span class="tok-p">]</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">data</span><span class="tok-p">()[</span><span class="tok-n">n</span> <span class="tok-o">*</span> <span class="tok-n">r</span> <span class="tok-o">+</span> <span class="tok-n">c</span><span class="tok-p">];</span>
    <span class="tok-p">}</span>
  <span class="tok-p">}</span>
<span class="tok-p">}</span>
</pre></td></tr></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>Below animation demonstrates the progress of the above algorithm being applied to a small 22x22 matrix.
Color denotes status of the element: already processed (<strong class="green">green</strong>), current (<strong class="blue">blue</strong>),
or not yet processed (<strong class="red">red</strong>).
Fading effect encodes which elements are currently in cache and their age since they were last accessed.
In this toy simulation cache size was set to 10 entries with last-recently-used (LRU) eviction policy,
where each cache line is 8 bytes long.
Real x86_64 systems have caches of thousands of 64-byte long cache lines.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="srcwise.gif" alt="srcwise">
</div>
</div>
<div class="paragraph">
<p>The implementation we currently have is the simplest possible and, as it usually happens, also the slowest one.
We will use it as a baseline to compare more advanced implementations against it.
But before moving forward, we would like to figure out the source of its inefficiency.
To do it, let&#8217;s trace mentally the function step by step and look at what happens at hardware level.
We are primarily interested in the memory subsystem.
Since there are two logical streams of data&#8201;&#8212;&#8201;read from <code>src</code> and write to <code>dst</code>&#8201;&#8212;&#8201;we will analyze them separately.</p>
</div>
<div class="sect2">
<h3 id="_read_stream">Read stream</h3>
<div class="paragraph">
<p>There are NxN reads in total.
On a hardware level, each such read is handled by <code>MOV *addr &#8594; reg</code> instruction.
Unlike arithmetic instructions, which have somewhat fixed latencies, latency of a <em>standalone</em> MOV instruction
varies widely in the range approximately between 4 and 300 cycles depending on where requested data is located.
If we are lucky and data is found in L1d cache, then MOV is nearly instant (4-5 cycles).
Cache hits to deeper levels are progressively <a href="https://www.7-cpu.com/cpu/Skylake.html">slower</a>: 12-15 cycles for L2, and 40-45 cycles for L3.
Cache miss to L3 causes MOV to experience the latency of RAM access, which costs up to 300 cycles.
The exact cost of accessing RAM is also not fixed but depends on a variety of factors including RAM clock frequency, RAM timings,
location of the requested data within complicated RAM geometry, current RAM state, and contention caused
by other CPU cores.
The following drawing roughly approximates the cost of accessing different layers of memory subsystem versus their sizes.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="cache.png" alt="cache" width="70%">
</div>
</div>
<div class="paragraph">
<p>In short, the variation in MOV latency is huge.
Fortunately for us, our read pattern is so simple, that we can easily guess where data comes from at every iteration.
Since our matrix organization is row-major and we also scan it row-major,
the loop simply walks linearly along addresses from <code>src.data()+0</code> to <code>src.data()+n*n</code>.
When the very first element of the very first row of the <code>src</code> matrix is accessed, it surely causes full memory access
because this element is not cached anywhere yet.
In terms of latency this access is extremely expensive: let&#8217;s assume 300 cycles.
However, to load one byte means actually to load 64 consecutive bytes at once because unit of cache operation
is 64B-long 64B-aligned <em>cache line</em>.
We humans think of a memory as an array of one-byte elements, but memory subsystem treats it as an array of 64B-long cache lines.
For example, when a MOV instruction is issued to read byte at address 3611<sub>10</sub>, then actually the whole sequence of bytes in range
3584<sub>10</sub> .. 3647<sub>10</sub> is read and brought into cache simultaneously.
To simplify explanations and without loss of generality, we can assume that beginning of <code>src</code> is aligned by 64.
In this case access to the (r=0,c=0) element causes the entire sequence of (r=0,c=0)..(r=0,c=63) elements to be brought into L1d at once.
This makes next 63 iterations as cheap as they could possibly be because they are served directly from L1d (just couple of cycles)
without being blocked by accesses to slower memory subsystem layers.</p>
</div>
<div class="paragraph">
<p>Furthermore, iterations of the loop do not have any dependencies between them.
This allows CPU to effectively engage out of order execution, meaning that multiple instructions can
be executed in parallel and in order different from program flow.
Out of order execution is possible thanks to the presence of multiple pipelined execution units inside CPU and
a CPU frontend which decodes instructions and feeds them to execution units faster than they execute the instructions.
This has net effect that once cache line is loaded into L1d, further iterations of the loop can be processed in parallel.
In total the latency of reading first 64 elements can be roughly estimated as 300c+(63x5c)/4, where
300c is the latency of loading cache line from memory, 5c&#8201;&#8212;&#8201;latency of accessing L1d, and 4 is the speedup due to
out of order execution.
The cost of initial memory read dominates the overall latency.</p>
</div>
<div class="paragraph">
<p>Next iteration accesses adjacent cache line (r=0,c=64)..(r=0,c=127), which is already&#8230;&#8203; in cache.
Three important optimizations make it possible to load data ahead of time:
prefetching, in-memory buffering, and high memory parallelism.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The goal of the first optimization&#8201;&#8212;&#8201;prefetching&#8201;&#8212;&#8201;is to predict which cache lines will be accessed in the long run and to load
them into cache in advance.
Sophistication of prefetchers&#8201;&#8212;&#8201;what and when they preload&#8201;&#8212;&#8201;differs between CPUs, however preloading
next cache line(s) in the course of linear scan is universally implemented in all CPUs due to ubiquity
and simplicity of this access pattern.
After just a few iterations prefetcher will sense that addresses are accessed sequentially one by one
and initiate aggressive preloading of successive cache lines, much earlier that they would be accessed
in the course of the program.</p>
</li>
<li>
<p>DRAM organization also plays crucial role.
It favors requests to the data located close together in address space.
Prefetching alone can&#8217;t help much by itself&#8201;&#8212;&#8201;it would only partially mask long latency of memory read.
Execution would still stall every 64&#8217;th iteration to wait for load to complete,
albeit for a shorter time period than otherwise would be without a prefetcher.
This problem is alleviated by buffered nature of DRAM design
("buffered" here is used as a general term and has nothing to do with buffered/registered DRAM).
Similar to cache, which works in units of 64B, RAM also has its own unit of operation&#8201;&#8212;&#8201;8KiB <em>row</em>.
When data at some address is accessed, DRAM loads the whole 8KiB row of adjacent data
from slow capacitor-based storage into much faster transistor-based buffer.
Successive requests to nearby addresses will be served directly from this buffer,
an operation that is several times faster than initial access.
In our case let&#8217;s assume that initial load takes 300c and further loads to nearby addresses cost only 100c.</p>
</li>
<li>
<p>Final major optimization relates to memory parallelism.
Internally memory is organized into hierarchy of <em>banks</em>, <em>ranks</em> and <em>channels</em>.
Lowest units of this hierarchy&#8201;&#8212;&#8201;banks&#8201;&#8212;&#8201;can work independently.
Memory controller makes use of this feature by interleaving address space across the banks.
It maps first cache line into the first bank, second cache line&#8201;&#8212;&#8201;to the second bank, and so on.
This doesn&#8217;t reduce the latency, but it allows to (pre)load multiple cache lines in parallel
if they are located in different banks, which is exactly what is happening during
sequentially accessed address space.
Typical parallelism is ~16 on a typical desktop and many times larger on server-grade CPUs
with a lot of memory modules installed.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When all three optimizations are combined, data is preloaded into L1d significantly
earlier than it is accessed by the program, and long latency of RAM stops to be a bottleneck.
For all iterations except the very first one performance is limited not by the latency of RAM,
but by the latency of L1d and execution units, which are already perfectly small and cannot be optimized any further.
This leads us to a final estimation that no matter how large the matrix is,
read stream is capable of processing the whole matrix in approximately (NxN)x(5/4) cycles, or 1.25c per element on average.
Timeline below roughly approximates sequence of events during sequential read.
Eventually the system will stabilize in a state when cache is entirely full: there will be a number of preloaded yet not accessed
cache lines, while all other cache lines will have already been processed by the program.
During each group of 64 iterations one more cache line will be preloaded, one cache line will be accessed by the program,
and one old cache line will be evicted from the cache due to limited L1d size.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="linread.png" alt="linread" width="100%">
</div>
</div>
<div class="paragraph">
<p>Note that all described features are important for achieving good sequential read performance:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>L1d serves read requests two orders of magnitude faster than RAM does, provided that data was previously cached</p>
</li>
<li>
<p>out of order execution has an effect of automatic parallelization of linear program flow inside small code window</p>
</li>
<li>
<p>in-memory buffer provides lower latency for closely located addresses compared to random addresses</p>
</li>
<li>
<p>prefetcher predicts which data will be needed in the future and preloads it into cache ahead of time</p>
</li>
<li>
<p>multi-bank memory geometry makes it possible to handle multiple prefetcher requests simultaneously</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Below table lists performance estimations as we "enable" features one by one.
Numbers are based on already given values: 5c for accessing L1d, 300c&#8201;&#8212;&#8201;for initial RAM load
(which we can decompose into 295c it takes to load cache line from RAM into L1d plus 5c for accessing L1d),
100c (=95c+5c)&#8201;&#8212;&#8201;for the following RAM loads to nearby addresses, x16 is the memory parallelism,
and x4 is the speedup due to out-of-order execution.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Cycles per element</th>
<th class="tableblock halign-center valign-top">=</th>
<th class="tableblock halign-center valign-top">Features enabled</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">295+5</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">300.0</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">none</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">(295+5*64)/64</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9.6</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">+caching</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">(295+5*64/4)/64</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.9</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">+out-of-order execution</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">(95+5*64/4)/64</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.7</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">+in-memory buffering</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">max(95/64, 5/4)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.5</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">+prefetching</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">max(95/64/16, 5/4)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.3</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">+multibank geometry</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Summarizing long explanation, sequential read is the best possible way to work with memory.
It is ubiquitous in practice, easily recognized by prefetchers, and there are many ways in hardware to optimize it for.
Once you have paid for initiating sequential read (which is high), data flows out of RAM at the speed that
is limited only by how fast you are able to process it.
In case of naive matrix transpose algorithm we can be absolutely sure that reading <code>src</code> matrix is not the bottleneck.</p>
</div>
</div>
<div class="sect2">
<h3 id="_write_stream">Write stream</h3>
<div class="paragraph">
<p>Write stream is different in two aspects:
1) addresses are not accessed in sequential manner anymore;
2) the performed operation is write, not read.
Both aspects make negative impact on performance.</p>
</div>
<div class="paragraph">
<p>It is common misconception to think of writing as of symmetric operation to reading.
While logically it may look so, the underlying mechanics is quite different,
giving both positive and negative traits to both types of operations.
Big downside of writing compared to reading is that to complete write operation memory must be accessed twice.
Recall that unit of memory operation is 64B-long cache line.
Most of the time programs want to modify only couple of bytes.
In order to fully complete a write, CPU has to perform three steps: load entire cache line first,
then apply the modification, then write entire cache line back to memory.
Such complex procedure increases overall cost of writing up to two times higher than it is for reading.
However, the good thing about writing is that often there is no need to wait for all of this.
Unlike reading, which requires that data is delivered to execution unit before continuing execution of the program
(surely, you cannot compute x+y until both of the arguments are loaded),
there is no need to complete write request here and now.
Instead of this, CPU memorizes write request and immediately continues execution of the program.
Write requests are stored in a special buffer&#8201;&#8212;&#8201;<em>store buffer</em>, which is 50-100 entries long.
Every entry in store buffer consists of an address of the target cache line, new data,
and the mask signifying which bytes must be modified.
CPU commits memorized write requests into cache and/or memory independently of program execution,
and even does so with multiple requests in parallel to leverage memory parallelism.
Net effect is that when number of unique affected cache lines per unit of time is small, store buffer
entirely masks long latency of memory subsystem, i.e. all write instructions appear to happen instantaneously fast.
Yes, somewhere in the background there is the process of loading cache lines, applying modifications
and writing cache lines back, but the program execution is too slow to notice all of this.
However, if write requests are appended to store buffer faster than they can be committed,
then store buffer becomes full and the execution is blocked.
If such thing happens, store buffer stops to be of any use and we are again exposed to memory subsystem latency,
additionally multiplied by a factor of up to 2.0 because each commit requires one read and one write.</p>
</div>
<div class="paragraph">
<p>Drawing below demonstrates the described worst-case scenario.
After short warm-up phase store buffer and all caches become full and the system transitions
into stable state.
Any attempt of store buffer to place modified cache line into L1d causes
eviction of some older cache line from L1d to L2, which causes eviction of some cache line from L2 to L3,
which in turn results in eviction of some cache line from L3 to DRAM itself.
Using previously used numbers, we can roughly estimate average processing time per element as (300c+100c)/16=25c.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="sb.png" alt="sb" width="80%">
</div>
</div>
<div class="paragraph">
<p>Another big difference of the write stream in our program is that matrix elements are accessed in column major order:
<code>src</code>, <code>src+N</code>, <code>src+2*N</code>,&#8230;&#8203;
This is called <em>strided</em> access pattern, which in general form is <code>base±stride*i</code>, where <code>i=0,1,2,&#8230;&#8203;</code>,
and <code>stride</code> is a constant value.
All previously described optimizations are in place, but now they are much less efficient.
The primary show-stopper comes from significantly reduced usefulness of caching.
Look at the animation in the beginning.
Due to strided access, first N elements are mapped to different cache lines and therefore
must be loaded directly from RAM.
Loading each of these cache lines is expensive, and even prefetcher with high memory parallelism can&#8217;t entirely
resolve this problem: they will deliver each next cache line 300c/16 on average.
But that is not all.
When the algorithm moves on to the next column, it will access elements mostly in the same cache lines.
We, of course, hope that they are already in L1d and thereby working with them will be fast.
But are they?
Not always.
It all depends on value of N versus cache sizes.
For small N ≤ size(L1d)/64 ≃ 512 our reasoning is valid, and the next column and actually
many more columns will be processed fast, since their elements are served from L1d.
But larger N will cause eviction of cache lines to L2, even larger N&#8201;&#8212;&#8201;to L3, and even more larger&#8201;&#8212;&#8201;to RAM.
This means that each time an element is accessed, its cache line has long been evicted from L1d
to a deeper-level cache and must be served from there.
The worst case happens when N is so big that single Nx64 block doesn&#8217;t fit even into L3.
In this case all accesses will be to RAM, and caching stops to be of any use.
For a system where entire 6 MiB L3 cache is available exclusively to transposer program,
this will happen with N at least (6*1024*1024)/64=98304.
It is hard to imagine a practical application which operates on a dense, 9GiB-sized matrices.
However, the insufficiency of L1d and L2 sizes is quite common.
For example, one color plane of Full HD frame is 1080 in height, and therefore
all cache lines backing single 1080x64 column would fit only in L2.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Cache</th>
<th class="tableblock halign-left valign-top">Cycles</th>
<th class="tableblock halign-left valign-top">Size</th>
<th class="tableblock halign-right valign-top">Cache lines (max N)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">L1d, private</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4-5</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">32 KiB</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">512</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">L2, private</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">12-15</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">256 KiB</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4096</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">L3, shared between 4 cores</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">40-45</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">6 MiB</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">98304</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The conclusion is that performance of strided access pattern depends on N.
We can roughly estimate that it can range between 5c/4 and 300c/16 per element, with more realistic
figure of 40c/4 (L2 latency divided by out-of-order speedup).
This last value is order of magnitude larger than 1.3c we estimated for read stream
and indicates that write stream is a severe bottleneck in naive transpose algorithm.</p>
</div>
</div>
<div class="sect2">
<h3 id="_combining_read_and_write_streams_together">Combining read and write streams together</h3>
<div class="paragraph">
<p>In the two previous sections we were analyzing read and write streams independently,
like if there was only one stream at a time.
As you probably see, even with such decomposition, theoretical explanation
was quite long due to complexity of CPU architecture.
When both streams are fused together into single algorithm, things become even more complicated
since both streams compete for the same resources.
This is further aggravated by the fact that we ignored many non-essential details,
such as virtual to physical address space translation, contention for cache resources from
different cores, prefetcher limits, etc.
Nevertheless, the key predictions for our algorithm holds true:
performance degrades as N grows, and the write stream is the bottleneck.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="naive_transpose_experiment.png" alt="naive transpose experiment" width="100%">
</div>
</div>
<div class="paragraph">
<p>Testing was performed by transposing matrices of different sizes filled with random values.
To avoid issues associated with microbenchmarking, each test processed at least 8GiB of data,
i.e. for N=1397, a group of K=4401 source/destination pairs of matrices were prepared and transposed in a burst;
the overall time was measured and divided by K.
Time was measured with <a href="https://www.felixcloutier.com/x86/rdtscp">rdtscp</a> instruction,
which (at least on this particular CPU) is a counter that increases by one each 1/max_non_turbo_frequency_hz.
Dividing per-matrix running time further by N^2 provides us with an average number of cycles it takes to transpose one element.
Blue plot shows how this value depends on N.</p>
</div>
<div class="paragraph">
<p>Except occasional spikes, which will be the topic of the next section, our prediction holds true.
We can clearly identify four performance regions: up to approximately N=487, up to N=3388, up to N=58838, and everything larger than that.
Up to N=487 the performance is around 2.5c per element.
This corresponds to a single Nx64 block fitting entirely into L1d cache.
Other regions are progressively slower: L2, L3 or RAM respectively are required to fit such a big block.</p>
</div>
<div class="paragraph">
<p>In addition to cycle measurements, I also included two counters related to L2 cache: <code>l2_rqsts.rfo_hit</code> and <code>l2_rqsts.rfo_miss</code>.
They come from performance monitoring unit (PMU) and can be measured with <code>perf</code> command line tool.
These counters measure number of L2 cache hit and miss events with respect to writes.
More specifically, RFO relates to multicore systems.
It is the message that CPU core sends in order to load cache line with intention to modify it.
<a href="https://perfmon-events.intel.com/skylake.html#L2_RQSTS.RFO_HIT">RFO hit</a> means that target cache line was found in cache
and in a state that allows its modification.
RFO miss means that cache line is either not found in cache or this core is not the exclusive owner of it.
Since our program is single threaded, we can assume that, with rare exceptions, all miss events were triggered by the former,
i.e. that cache line was not found in L2 and must be loaded from L3 or RAM.</p>
</div>
<div class="paragraph">
<p>Again, counter values agree perfectly with our expectations.
Up to N=487 there were neither hit nor miss L2 write events because Nx64 block fits into L1d cache
and therefore no RFOs were propagated to L2 at all.
Up to N=3388 hit rate is always 100%, confirming that data fits into L2.
With even larger N values hit rate slowly falls up to approximately N=5712,
when all accesses result in misses.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_dealing_with_cache_aliasing">Dealing with cache aliasing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While our expectations in general work good, for some N we can observe high spikes in running time.
They occur usually around some "good" numbers, such as powers of two: 256, 2048, 4096.
By looking into PMU counters we can detect that these spikes coincide with suspiciously bad cache behaviour&#8201;&#8212;&#8201;something we didn&#8217;t anticipate.
What exactly could the be the cause?</p>
</div>
<div class="paragraph">
<p>The issue stems from cache organization.
Usually we model cache as a black box, which is able to store K cache lines with last recently used eviction policy.
In another words, if we access K different cache lines, then they all will happily settle down in cache,
and only after accessing K+1&#8217;th distinct cache line some older cache line will have to be evicted.
Such behaviour requires the cache to be <em>fully associative</em>, i.e. every slot in the cache must be able
to store any cache line.
In reality fully associative caches are rarely used because of performance issues&#8201;&#8212;&#8201;imagine, to search
for a cache line, hardware would have to compare target address with all K (tens or even hundreds of thousands) cache slots simultaneously.
Instead, archetypical cache is organized as a collection of <em>multi-way sets</em>.
Logically, each set acts as a small, independent, fully-associative cache with capacity equal to number of ways (typically 4-16).
Given index of the cache line, it can be stored only inside single set, which is computed using modulo operator.
Example below shows the computations that are performed when you dereference a pointer with value 57690<sub>10</sub>.
First, cache line index is computed by dividing address by cache line size: ⌊57690<sub>10</sub>/64⌋=901.
And next, taking the remainder of dividing index by number of sets gives the set index: 901 % 64 = 5.
Set number 5 is the only set where cache line backing address 57690<sub>10</sub> can be cached.
Note that all other cache lines sharing the same remainder have to compete for the same set number 5.
Such organization is very efficient.
Since cache line size and number of sets in practice are both powers of two, the result of division and modulo operators
is just subsequence of bits inside binary representation of the address, so in fact nothing needs to be "computed".
And once the set index is known, only 8 ways must be checked simultaneously to find whether cache line at given address is cached or not.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="aliasing.png" alt="aliasing" width="65%">
</div>
</div>
<div class="paragraph">
<p>For the majority of programs multi-set design is no different from fully-associative cache
since sequence of accessed addresses in a random program is irregular.
For a typical program, the majority of memory accesses are caused by walking
through complex hierarchies of objects by dereferencing pointers into the heap.
Hence all the cache sets are loaded approximately evenly.
However, algorithms operating on multi-dimensional data are a notable exception from this rule.
Majority of such algorithms walk along columns or planes of the data, thus inducing strided access pattern.
Naive transposer is exactly the algorithm of this kind.
Its inner loop fills in single column, thus accessing elements standing apart by N: ColumnBegin+i*N, i=0,1,2,&#8230;&#8203;
Strided access pattern is notorious for its interference with multi-set cache organization.
Suppose that ColumnBegin=12345678<sub>10</sub>, N=256, and number of sets is 64.
The elements of this column are mapped to the following sequence of set indices:</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="exalias.png" alt="exalias" width="75%">
</div>
</div>
<div class="paragraph">
<p>You can see that the pattern repeats and that only 16 sets are involved instead of all 64.
The remaining 48 sets have zero load.
This effectively reduces cache size from 512 entries down to just 128.
Since the latter is smaller than N=256, data overflows into L2, ruining performance for this specific N.
For each level of cache there exist some values of N, for which one column of data should fit into cache
but it doesn&#8217;t because of multi-set design.
These bad N values may differ from one level of cache to another one because caches have different number of sets and ways.</p>
</div>
<div class="paragraph">
<p>Now it becomes clear that it is not enough to compare N with number of cache entries to predict performance.
To ensure perfect caching we need to compute load of each of the sets by simulating first N iterations.
If load for all sets is less or equal to number of ways, then Nx64 block will fit into the desired cache.
If load exceeds number of ways for at least one set, then there will be overflow, leading
to suboptimal performance.
Of course, simulation is too cumbersome to do in practice.
Fortunately, it is possible to avoid cache aliasing (how this issue is called) without knowing
the exact geometry of caches.
Let us state sufficient condition on N that ensures perfect caching: N must equal to cache line size multiplied by some odd number.
All of N=64, N=192, N=6464, N=6592 will guarantee perfect caching for all cache levels.
To prove the stated condition it is enough to show that for every two iterations i1, i2, such that 0 &lt; |i1 - i2| &lt; 64,
accessed addresses are mapped to different cache sets.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="oddproof.png" alt="oddproof" width="90%">
</div>
</div>
<div class="paragraph">
<p>The last transition follows from the fact that since NumSets is always power of two, then NumSets and OddNumber are co-prime.
This is where we depend on the oddity.
Final statement is true since 0 &lt; |i1 - i2| &lt; NumSets = 64.
Net result is that if N equals to some odd number times cache line size, then any N consecutive iterations
will access cache lines mapped to different sets.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="cache_set_load.png" alt="cache set load" width="90%">
</div>
</div>
<div class="paragraph">
<p>How can the knowledge of which sizes are good and which are bad be of any help to us?
We cannot require user to work only with matrices of good sizes&#8201;&#8212;&#8201;we have to support arbitrary-sized matrices.
But what we can do is to require user to pad matrix up to an odd number of cache lines during memory allocation.
If user wants matrix of, let&#8217;s say, logical size 4000x4000, then they should allocate matrix of physical size 4032x4032 instead.
Top left block of size 4000x4000 would carry the actual data, while the remaining fringe is just a placeholder to ensure that
the stride value is proportional to an odd number of cache lines.
In terms of memory overhead requirement for padding is costly only for small matrices.
With N growing to infinity, the overhead fades to zero very soon.
For N=4000 it is less than 2%.</p>
</div>
<div class="paragraph">
<p>In addition to padding, we will also require that the beginning of matrices is aligned by cache line size (64).
Together with requirement for padding this means that each row will be covered by integer number of cache lines.
The regularity of such layout grants us a number of benefits:
it is much easier to analyze;
cache lines backing matrices become "matrix-private" in the sense that they carry only the data of the matrix and
never the data outside of it; finally, but of no least importance, is that such layout is aesthetically pleasing.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="srcwisepad.gif" alt="srcwisepad">
</div>
</div>
<div class="paragraph">
<p>After imposing new requirements and rerunning our previous examples, all timings become as expected.
There are no more unpleasant spikes.
For the sake of brevity, from now on, we will assume that matrix size N itself is proportional to an odd number of cache lines
and that its beginning is aligned with the beginning of cache line.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-center valign-top">Original size</th>
<th class="tableblock halign-center valign-top">Padded size</th>
<th class="tableblock halign-center valign-top">CPE improvement</th>
<th class="tableblock halign-right valign-top">Memory overhead</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">256</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">320</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">6.1 →  2.3</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">56%</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">1024</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">1088</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">13.3 →  4.0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">13%</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">2048</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">2112</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">14.0 →  3.9</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6%</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">4096</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">4160</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">23.4 →  5.6</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">3%</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">30720</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">30784</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">24.3 →  6.7</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&lt;1%</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">77824</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">77888</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">26.9 →  9.2</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&lt;1%</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">111744</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">111808</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">21.8 → 12.4</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&lt;1%</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_reversing_the_order_2_61">Reversing the order [2.61]</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the naive algorithm, we have perfectly good read performance but very poor write performance.
The latter is caused primarily by the fact that single Nx64 rectangle of <code>dst</code> is not able to fit into small L1d cache
and eventually has to settle down in caches of deeper levels or RAM, from where it has to be reloaded 64 times.
But also poor performance to a lesser extent is the result of the mechanics of write operation.
Unlike read operations, which can discard cache line after the load is complete, write operations
have to load cache line, modify it, and store it back into cache.
They cannot throw it away.
In this sense, writes are more expensive that reads in our algorithm.
This leads us to a straightforward idea of reversing the order of scans.
Let&#8217;s scan <em>destination</em> matrix in row-major order, while source matrix in column-major order.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="dstwise.gif" alt="dstwise">
</div>
</div>
<div class="paragraph">
<p>We will call new algorithm the "reverse" one.
Its code is exactly the same as for "naive" algorithm except that <code>src</code> and <code>dst</code> are swapped.
Note that now it is <code>dst</code> matrix that is processed in a natural way: row by row, column by column.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><table class="linenotable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8</pre></div></td><td class="code"><pre><span></span><span class="tok-kt">void</span> <span class="tok-nf">transpose_Reverse</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">n</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">n</span><span class="tok-p">();</span>
  <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">r</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">r</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-p">;</span> <span class="tok-n">r</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
    <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">c</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">c</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-p">;</span> <span class="tok-n">c</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
      <span class="tok-n">dst</span><span class="tok-o">-&gt;</span><span class="tok-n">data</span><span class="tok-p">()[</span><span class="tok-n">n</span> <span class="tok-o">*</span> <span class="tok-n">r</span> <span class="tok-o">+</span> <span class="tok-n">c</span><span class="tok-p">]</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">data</span><span class="tok-p">()[</span><span class="tok-n">n</span> <span class="tok-o">*</span> <span class="tok-n">c</span> <span class="tok-o">+</span> <span class="tok-n">r</span><span class="tok-p">];</span>
    <span class="tok-p">}</span>
  <span class="tok-p">}</span>
<span class="tok-p">}</span>
</pre></td></tr></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>By changing the order of scans we optimize for writes while sacrifiying reads.
Now writes are perfect instead of reads.
Since destination matrix is scanned in row-major order, a group of 64 consecutive <code>dst</code> elements is mapped to a single cache line.
This allows for the cache line to be fully assembled from scratch in a store buffer and then flushed into cache,
all of this without waiting for the load of the cache line to complete.
Reads become bad, of course, for the same reasons as previously writes were.
They are served from the fastest cache where Nx64 rectangle fits.</p>
</div>
<div class="paragraph">
<p>To see the difference between naive and reverse algorithms, let&#8217;s apply them
to a matrix of size N=2112 and look at PMU counters related to caching.
Table below lists such counters for transposing 1925 different N=2112 matrices,
covering in total 8GiB memory region.
Two right columns contain counter values normalized by total number of elements (2112x2112x1925).
Normalized values close to integers are of primary interest to us.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.6666%;">
<col style="width: 16.667%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top">[unit]</th>
<th class="tableblock halign-right valign-top">Naive</th>
<th class="tableblock halign-right valign-top">Reverse</th>
<th class="tableblock halign-right valign-top">Naive (per element)</th>
<th class="tableblock halign-right valign-top">Reverse (per element)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">32,871,245,961</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">23,190,957,114</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">3.83</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.70</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resource_stalls.sb</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">13,003,900,440</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6,354,997</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.51</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">l2_rqsts.rfo_miss</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">168,060,516</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">10,456,844</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.02</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">l2_rqsts.rfo_hit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">8,415,924,030</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">118,731,907</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>0.98</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.01</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">mem_load_retired.l1_hit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">25,579,546,018</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">17,215,727,397</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>2.97</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>2.00</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">mem_load_retired.l1_miss</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">40,903,824</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">8,566,909,802</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>1.00</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">mem_load_retired.l2_hit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">33,705,885</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">8,329,067,355</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>0.97</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">mem_load_retired.l2_miss</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">7,204,398</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">237,847,533</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.03</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>First let&#8217;s compare writes.
As was already mentioned before, l2_rqsts.rfo_{miss|hit} count L2 misses and hits with respect
to write operation.
Naive algorithm demonstrates one hit per each element and no misses, proving that for each processed
element cache line has to be fetched from L2.
This is exactly where we expect to find it, since single Nx64 block cannot fit into L1 but fits into L2:
32KiB &lt; 2112x64=132KiB ≤ 256KiB.
On the other hand, both counters are zero for the reverse algorithm, indicating that there was no need
to search L2 or higher-level caches at all.
Another way to see that reverse algorithm significantly improves write performance is resource_stalls.sb counter.
Roughly speaking, it measures number of cycles when execution couldn&#8217;t advance because store buffer had no free entry.
Its value is approximately 40% of all cycles for the naive algorithm, but is nearly zero for the reverse algorithm.</p>
</div>
<div class="paragraph">
<p>Comparing read performance is slightly more complicated.
Presence of three L1 hits per each element and no L2 misses for the naive algorithm proves our reasoning
that all data comes from L1.
But why three loads and not one as we expected?
To answer this question, let&#8217;s look at the generated machine code.
Below is the inner loop of the naive transpose algorithm generated by GCC-10.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="asm"><table class="linenotable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16</pre></div></td><td class="code"><pre><span></span><span class="tok-c1">;+----------+--------------------------------------------------------+</span>
<span class="tok-c1">;| r9/r10   | src/dst                                                |</span>
<span class="tok-c1">;| 24       | offset of _data inside Mat class                       |</span>
<span class="tok-c1">;| rax/rdx  | offset of the current element from src._data/dst._data |</span>
<span class="tok-c1">;| rdi      | dst stride                                             |</span>
<span class="tok-c1">;+----------+--------------------------------------------------------+</span>

<span class="tok-nl">.L7:</span>
  <span class="tok-nf">movq</span>    <span class="tok-mi">24</span><span class="tok-p">(</span><span class="tok-nv">%r9</span><span class="tok-p">),</span> <span class="tok-nv">%rcx</span>       <span class="tok-c1">; load (extra)</span>
  <span class="tok-nf">movzbl</span>  <span class="tok-p">(</span><span class="tok-nv">%rcx</span><span class="tok-p">,</span><span class="tok-nv">%rax</span><span class="tok-p">),</span> <span class="tok-nv">%esi</span>   <span class="tok-c1">; load (data)</span>
  <span class="tok-nf">movq</span>    <span class="tok-mi">24</span><span class="tok-p">(</span><span class="tok-nv">%r10</span><span class="tok-p">),</span> <span class="tok-nv">%rcx</span>      <span class="tok-c1">; load (extra)</span>
  <span class="tok-nf">incq</span>    <span class="tok-nv">%rax</span>
  <span class="tok-nf">movb</span>    <span class="tok-nv">%sil</span><span class="tok-p">,</span> <span class="tok-p">(</span><span class="tok-nv">%rcx</span><span class="tok-p">,</span><span class="tok-nv">%rdx</span><span class="tok-p">)</span>   <span class="tok-c1">; store</span>
  <span class="tok-nf">addq</span>    <span class="tok-nv">%rdi</span><span class="tok-p">,</span> <span class="tok-nv">%rdx</span>
  <span class="tok-nf">cmpq</span>    <span class="tok-nv">%rax</span><span class="tok-p">,</span> <span class="tok-nv">%r8</span>
  <span class="tok-nf">jne</span>     <span class="tok-no">.L7</span>
</pre></td></tr></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>It turns out that in lines 9 and 11 there are extra loads.
They reload addresses stored in <code>src._data</code> and <code>dst._data</code> into registers during each iteration
of the algorithm.
Since these values are constant, they obviously are always served from L1 and do not have
any noticeable negative impact on performance.
However, they increase counter values in the same way as truly heavy data loads do.
That&#8217;s why we observe two extra loads from L1d per each processed element than we expected.
This happens for both algorithms since generated code is identical with the exception that
references to <code>src</code> and <code>dst</code> are swapped.
Of course, different compilers and even different options of the same compiler may result
in different generated code, with bigger or fewer number of loads per each element.</p>
</div>
<div class="paragraph">
<p>Now with extra loads demystified, we can clearly see that performance of reads mirrors that of writes.
All data loads of the naive algorithm are served from L1 with no accesses to deeper level caches.
But for the reverse algorithm data loads are served from L2.
The latter can be deduced from l1_miss≈l2_hit≈1.0 and no accesses to L3 (l2_miss≈0.0).</p>
</div>
<div class="paragraph">
<p>To conclude the analysis, let&#8217;s look into the most important metric&#8201;&#8212;&#8201;number of cycles per element.
Here we can see that reverse algorithm is faster by 1.4 times.
Not bad taking into account that the only thing we had to do is to swap <code>src</code> and <code>dst</code> references.
Although read and write performance in terms of events per element are entirely symmetric for
two algorithms, write "events" turn out to be more expensive.
This explains the superiority of reverse algorithm compared to the naive one.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="cmprw.png" alt="cmprw" width="75%">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_exploring_block_structure_1_46">Exploring block structure [1.46]</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Both naive and reverse algorithms are inefficient because the way they access memory
causes continual eviction of L1d cache lines into deeper-level caches (or into RAM) and further reload
of these cache lines back into L1d.
In other words, working set is too huge compared to L1d cache.
For both algorithms it equals to 64×N+<em>ε</em>.
As soon as N exceeds size(L1d)/64≃512, continual eviction and reload of cache lines takes place.
During this time program flow is blocked by large number of cycles during each iteration,
leading to poor performance.
The larger N, the deeper memory subsystem layers are accessed and the more prominent degradation becomes.
If we want to make any progress on optimization, we need to somehow make working set fit into L1d no matter how large N is.</p>
</div>
<div class="paragraph">
<p>Luckily, the transpose operation preserves 2D data locality.
If a group of elements are located close in source matrix, then they will be close in destination matrix.
They are not scattered all over the <code>dst</code> matrix, but closely reside in a submatrix of the same shape and size.
Not all operations are so good.
For example, when squaring a matrix, each output element depends on one full row and one full column of the input matrix.
In another example&#8201;&#8212;&#8201;computing partial sums&#8201;&#8212;&#8201;each output element is the sum of all i⋅j preceding elements.
But the majority of matrix operations preserve 2D data locality,
since matrix as a data structure is designed to represent spatially related data.
Transpose also falls into category of such operations.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="face.png" alt="face" width="40%">
</div>
</div>
<div class="paragraph">
<p>Data locality is a very powerful property because it allows to split matrix into arbitrary
non-overlapping blocks and process one block at a time.
The result of a single block transpose will be also a block of the same shape and size,
therefore allowing caching to do its best.
If block size is small enough, then it can be processed entirely within L1d boundaries
without overflowing into deeper-level caches&#8201;&#8212;&#8201;exactly what we need.
Image below demonstrates how we can split 12x12 matrix into 4x4 blocks.
Note an obvious fact that locations of the blocks themselves are also transposed.
For example, when we are working on top-right AC block, we are transposing its elements and writing them
into bottom-left block of the destination matrix.
More theoretically it can be said that what we are doing is a composition of 1+(12/4)(12/4)=10 transpositions.
One of them is the outer transposition of the 3x3 block matrix itself, where each element is a single 4x4 block.
All other 9 transpositions are byte-wise transpositions of the respective blocks.
We can perform transpositions in any order we like among all 10! variants and will get identical results in the end.
In practice we will process blocks one at a time and write elements to the final positions in <code>dst</code> matrix,
thereby implicitly performing outer transposition.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="blkinterpret.png" alt="blkinterpret" width="60%">
</div>
</div>
<div class="paragraph">
<p>Guided by the idea, let&#8217;s treat matrix as consisting of cache aligned blocks of size 64x64,
so that every row of a block maps to exactly one cache line.
With such setup, no matter how large original NxN matrix is, the working set of any transposer
is bounded from above by 2×64×64+<em>ε</em> = 8KiB+<em>ε</em> &lt; size(L1d).
This bound consists of one 64x64 block of the source matrix, one 64x64 block of the destination matrix, and some small,
constant-sized volume of memory for the algorithm itself.</p>
</div>
<div class="paragraph">
<p>Source code for block transpose requires four nested loops.
Two outer loops iterate over the blocks, while two inner loops iterate over the elements of a single block.
The order of scans (row-major or column-major) is of no particular importance thanks to small working set,
with only marginal difference in performance between all four possible options.
The piece of code below scans blocks in row-major order, while the elements of a single block in a column-major order,
exactly as in the animation.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><table class="linenotable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15</pre></div></td><td class="code"><pre><span></span><span class="tok-kt">void</span> <span class="tok-nf">transpose_Blocks</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">n</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">n</span><span class="tok-p">();</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">bsize</span> <span class="tok-o">=</span> <span class="tok-mi">64</span><span class="tok-p">;</span>
  <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">rb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">rb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">rb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
    <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">cb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">cb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">cb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
      <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">src_origin</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">data</span><span class="tok-p">()</span>  <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">rb</span> <span class="tok-o">*</span> <span class="tok-n">n</span> <span class="tok-o">+</span> <span class="tok-n">cb</span><span class="tok-p">)</span> <span class="tok-o">*</span> <span class="tok-n">bsize</span><span class="tok-p">;</span>
            <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dst_origin</span> <span class="tok-o">=</span> <span class="tok-n">dst</span><span class="tok-o">-&gt;</span><span class="tok-n">data</span><span class="tok-p">()</span> <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">cb</span> <span class="tok-o">*</span> <span class="tok-n">n</span> <span class="tok-o">+</span> <span class="tok-n">rb</span><span class="tok-p">)</span> <span class="tok-o">*</span> <span class="tok-n">bsize</span><span class="tok-p">;</span>
      <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">r</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">r</span> <span class="tok-o">&lt;</span> <span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">r</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
        <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">c</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">c</span> <span class="tok-o">&lt;</span> <span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">c</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
          <span class="tok-n">dst_origin</span><span class="tok-p">[</span><span class="tok-n">r</span> <span class="tok-o">*</span> <span class="tok-n">n</span> <span class="tok-o">+</span> <span class="tok-n">c</span><span class="tok-p">]</span> <span class="tok-o">=</span> <span class="tok-n">src_origin</span><span class="tok-p">[</span><span class="tok-n">c</span> <span class="tok-o">*</span> <span class="tok-n">n</span> <span class="tok-o">+</span> <span class="tok-n">r</span><span class="tok-p">];</span>
        <span class="tok-p">}</span>
      <span class="tok-p">}</span>
    <span class="tok-p">}</span>
  <span class="tok-p">}</span>
<span class="tok-p">}</span>
</pre></td></tr></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>The efficient usage of cache should be evident from the animation.
Now reading can cause expensive stalls only during the first 64 iterations of transposing each block,
when the block is being loaded into cache, while the data for the remaining 63*64 iterations is cheaply served from L1d.
Situation with writes is similar.
Rows of <code>dst</code> block are fully assembled in store buffer and flushed at some point later
asynchronously to code execution.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="blocks.gif" alt="blocks">
</div>
</div>
<div class="paragraph">
<p>As before, per-element PMU counters can be used to confirm our findings.
If we apply all three algorithms to a matrix of N=2112, we can observe
that block algorithm is the only one not affected by any cache misses.
All L2-related counters are zero, meaning that there was no need to send requests to L2 or deeper caches.
This is true for both reads (mem_load_retired.l2_{hit,miss}) and writes (l2_rqsts.rfo_{hit,miss}).
In this sense block algorithm inherits all the benefits of naive and reverse algorithms without inheriting any
of their drawbacks.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top">[unit]</th>
<th class="tableblock halign-right valign-top">Naive</th>
<th class="tableblock halign-right valign-top">Reverse</th>
<th class="tableblock halign-right valign-top">Blocks</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">3.83</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.70</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.46</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resource_stalls.sb</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.51</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">l2_rqsts.rfo_miss</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.02</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.02</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">l2_rqsts.rfo_hit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>0.98</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.01</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">mem_load_retired.l1_hit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>2.97</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>2.00</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>0.99</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">mem_load_retired.l1_miss</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>1.00</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.01</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">mem_load_retired.l2_hit</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>0.97</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">mem_load_retired.l2_miss</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.00</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.03</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.01</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Now let&#8217;s rerun the tests for matrices larger than N=2112.
We can now clearly see what makes block algorithm so good.
Its performance is constant for both small and big matrices&#8201;&#8212;&#8201;a property of the algorithms
so much welcomed in practice.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="nrb.png" alt="nrb" width="100%">
</div>
</div>
<div class="paragraph">
<p>It is hard to overestimate the importance of block decomposition for the matrix algorithms.
Like all such algorithms, block transposer provides constantly good performance,
and the code listing is still fewer than 20 lines of non-architectural code.
Block transpose is probably the method of choice for the majority of applications.
It is efficient, simple, and doesn&#8217;t depend on specifics of particular CPU architecture.
Upcoming methods are even faster, but they lack these properties.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_software_prefetching_1_35">Software prefetching [1.35]</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In our next improvement towards reducing memory latency we will stop relying
on automatic prefetching and replace it with manually issued hints to prefetch data.
In optimization parlance the former is usually called "hardware" prefetching
to emphasize that prefetch requests are initiated by hardwired algorithms,
while the latter is called "software" prefetching.</p>
</div>
<div class="paragraph">
<p>The benefits of hardware prefetching are many.
Its killer feature is that it works fully automatically without any input from the coder
as we have already seen.
Of course, you still have to know basic dos and don&#8217;ts.
For example, unnecessary using linked lists instead of arrays will destroy prefetching altogether.
But for an average program without blatant flaws hardware prefetching works well.
Another good thing about hardware prefetching is that, since it is implemented in hardware,
it can take into account (at least theoretically) all the architectural details of specific
CPU/RAM setup, such as cache sizes, parallelism, memory timings, etc.
Moreover, behaviour of hardware prefetching can change dynamically along with program execution.
Imagine, for example, that prefetcher mispredicts that a sequence of addresses a1,a2,a3,&#8230;&#8203; will be accessed
in the future and starts aggressively to preload them.
With the help of counters implemented in the cache, it can figure out its mistake after some time,
i.e. the fact that preloaded data is not accessed by the program.
In this case prefetcher can stop prefetching this sequence of addresses to avoid cache pollution
and to give opportunity to other streams of requests competing for memory access.
Such dynamic algorithm can be implemented with not-so-complicated counters in hardware,
but implementing it in software would be cumbersome to say the least.</p>
</div>
<div class="paragraph">
<p>And yet hardware prefetching is far from perfect.
Its scope of prefetching is usually limited to 4KiB memory page,
meaning that it won&#8217;t preload data that is beyond already accessed page.
Another downside is that number of independently tracked streams is not very big.
In our block transpose algorithm we need 64 of them to fully preload next block ahead of time.
But probably the largest downside is that hardware prefetcher, working on a very low level,
cannot grasp the high-level idea of the algorithm.
For example, in block transpose algorithm it is quite improbable that it would predict
correctly and preload block in position (1,0), while CPU is crunching top right block (0,nr_blocks-1).
In general, in matrix algorithms, where operations are quite simple but are applied to large volume of data,
it makes sense to try to guide prefetcher by issuing prefetch commands manually.
Sometimes it improves performance, sometimes it doesn&#8217;t.</p>
</div>
<div class="paragraph">
<p>Prefetch instructions are architecture-dependent and are usually called through intrinsics&#8201;&#8212;&#8201;C/C++ library functions provided by the compiler.
In case of x86_64 we will call <code>__mm_prefetch(char* addr, _MM_HINT_NTA)</code>, which is translated
by all popular C/C++ compilers into one of the corresponding x86_64 <a href="https://www.felixcloutier.com/x86/prefetchh">PREFETCH<em>h</em></a> instructions.
This function takes two arguments: address and locality hint.
Address can be any, not necessarily aligned with cache line boundary, and it can be even an invalid address,
in which case all access violation errors are silently suppressed.
The latter feature is useful since it allows to avoid conditional logics when working near boundary of vectors/matrices.
Second argument is a locality hint that indicates our intention towards the data.
<code>_MM_HINT_NTA</code> ("non-temporal") indicates that we are going to work with data only briefly,
thereby helping CPU to take appropriate steps to evict data soon and avoid cache pollution.
Note that issuing <code>__mm_prefetch()</code> is merely a hint and does not guarantee any solid outcome.
CPU is free to entirely ignore it.
Explanation of different locality hints (T0, T1, T2, NTA) also should be taken with a grain of salt
due to vast number of subtleties in modern CPUs.
Treat locality hints more like an opaque enumeration with no clear semantics attached.
In general, the most reliable approach is to try different combinations of <code>__mm_prefetch()</code> placements and locality hints
and to see which one improves performance the most.</p>
</div>
<div class="paragraph">
<p>Now let&#8217;s apply software prefetching to block transpose algorithm.
While we are working on a single block, we would like to initiate preload
of the next block in the background.
Our changes are the following.
First, we include <code>immintrin.h</code> header to get access to x86_64 prefetch functions.
Second, each time we start processing new block, we also compute <code>prf_origin</code>&#8201;&#8212;&#8201;pointer
to the next block of the <code>src</code> matrix.
Finally, we issue <code>__mm_prefetch</code> per every row of the next block, 64 in total.
It is quite doubtful that CPU would be able to process them efficiently if all of them were issued at once.
So, instead we insert prefetch commands before processing every row of the current block.
Such placement scatters prefetch commands evenly along the instruction sequence.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><table class="linenotable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30</pre></div></td><td class="code"><pre><span></span><span class="tok-cp">#include</span> <span class="tok-cpf">&lt;immintrin.h&gt;</span><span class="tok-cp"></span>

<span class="tok-cm">/* Compute origin of the 64-block next to (rb, cb) in row-major order */</span>
<span class="tok-kr">inline</span> <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">next_block</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-kt">int64_t</span> <span class="tok-n">rb</span><span class="tok-p">,</span> <span class="tok-kt">int64_t</span> <span class="tok-n">cb</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-kt">int64_t</span> <span class="tok-n">cb1</span> <span class="tok-o">=</span> <span class="tok-n">cb</span> <span class="tok-o">+</span> <span class="tok-mi">1</span><span class="tok-p">;</span>
  <span class="tok-kt">int64_t</span> <span class="tok-n">rb1</span> <span class="tok-o">=</span> <span class="tok-n">rb</span><span class="tok-p">;</span>
  <span class="tok-k">if</span> <span class="tok-p">(</span><span class="tok-n">cb1</span> <span class="tok-o">==</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">n</span><span class="tok-p">()</span><span class="tok-o">/</span><span class="tok-mi">64</span><span class="tok-p">)</span> <span class="tok-p">{</span>
    <span class="tok-n">rb1</span> <span class="tok-o">+=</span> <span class="tok-mi">1</span><span class="tok-p">;</span>
    <span class="tok-n">cb1</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span>
  <span class="tok-p">}</span>
  <span class="tok-k">return</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">data</span><span class="tok-p">()</span> <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">rb1</span><span class="tok-o">*</span><span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">n</span><span class="tok-p">()</span> <span class="tok-o">+</span> <span class="tok-n">cb1</span><span class="tok-p">)</span> <span class="tok-o">*</span> <span class="tok-mi">64</span><span class="tok-p">;</span>
<span class="tok-p">}</span>

<span class="tok-kt">void</span> <span class="tok-n">transpose_BlocksPrf</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">n</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">n</span><span class="tok-p">();</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">bsize</span> <span class="tok-o">=</span> <span class="tok-mi">64</span><span class="tok-p">;</span>
  <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">rb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">rb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">rb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
    <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">cb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">cb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">cb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
      <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">src_origin</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">data</span><span class="tok-p">()</span> <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">rb</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-n">cb</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-n">bsize</span><span class="tok-p">;</span>
            <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dst_origin</span> <span class="tok-o">=</span> <span class="tok-n">dst</span><span class="tok-o">-&gt;</span><span class="tok-n">data</span><span class="tok-p">()</span> <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">cb</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-n">rb</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-n">bsize</span><span class="tok-p">;</span>
      <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">prf_origin</span> <span class="tok-o">=</span> <span class="tok-n">next_block</span><span class="tok-p">(</span><span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">rb</span><span class="tok-p">,</span> <span class="tok-n">cb</span><span class="tok-p">);</span>
      <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">r</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">r</span> <span class="tok-o">&lt;</span> <span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">r</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
        <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span> <span class="tok-o">+</span> <span class="tok-n">r</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
        <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">c</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">c</span> <span class="tok-o">&lt;</span> <span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">c</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
          <span class="tok-n">dst_origin</span><span class="tok-p">[</span><span class="tok-n">r</span> <span class="tok-o">*</span> <span class="tok-n">n</span> <span class="tok-o">+</span> <span class="tok-n">c</span><span class="tok-p">]</span> <span class="tok-o">=</span> <span class="tok-n">src_origin</span><span class="tok-p">[</span><span class="tok-n">c</span> <span class="tok-o">*</span> <span class="tok-n">n</span> <span class="tok-o">+</span> <span class="tok-n">r</span><span class="tok-p">];</span>
        <span class="tok-p">}</span>
      <span class="tok-p">}</span>
    <span class="tok-p">}</span>
  <span class="tok-p">}</span>
<span class="tok-p">}</span>
</pre></td></tr></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>Table below compares performance of four previously described algorithms.
Impact of prefetching is easier to demonstrate using huge matrix.
Therefore testing was performed by transposing three different N=46400 matrices filled with random values.
In addition to cycles and cycles per element (cpe) metrics I also included <code>cycle_activity.stalls_total</code>.
The latter is PMU metric that counts number of cycles when execution was stalled for any reason.
Since we know a priori that we have big issues with memory access,
we can expect that most stalls in the compared algorithms are caused by the data being
requested from memory subsystem but not delivered yet, and therefore CPU has to idle.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 80%;">
<colgroup>
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">&nbsp;</th>
<th class="tableblock halign-right valign-top">cpe</th>
<th class="tableblock halign-right valign-top">cycles</th>
<th class="tableblock halign-right valign-top">cycle_activity.stalls_total</th>
<th class="tableblock halign-right valign-top">% stalled</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Naive</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.95</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">46,457,240,255</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">23,955,991,848</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">51.6%</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Reverse</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.84</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">38,896,868,050</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">16,055,020,614</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">41.3%</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Blocks</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.64</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">10,920,664,329</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2,405,368,005</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">22.0%</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">BlocksPrf</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>1.41</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9,389,790,532</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">900,270,568</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9.6%</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>According to the table the largest gain in performance was achieved by switching to block algorithm.
This is understandable because block algorithm makes caching perfect.
Impact of software prefetcher is not so dramatic.
However, given that it costs virtually nothing to implement, cpe reduction by 14% is also quite pleasant.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_64_bit_simd_0_74">64-bit SIMD [0.74]</h2>
<div class="sectionbody">
<div class="paragraph">
<p>So far we were dealing with high latency of memory subsystem.
Exploiting block property helped to solve this problem quite well.
Since now on we will stick with the idea of processing one 64×64 cache aligned block at a time
and will focus on improving the algorithm of transposing such blocks.</p>
</div>
<div class="paragraph">
<p>Block transpose algorithm we designed so far makes only one load and one store per every element.
This is the lowest as it can be provided that we limit ourselves with processing only one element at a time.
Now recall that elements of our matrix are bytes, and, therefore, loads and stores we perform act on single bytes.
This doesn&#8217;t fully utilize the power of x86_64 since its registers and almost all instructions are 64-bit.
Using narrower registers and instructions (8/16/32 bits) typically costs the same as if using
native 64-bits instructions.
So, let&#8217;s try to design an algorithm that will process multiple elements simultaneously.</p>
</div>
<div class="paragraph">
<p>Since single 64-bit register can hold 8 elements at once, it is tempting to apply block decomposition again
and design an algorithm that is capable of transposing 8x8 matrices.
Such an algorithm would start from loading rows of the input 8x8 matrix into eight 64-bit variables,
then it would perform some black-box computations on these variables, finally producing eight
64-bit variables holding rows of the <em>transposed</em> matrix.
These rows would be directly written into their proper positions of <code>dst</code> matrix.
Now, given such black-box algorithm, if we decompose <code>src</code> matrix into 64x64 blocks first (to optimize caching),
and then decompose every 64x64 block into 8x8 blocks, and then apply 8x8 transposer
to each of these blocks, we will get properly transposed <code>dst</code> matrix.</p>
</div>
<div class="paragraph">
<p>Designing efficient 8x8 transposer with 64-bit variables provides some challenges, though.
Every row of the destination matrix depends on all rows of the source matrix.
It would be very inefficient to use brute force transposer that would compute every output row independently:
to compute single output row, every input row must be masked, shifted properly and ORed with the result.
Overall complexity of such transposer would be proportional to 8x8, thereby providing no improvement in performance
over the naive algorithm.
A better solution is to use block decomposition again; now it will be the third time when we make use of it.</p>
</div>
<div class="paragraph">
<p>The idea is to reinterpret 8x8 matrix recursively as 2x2 block matrices.
This requires log<sub>2</sub>(8)=3 levels of block decomposition.
At first we reinterpret 8x8 matrix as 2x2 block matrix with blocks of size 4x4 as its elements.
Next we reinterpret each of these blocks as 2x2 matrix with 2x2 blocks as its elements.
Finally each 2x2 block is a 2x2 block matrix itself.
Transposing all 2x2 block matrices at all three levels (1+4+16=21 matrices in total)
is equivalent to transposing original 8x8 matrix thanks to block property.
We are free to select any order in which to process block matrices: small matrices first, large matrices first, or any mix of thereof.
Image below demonstrates the progress of the algorithm "bottom-to-top", starting from the smallest matrices.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="blocktree.png" alt="blocktree" width="80%">
</div>
</div>
<div class="paragraph">
<p>Note an important fact that at every level we have only 2x2 block matrices,
although with blocks of different sizes.
This is beneficial for us because every output row of 2x2 block transpose depends only
on two input rows, and this holds true for every block size.
Look, for example, at row <code>b0</code>.
It can be easily computed by applying bitwise operations to rows <code>a0</code> and <code>a1</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><span></span><span class="tok-n">b0</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a0</span> <span class="tok-o">&amp;</span> <span class="tok-n">ZERO_ODD_BYTES</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a1</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-n">SHIFT_BY_ONE_BYTE</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-n">ZERO_EVEN_BYTES</span><span class="tok-p">)</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Similarly, row <code>d0</code> depends only on rows <code>c0</code> and <code>c4</code> but requires different masks and shift values.
In this manner we can do all computations row by row, thereby working on multiple matrices in parallel.
Total complexity of such approach is α*8*log<sub>2</sub>(8), which is significantly faster
than β*8*8 complexity of the naive transposer, even if we take into account that α is slightly larger than β
because of a more complicated instruction sequence per each step.
We will call our new transposer Vec64 to highlight that we process multiple elements in parallel.
Obviously, the benefits grow as word width increases.
For 32-bit architecture (decomposition into 2 levels) the benefits over naive transposer would diminish,
while for hypothetical 128-bit architecture (4 levels) they would be even more prominent.</p>
</div>
<div class="paragraph">
<p>Another challenge of our new algorithm is where to place prefetch calls.
Vec64 is not very homogenic:
it is hard to split it equally into 64 equal parts without complicating code too much.
One of the solutions is to issue prefetch calls in batches of eight,
hoping that such size is not very big to cause any performance issues.</p>
</div>
<div class="paragraph">
<p>Full listing of our new transposer is presented below.
There are two versions in fact: the faster 64-bit and the slower 32-bit.
It would be natural to code such regular algorithm with a bunch of macros and templates
to avoid trivial mistakes.
However, I kept the code uncondensed for ease of comprehension.
While reading the listing, keep in mind that x86_64 is a little endian architecture.
Left-most elements in the image of 8x8 transposer are loaded into least significant bytes of the 64-bit registers.
That&#8217;s why all masks and directions of shifts are also reversed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><table class="linenotable"><tr><td class="linenos"><div class="linenodiv"><pre>  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120</pre></div></td><td class="code"><pre><span></span><span class="tok-k">template</span><span class="tok-o">&lt;</span><span class="tok-k">typename</span> <span class="tok-nc">W</span><span class="tok-o">&gt;</span> <span class="tok-cm">/* W is either u32 or u64 and defines word size */</span>
<span class="tok-kt">void</span> <span class="tok-n">transpose_Vec_kernel</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">,</span> <span class="tok-kt">int64_t</span> <span class="tok-n">stride</span><span class="tok-p">);</span>

<span class="tok-k">template</span><span class="tok-o">&lt;&gt;</span>
<span class="tok-kt">void</span> <span class="tok-n">transpose_Vec_kernel</span><span class="tok-o">&lt;</span><span class="tok-kt">uint32_t</span><span class="tok-o">&gt;</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">,</span> <span class="tok-kt">int64_t</span> <span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-c1">// load rows of src matrix</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">a0</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint32_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">0</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">a1</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint32_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">1</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">a2</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint32_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">2</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">a3</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint32_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">3</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>

  <span class="tok-c1">// 2x2 block matrices</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">b0</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a0</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ffU</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a1</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00U</span><span class="tok-p">);</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">b1</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a1</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00U</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a0</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ffU</span><span class="tok-p">);</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">b2</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a2</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ffU</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a3</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00U</span><span class="tok-p">);</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">b3</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a3</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00U</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a2</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ffU</span><span class="tok-p">);</span>

  <span class="tok-c1">// 4x4 block matrices</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">c0</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b0</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffffU</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b2</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000U</span><span class="tok-p">);</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">c1</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b1</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffffU</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b3</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000U</span><span class="tok-p">);</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">c2</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b2</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000U</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b0</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffffU</span><span class="tok-p">);</span>
  <span class="tok-kt">uint32_t</span> <span class="tok-n">c3</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b3</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000U</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b1</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffffU</span><span class="tok-p">);</span>

  <span class="tok-c1">// write to dst matrix</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint32_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">0</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">c0</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint32_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">1</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">c1</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint32_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">2</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">c2</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint32_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">3</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">c3</span><span class="tok-p">;</span>
<span class="tok-p">}</span>


<span class="tok-k">template</span><span class="tok-o">&lt;&gt;</span>
<span class="tok-kt">void</span> <span class="tok-n">transpose_Vec_kernel</span><span class="tok-o">&lt;</span><span class="tok-kt">uint64_t</span><span class="tok-o">&gt;</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">,</span> <span class="tok-kt">int64_t</span> <span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-c1">// load rows of src matrix</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">a0</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">0</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">a1</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">1</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">a2</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">2</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">a3</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">3</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">a4</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">4</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">a5</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">5</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">a6</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">6</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">a7</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">((</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src</span><span class="tok-o">+</span><span class="tok-mi">7</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">));</span>

  <span class="tok-c1">// 2x2 block matrices</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">b0</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a0</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ff00ff00ffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a1</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00ff00ff00ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">b1</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a1</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00ff00ff00ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a0</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ff00ff00ffULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">b2</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a2</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ff00ff00ffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a3</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00ff00ff00ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">b3</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a3</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00ff00ff00ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a2</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ff00ff00ffULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">b4</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a4</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ff00ff00ffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a5</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00ff00ff00ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">b5</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a5</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00ff00ff00ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a4</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ff00ff00ffULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">b6</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a6</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ff00ff00ffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a7</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00ff00ff00ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">b7</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">a7</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xff00ff00ff00ff00ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">a6</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">8</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00ff00ff00ff00ffULL</span><span class="tok-p">);</span>

  <span class="tok-c1">// 4x4 block matrices</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">c0</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b0</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffff0000ffffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b2</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000ffff0000ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">c1</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b1</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffff0000ffffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b3</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000ffff0000ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">c2</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b2</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000ffff0000ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b0</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffff0000ffffULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">c3</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b3</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000ffff0000ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b1</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffff0000ffffULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">c4</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b4</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffff0000ffffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b6</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000ffff0000ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">c5</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b5</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffff0000ffffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b7</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000ffff0000ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">c6</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b6</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000ffff0000ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b4</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffff0000ffffULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">c7</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">b7</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffff0000ffff0000ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">b5</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">16</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x0000ffff0000ffffULL</span><span class="tok-p">);</span>

  <span class="tok-c1">// 8x8 block matrix</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">d0</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">c0</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00000000ffffffffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">c4</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">32</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffffffff00000000ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">d1</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">c1</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00000000ffffffffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">c5</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">32</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffffffff00000000ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">d2</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">c2</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00000000ffffffffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">c6</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">32</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffffffff00000000ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">d3</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">c3</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00000000ffffffffULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">c7</span> <span class="tok-o">&lt;&lt;</span> <span class="tok-mi">32</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffffffff00000000ULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">d4</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">c4</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffffffff00000000ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">c0</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">32</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00000000ffffffffULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">d5</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">c5</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffffffff00000000ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">c1</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">32</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00000000ffffffffULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">d6</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">c6</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffffffff00000000ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">c2</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">32</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00000000ffffffffULL</span><span class="tok-p">);</span>
  <span class="tok-kt">uint64_t</span> <span class="tok-n">d7</span> <span class="tok-o">=</span> <span class="tok-p">(</span><span class="tok-n">c7</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0xffffffff00000000ULL</span><span class="tok-p">)</span> <span class="tok-o">|</span> <span class="tok-p">((</span><span class="tok-n">c3</span> <span class="tok-o">&gt;&gt;</span> <span class="tok-mi">32</span><span class="tok-p">)</span> <span class="tok-o">&amp;</span> <span class="tok-mh">0x00000000ffffffffULL</span><span class="tok-p">);</span>

  <span class="tok-c1">// write to dst matrix</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">0</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">d0</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">1</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">d1</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">2</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">d2</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">3</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">d3</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">4</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">d4</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">5</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">d5</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">6</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">d6</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-kt">uint64_t</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst</span> <span class="tok-o">+</span> <span class="tok-mi">7</span><span class="tok-o">*</span><span class="tok-n">stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">d7</span><span class="tok-p">;</span>
<span class="tok-p">}</span>

<span class="tok-k">template</span><span class="tok-o">&lt;</span><span class="tok-k">typename</span> <span class="tok-nc">W</span><span class="tok-o">&gt;</span>
<span class="tok-kt">void</span> <span class="tok-n">transpose_Vec</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">n</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">n</span><span class="tok-p">();</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">bsize</span> <span class="tok-o">=</span> <span class="tok-mi">64</span><span class="tok-p">;</span>

  <span class="tok-c1">// iterate over 64x64 block matrices</span>
  <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">rb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">rb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">rb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
    <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">cb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">cb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-n">bsize</span><span class="tok-p">;</span> <span class="tok-n">cb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
      <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">srcb_origin</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">data</span><span class="tok-p">()</span>  <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">rb</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-n">cb</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-n">bsize</span><span class="tok-p">;</span>
            <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dstb_origin</span> <span class="tok-o">=</span> <span class="tok-n">dst</span><span class="tok-o">-&gt;</span><span class="tok-n">data</span><span class="tok-p">()</span> <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">cb</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-n">rb</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-n">bsize</span><span class="tok-p">;</span>
      <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">prfb_origin</span> <span class="tok-o">=</span> <span class="tok-n">next_block</span><span class="tok-p">(</span><span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">rb</span><span class="tok-p">,</span> <span class="tok-n">cb</span><span class="tok-p">);</span>

      <span class="tok-c1">// iterate over sizeof(W)xsizeof(W) block matrices inside 64x64 block</span>
      <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">size_t</span> <span class="tok-n">rw</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">rw</span> <span class="tok-o">&lt;</span> <span class="tok-mi">64</span><span class="tok-o">/</span><span class="tok-k">sizeof</span><span class="tok-p">(</span><span class="tok-n">W</span><span class="tok-p">);</span> <span class="tok-n">rw</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
        <span class="tok-c1">// preload sizeof(W) rows of the next 64x64 block</span>
        <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">size_t</span> <span class="tok-n">i</span> <span class="tok-o">=</span> <span class="tok-n">rw</span><span class="tok-o">*</span><span class="tok-k">sizeof</span><span class="tok-p">(</span><span class="tok-n">W</span><span class="tok-p">);</span> <span class="tok-n">i</span> <span class="tok-o">&lt;</span> <span class="tok-p">(</span><span class="tok-n">rw</span><span class="tok-o">+</span><span class="tok-mi">1</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-k">sizeof</span><span class="tok-p">(</span><span class="tok-n">W</span><span class="tok-p">);</span> <span class="tok-n">i</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
          <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prfb_origin</span> <span class="tok-o">+</span> <span class="tok-n">i</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
        <span class="tok-p">}</span>
        <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">size_t</span> <span class="tok-n">cw</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">cw</span> <span class="tok-o">&lt;</span> <span class="tok-mi">64</span><span class="tok-o">/</span><span class="tok-k">sizeof</span><span class="tok-p">(</span><span class="tok-n">W</span><span class="tok-p">);</span> <span class="tok-n">cw</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
          <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">srcw_origin</span> <span class="tok-o">=</span> <span class="tok-n">srcb_origin</span> <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">cw</span><span class="tok-o">*</span><span class="tok-n">n</span> <span class="tok-o">+</span> <span class="tok-n">rw</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-k">sizeof</span><span class="tok-p">(</span><span class="tok-n">W</span><span class="tok-p">);</span>
                <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dstw_origin</span> <span class="tok-o">=</span> <span class="tok-n">dstb_origin</span> <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">rw</span><span class="tok-o">*</span><span class="tok-n">n</span> <span class="tok-o">+</span> <span class="tok-n">cw</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-k">sizeof</span><span class="tok-p">(</span><span class="tok-n">W</span><span class="tok-p">);</span>
          <span class="tok-c1">// use vector kernel to transpose sizeof(W)xsizeof(W) matrix</span>
          <span class="tok-n">transpose_Vec_kernel</span><span class="tok-o">&lt;</span><span class="tok-n">W</span><span class="tok-o">&gt;</span><span class="tok-p">(</span><span class="tok-n">srcw_origin</span><span class="tok-p">,</span> <span class="tok-n">dstw_origin</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">);</span>
        <span class="tok-p">}</span>
      <span class="tok-p">}</span>
    <span class="tok-p">}</span>
  <span class="tok-p">}</span>
<span class="tok-p">}</span>

<span class="tok-kt">void</span> <span class="tok-n">transpose_Vec32</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-n">transpose_Vec</span><span class="tok-o">&lt;</span><span class="tok-kt">uint32_t</span><span class="tok-o">&gt;</span><span class="tok-p">(</span><span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">dst</span><span class="tok-p">);</span>
<span class="tok-p">}</span>

<span class="tok-kt">void</span> <span class="tok-n">transpose_Vec64</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-n">transpose_Vec</span><span class="tok-o">&lt;</span><span class="tok-kt">uint64_t</span><span class="tok-o">&gt;</span><span class="tok-p">(</span><span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">dst</span><span class="tok-p">);</span>
<span class="tok-p">}</span>
</pre></td></tr></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>The outcome is x1.8 boost in performance compared to the previous block prefetch algorithm.
Now it takes less than one cycle to process single element.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 40%;">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">&nbsp;</th>
<th class="tableblock halign-right valign-top">cpe (N=320)</th>
<th class="tableblock halign-right valign-top">cpe (N=2112)</th>
<th class="tableblock halign-right valign-top">cpe (N=46400)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">BlocksPrf</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.36</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.35</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.41</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Vec32</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.85</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.85</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.89</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Vec64</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.76</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.74</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.80</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>As the final remark I would like to emphasize that in this algorithm we used block decomposition three times,
and every time we did it for different reasons:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Processing original <code>src</code> and <code>dst</code> matrices as a sequence of 64x64 blocks makes caching perfect</p>
</li>
<li>
<p>Decomposition of 64x64 blocks into 8x8 matrices allows to hold entire rows in 64-bit variables</p>
</li>
<li>
<p>Finally, multi-level decomposition of 8x8 blocks into 2x2 matrices allows to transpose
multiple matrices in parallel using two-argument bitwise operations</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>It should be obvious by now that dividing matrices into blocks and processing them independently is the key
to designing any efficient matrix algorithm, not only transposer.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_256_bit_simd_0_49">256-bit SIMD [0.49]</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As was mentioned earlier, block algorithm becomes more efficient as word size increases.
General purpose registers are only 64 bit long, so there is nothing more we can do about them.
On the other hand, x86_64 carries dedicated SIMD registers and instructions
with widths of 128, 256 and 512 bits depending on CPU generation.
For example, with 256 bit registers we could create efficient 32×32 transposer
by using log<sub>2</sub>(32)=5 levels of 2×2 block decomposition.
Alas, working with SIMD extensions is much more challenging than issuing <code>+</code> or <code>&gt;&gt;</code>.
A multitude of high-level instructions allows to do the same thing in different ways
and with different performance results.
Atop of that, availability of SIMD extensions heavily depends on CPU architecture and generation.
Here we will focus on relatively modern generations of x86_64 CPUs.</p>
</div>
<div class="paragraph">
<p>First let&#8217;s do short review of what SIMD extensions x64_64 CPUs offer.
The very first widely used SIMD extension for x86 CPUs was MMX ("multimedia extensions"), now already forgotten.
It introduced a set of new registers which could be treated as vectors of 8/16/32-bit integers
and a dedicated instruction set that allowed to perform operations on the elements of these registers in parallel.
As time passed, more and more extensions were added.
Newly released extensions expanded register width, increased number of registers or introduced brand new instructions.
Extensions below are grouped by target vector width and are listed in historical order:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>SSE family (<span class="small">SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2</span>): 128-bit registers, universal availability.
Full set of SSE extensions provides a multitude of instructions that treat 128-bit registers
as vectors of 32/64-bit floating point or 8/16/32/64 integer values.
It would be hard to find a CPU without SSE support as it is already two decades old.
In fact, most widely used x86_64 ABIs (application binary interfaces) mandate
that floating point numbers are to be passed to functions in SSE registers
rather than de-facto obsolete FPU.
As such, SSE-capable CPU is mandatory for running modern applications.</p>
</li>
<li>
<p>AVX family (<span class="small">AVX, AVX2</span>): 256-bit registers, availability is almost universal.
When original AVX was introduced, it targeted only floating point numbers.
AVX2 later fixed this by adding missing integer instructions.
All CPUs manufactured after approximately 2015 support both original AVX and AVX2 extensions.
It is perfectly safe to use all AVX instructions in server environment,
however a runtime check and a separate non-AVX algorithm is advised if you target application for workstations.</p>
</li>
<li>
<p>AVX-512 (<span class="small">AVX-512F, AVX-512VL, AVX-512BW and a dozen of others</span>)
family further expands the width of the registers, now to 512 bits.
However, the availability of CPUs with AVX-512 is still not universal as of 2023.
Besides, the whole family of AVX-512 family was fragmented into more than a dozen
of small extensions since its inception.
The most common extensions are supported by all AVX-512-advertised CPUs,
but support for other extensions is flappy:
recently manufactured CPUs may lack some extensions supported by CPUs of previous generations.
This leaves the choice of selecting which instructions are safe to use
and which should be avoided to program authors.</p>
</li>
<li>
<p>AMX is the most recent extension relevant to our problem.
It defines high-level operations on small matrices, such as matrix multiplication.
Its registers ("tiles") are two dimensional, 1 KiB each and must be configured before use.
Configuration specifies effective number of rows (≤16) and row size (≤64 bytes) of the tile.
Unlike vector extensions, AMX acts directly on matrices and therefore is particularly promising for
the problem at hand.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>I will assume that we have access to AVX2 but not the newer extensions.
AVX2 gives us access to 256 bit registers and integer instructions.
This means that we will be able to create efficient 32x32 transposer by using log<sub>2</sub>(32)=5 levels of block decomposition.</p>
</div>
<div class="paragraph">
<p>As before, we will work level by level, row by row.
Every row will be represented by a variable of <code>__m256i</code> type, which we will treat as a vector of type <code>uint8_t[32]</code>.
This type is defined in <code>immintrin.h</code> header, shipped with all major compilers (intel, gcc, clang) which support AVX2.
Variable of this type can be naturally mapped to a 256-bit AVX register by the compiler in the same
way as <code>uint64_t</code> variable can be mapped to a general purpose 64-bit register.
The same header also provides the intrinsics to work with these registers.
Each intrinsic is mapped to respective CPU instruction, thus making it unnecessary to manually write assembler code.
In our code we will rely on only three instructions: <code>_mm256_shuffle_epi8</code>, <code>_mm256_blendv_epi8</code>, <code>_mm256_permute2x128_si256</code>.
In addition, two more instructions will be used implicitly: <code>_mm256_load_si256</code> and <code>_mm256_store_si256</code>.
They move data between memory and 256-bit registers.
We do not need to call them manually since compilers can insert such instructions automatically
on encountering <code>_mm256i r = *ptr;</code> and <code>*ptr = r;</code> respectively.
Semantics of the other three instructions is more complicated and is provided below.</p>
</div>
<div class="paragraph">
<p><strong>Shuffle.</strong>
<a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_shuffle_epi8">_mm256_shuffle_epi8</a>
instruction accepts a source 256-bit register and a 256-bit control register, and, treating input
as a vector of 8-bit values, shuffles them according to the control register, returning the result.
Control register specifies for every index (0..31) of the output vector either the index
of the source vector where to copy an element from, or carries a special
value indicating that target location must be zeroed.
Such semantics makes shuffle a very powerful instruction.
If this instruction didn&#8217;t have any limitations, it would be possible to do any of the following
with just single short-running instruction:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>reverse the order of the elements</p>
</li>
<li>
<p>perform cyclic shift left or right</p>
</li>
<li>
<p>perform left or right zero padded shift</p>
</li>
<li>
<p>swap pairs of adjacent elements</p>
</li>
<li>
<p>create zero vector</p>
</li>
<li>
<p>broadcast one element into all the others</p>
</li>
<li>
<p>apply arbitrary permutation</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In contrast, coding the same operations with general purpose instructions would take a long sequence of mask,
shift and OR instructions.
Alas, <code>_mm256_shuffle_epi8</code> has one limitation: elements may be shuffled only within every
left and right 128-bit <em>lane</em> but not across the lanes.
It is not possible, for example, to copy an element from <code>src[30]</code> (left lane) to <code>dst[12]</code> (right lane).
So, technically speaking, <code>_mm256_shuffle_epi8</code> is a pair of independent general shuffle operations applied to left and right 128-bit lanes.</p>
</div>
<div class="paragraph">
<p>In our algorithm, we will use shuffle instruction in the first four stages (out of five) to reposition elements
of the registers holding rows of the matrix from the previous stage.
In the first level, we swap adjacent elements, in the second level&#8201;&#8212;&#8201;adjacent groups of two elements each, and so on.
Diagram below demonstrates the application of the shuffle during the third stage, when we swap adjacent groups of four elements each.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="shuffle.png" alt="shuffle" width="60%">
</div>
</div>
<div class="paragraph">
<p><strong>Permute.</strong>
In the fifth stage of the algorithm we are transposing 2x2 block matrices where each block is 16x16,
effectively meaning that we have to swap lanes.
There is not way in AVX2 to perform general shuffle that requires to move data between lanes with a single instruction.
But since our case is very special one&#8201;&#8212;&#8201;just swap the lanes&#8201;&#8212;&#8201;we can do this with another instruction&#8201;&#8212;&#8201;<a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_permute2x128_si256">_mm256_permute2x128_si256</a>.
Given two input data registers <code>a</code> and <code>b</code> and a control register, this instruction allows to construct
a register where each of the two 128-bit lanes takes any of the four options:
<code>a[0:127]</code>, <code>a[128:255]</code>, <code>b[0:127]</code>, <code>b[128:255]</code>.
Setting control register to 0x01 makes permute instruction to return <code>a</code> with swapped lanes; <code>b</code> can be anything in our case.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="permute.png" alt="permute" width="60%">
</div>
</div>
<div class="paragraph">
<p><strong>Blend.</strong>
<a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm256_blendv_epi8">Blend</a>
is another highly useful instruction.
It accepts two source registers and a mask, and constructs result where every <code>i</code>-th element
is selected from either <code>src1[i]</code> or <code>src2[i]</code> based on the mask.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="blendv.png" alt="blendv" width="60%">
</div>
</div>
<div class="paragraph">
<p>In total, a combination of shuffle/permute and blend allows us to build a value consisting of almost
arbitrary selection of elements (remember cross-lane restriction) from two input registers, <code>a</code> and <code>b</code>,
in the following way:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Shuffle elements of <code>a</code> by placing elements of interest to the positions where they should be in the result</p>
</li>
<li>
<p>Shuffle elements of <code>b</code></p>
</li>
<li>
<p>Blend two properly shuffled intermediate values to form the result</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Using this rule we are ready to build 32x32 transposer.
There will be log2(32)=5 levels.
Recall that at each level we build and intermediate matrix where each row is the result
of computations applied to two rows of the matrix from the previous level.
Every such computation can be expressed by the same sequence of operations.
The difference will be in which rows we take as input, which masks we apply, and
on the final level instead of shuffle we have to use permute.
Image below demonstrates how to compute the first two rows on the very first stage,
when we transpose each 2x2 matrix inside 32x32 matrix.
What originally was <span class="image"><img src="level_a.png" alt="level a" width="32px"></span>, now becomes <span class="image"><img src="level_b.png" alt="level b" width="32px"></span>.
And what was <span class="image"><img src="level_c.png" alt="level c" width="32px"></span>, now becomes <span class="image"><img src="level_d.png" alt="level d" width="32px"></span>.
At the second stage the same sequence of operations applies, but now we work on pairs of elements.
At the fifth stage we are working on groups of 16 elements, and instead of shuffle we use permute.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="level.png" alt="level" width="100%">
</div>
</div>
<div class="paragraph">
<p>On the global scale full sequence of computations can be expressed with the following network.
Each vertex corresponds to a 256-bit value that holds entire row of 32 elements of some matrix,
additionally associated with the instruction that was used to compute this value.
Edges encode dependencies which must be computed before we can compute target value itself.
In the first column of vertices we load the rows of the <code>src</code> matrix.
Next five pairs of columns correspond to five stages of the algorithm.
First column in each pair is the result of shuffle (or permute), second column&#8201;&#8212;&#8201;blendv.
Thus, the values computed in the penultimate column contain the rows of the transposed matrix,
and the final column is dedicated to saving them into the memory that backs <code>dst</code>.
Top vertices standing apart from the remaining network correspond to the loading of 256-bit masks,
which we need to pass to shuffle and blendv.
These masks should be computed beforehand.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="compgraph.png" alt="compgraph">
</div>
</div>
<div class="paragraph">
<p>Writing code for such complex transposer is not a simple task.
Manual coding in entirely unrolled format (as Vec64) is out of the question because it would require to write more than
400 lines of highly repetitive code.
Using nested loops (level by level, block by block, row by row) is definitely possible and the transposer would be quite compact,
but its performance would be at the mercy of the compiler.
For example, its choice which loops to unroll and to what extent will have impact on how well
execution units will be loaded.
Third alternative is to use code generation with the help of some scripting language,
and probably it is the cleanest way to create medium-sized kernels such as ours.</p>
</div>
<div class="paragraph">
<p>To make use of code generation, we need to explicitly instantiate the above network in terms of vertices and edges.
With each vertex we associate a piece of code that consists of executing single vector instruction
and writing the result into uniquely-named variable.
Arguments of the instructions are the variables associated with the left-adjacent vertices.</p>
</div>
<div class="paragraph">
<p>One of the benefits of such approach is that it is possible to play with different
orderings of the vertices.
Any valid topological sorting will produce the same result but performance may vary.
Computations may be performed column by column, in a n-ary tree manner, according to a randomly
generated topological sorting, or in any other clever way you can think of.
A priory it is hard to say which sorting will be better in terms of performance due to microarchitectural details.
Still, some general rules can be applied.
On the one hand, we would like to generate code that at any step keeps number of already
computed and still required in the future values as few as possible.
This is because compiler maps each variable to some register, and when it runs out
of registers (for AVX2, there are only 16 of them), it has to perform a <em>spill</em>:
a piece of code that saves value into stack and restores its back later when it is needed.
Obviously, the fewer spills&#8201;&#8212;&#8201;the better.
Another thing we need to consider is that invocations of the same instruction should be spread.
Issuing 32 identical instructions one by one is not the best idea,
since for each instruction type, there is only a few of the execution units which can handle it.
If we issue many identical instructions one by one, other execution units will starve for work.
Much better idea is to interleave instructions of different types, raising the chances
that they will be executed in parallel.</p>
</div>
<div class="paragraph">
<p>Long story short, the solution which appears to work well empirically is to perform
all computations level-by-level, similar to scalar vectorization.
At every step two rows of the matrix from the previous level serve as input
and four instructions are applied to them:
two shuffles and two blends, thus producing two rows of the next-level matrix.
This is repeated level by level, pair of rows by pair of rows.
The order in which values are computed according to these rules is denoted by the numbers
near the vertices in the network above.</p>
</div>
<div class="paragraph">
<p>Code-generated 32x32 transposer must be plugged into the overall algorithm, again with carefully
chosen strategy for preloading.
Since we employ code generation, the natural choice is to spread all 16 preload instructions
at equally-long chunks of the generated code.
Why 16?
Recall that we need 4 invocations of the 32x32 transposer to finish single 64x64 block.
Hence during each invocation we need to preload 16 rows of the next block.
There are 393 instructions in total, so we insert preload each 393/16=24 instructions into generated code.
In the code sample, three <code>origin</code> variables correspond to the offsets of the 32x32 submatrices
we are working on: source matrix, destination, and prefetch respectively.
<code>stride</code> values indicate the number of bytes that must be added to a corresponding
pointer to make it point to the same column of the next row.
For our current transposer both strides must be set to <code>N</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><table class="linenotable"><tr><td class="linenos"><div class="linenodiv"><pre>  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429</pre></div></td><td class="code"><pre><span></span><span class="tok-kt">void</span> <span class="tok-nf">transpose_Vec256_kernel</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">src_origin</span><span class="tok-p">,</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dst_origin</span><span class="tok-p">,</span>
                             <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">prf_origin</span><span class="tok-p">,</span>
                             <span class="tok-kt">int</span> <span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-kt">int</span> <span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shm_1</span> <span class="tok-o">=</span> <span class="tok-n">SHUFFLE_MASK</span><span class="tok-p">[</span><span class="tok-mi">0</span><span class="tok-p">];</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">blm_1</span> <span class="tok-o">=</span> <span class="tok-n">BLENDV_MASK</span><span class="tok-p">[</span><span class="tok-mi">0</span><span class="tok-p">];</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_0</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">0</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_1</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">1</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_0</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_1</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_0</span><span class="tok-p">,</span> <span class="tok-n">shf_1_1</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_0</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_1</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_2</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">2</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_3</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">3</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_2</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_3</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_2</span><span class="tok-p">,</span> <span class="tok-n">shf_1_3</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_2</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_3</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_4</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">4</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_5</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">5</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_4</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_5</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_4</span><span class="tok-p">,</span> <span class="tok-n">shf_1_5</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_4</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_5</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_6</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">6</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_7</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">7</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_6</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_7</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">0</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_6</span><span class="tok-p">,</span> <span class="tok-n">shf_1_7</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_6</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_7</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_8</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">8</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_9</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">9</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_8</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_9</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_8</span><span class="tok-p">,</span> <span class="tok-n">shf_1_9</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_8</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_9</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_10</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">10</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_11</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">11</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_10</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_11</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_10</span><span class="tok-p">,</span> <span class="tok-n">shf_1_11</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_10</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_11</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_12</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">12</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_13</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">13</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_12</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_13</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_12</span><span class="tok-p">,</span> <span class="tok-n">shf_1_13</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_12</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_13</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_14</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">14</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_15</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">15</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_14</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_15</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">1</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_14</span><span class="tok-p">,</span> <span class="tok-n">shf_1_15</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_14</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_15</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_16</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">16</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_17</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">17</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_16</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_17</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_16</span><span class="tok-p">,</span> <span class="tok-n">shf_1_17</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_16</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_17</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_18</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">18</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_19</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">19</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_18</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_19</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_18</span><span class="tok-p">,</span> <span class="tok-n">shf_1_19</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_18</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_19</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_20</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">20</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_21</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">21</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_20</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_21</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_20</span><span class="tok-p">,</span> <span class="tok-n">shf_1_21</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_20</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_21</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_22</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">22</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_23</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">23</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_22</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_23</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">2</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_22</span><span class="tok-p">,</span> <span class="tok-n">shf_1_23</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_22</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_23</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_24</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">24</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_25</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">25</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_24</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_25</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_24</span><span class="tok-p">,</span> <span class="tok-n">shf_1_25</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_24</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_25</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_26</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">26</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_27</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">27</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_26</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_27</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_26</span><span class="tok-p">,</span> <span class="tok-n">shf_1_27</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_26</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_27</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_28</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">28</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_29</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">29</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_28</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_29</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_28</span><span class="tok-p">,</span> <span class="tok-n">shf_1_29</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_28</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_29</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_30</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">30</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_0_31</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">src_origin</span> <span class="tok-o">+</span> <span class="tok-mi">31</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_30</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_1_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_31</span><span class="tok-p">,</span> <span class="tok-n">shm_1</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">3</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_0_30</span><span class="tok-p">,</span> <span class="tok-n">shf_1_31</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_1_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_1_30</span><span class="tok-p">,</span> <span class="tok-n">rnd_0_31</span><span class="tok-p">,</span> <span class="tok-n">blm_1</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shm_2</span> <span class="tok-o">=</span> <span class="tok-n">SHUFFLE_MASK</span><span class="tok-p">[</span><span class="tok-mi">1</span><span class="tok-p">];</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">blm_2</span> <span class="tok-o">=</span> <span class="tok-n">BLENDV_MASK</span><span class="tok-p">[</span><span class="tok-mi">1</span><span class="tok-p">];</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_0</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_2</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_0</span><span class="tok-p">,</span> <span class="tok-n">shf_2_2</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_0</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_2</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_1</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_3</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_1</span><span class="tok-p">,</span> <span class="tok-n">shf_2_3</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_1</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_3</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_4</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_6</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_4</span><span class="tok-p">,</span> <span class="tok-n">shf_2_6</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_4</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_6</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_5</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_7</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_5</span><span class="tok-p">,</span> <span class="tok-n">shf_2_7</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_5</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_7</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_8</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_10</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_8</span><span class="tok-p">,</span> <span class="tok-n">shf_2_10</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_8</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_10</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">4</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_9</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_11</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_9</span><span class="tok-p">,</span> <span class="tok-n">shf_2_11</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_9</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_11</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_12</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_14</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_12</span><span class="tok-p">,</span> <span class="tok-n">shf_2_14</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_12</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_14</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_13</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_15</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_13</span><span class="tok-p">,</span> <span class="tok-n">shf_2_15</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_13</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_15</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_16</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_18</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_16</span><span class="tok-p">,</span> <span class="tok-n">shf_2_18</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_16</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_18</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_17</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_19</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_17</span><span class="tok-p">,</span> <span class="tok-n">shf_2_19</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_17</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_19</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_20</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_22</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_20</span><span class="tok-p">,</span> <span class="tok-n">shf_2_22</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_20</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_22</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">5</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_21</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_23</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_21</span><span class="tok-p">,</span> <span class="tok-n">shf_2_23</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_21</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_23</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_24</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_26</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_24</span><span class="tok-p">,</span> <span class="tok-n">shf_2_26</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_24</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_26</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_25</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_27</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_25</span><span class="tok-p">,</span> <span class="tok-n">shf_2_27</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_25</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_27</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_28</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_30</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_28</span><span class="tok-p">,</span> <span class="tok-n">shf_2_30</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_28</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_30</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_29</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_2_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_31</span><span class="tok-p">,</span> <span class="tok-n">shm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_1_29</span><span class="tok-p">,</span> <span class="tok-n">shf_2_31</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_2_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_2_29</span><span class="tok-p">,</span> <span class="tok-n">rnd_1_31</span><span class="tok-p">,</span> <span class="tok-n">blm_2</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shm_3</span> <span class="tok-o">=</span> <span class="tok-n">SHUFFLE_MASK</span><span class="tok-p">[</span><span class="tok-mi">2</span><span class="tok-p">];</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">blm_3</span> <span class="tok-o">=</span> <span class="tok-n">BLENDV_MASK</span><span class="tok-p">[</span><span class="tok-mi">2</span><span class="tok-p">];</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_0</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_4</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">6</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_0</span><span class="tok-p">,</span> <span class="tok-n">shf_3_4</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_0</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_4</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_1</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_5</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_1</span><span class="tok-p">,</span> <span class="tok-n">shf_3_5</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_1</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_5</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_2</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_6</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_2</span><span class="tok-p">,</span> <span class="tok-n">shf_3_6</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_2</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_6</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_3</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_7</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_3</span><span class="tok-p">,</span> <span class="tok-n">shf_3_7</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_3</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_7</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_8</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_12</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_8</span><span class="tok-p">,</span> <span class="tok-n">shf_3_12</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_8</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_12</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_9</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_13</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_9</span><span class="tok-p">,</span> <span class="tok-n">shf_3_13</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_9</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_13</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_10</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_14</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">7</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_10</span><span class="tok-p">,</span> <span class="tok-n">shf_3_14</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_10</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_14</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_11</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_15</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_11</span><span class="tok-p">,</span> <span class="tok-n">shf_3_15</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_11</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_15</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_16</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_20</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_16</span><span class="tok-p">,</span> <span class="tok-n">shf_3_20</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_16</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_20</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_17</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_21</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_17</span><span class="tok-p">,</span> <span class="tok-n">shf_3_21</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_17</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_21</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_18</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_22</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_18</span><span class="tok-p">,</span> <span class="tok-n">shf_3_22</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_18</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_22</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_19</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_23</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_19</span><span class="tok-p">,</span> <span class="tok-n">shf_3_23</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_19</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_23</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_24</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_28</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">8</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_24</span><span class="tok-p">,</span> <span class="tok-n">shf_3_28</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_24</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_28</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_25</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_29</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_25</span><span class="tok-p">,</span> <span class="tok-n">shf_3_29</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_25</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_29</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_26</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_30</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_26</span><span class="tok-p">,</span> <span class="tok-n">shf_3_30</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_26</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_30</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_27</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_3_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_31</span><span class="tok-p">,</span> <span class="tok-n">shm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_2_27</span><span class="tok-p">,</span> <span class="tok-n">shf_3_31</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_3_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_3_27</span><span class="tok-p">,</span> <span class="tok-n">rnd_2_31</span><span class="tok-p">,</span> <span class="tok-n">blm_3</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shm_4</span> <span class="tok-o">=</span> <span class="tok-n">SHUFFLE_MASK</span><span class="tok-p">[</span><span class="tok-mi">3</span><span class="tok-p">];</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">blm_4</span> <span class="tok-o">=</span> <span class="tok-n">BLENDV_MASK</span><span class="tok-p">[</span><span class="tok-mi">3</span><span class="tok-p">];</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_0</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_8</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_0</span><span class="tok-p">,</span> <span class="tok-n">shf_4_8</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_0</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_8</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_1</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_9</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_1</span><span class="tok-p">,</span> <span class="tok-n">shf_4_9</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_1</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_9</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">9</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_2</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_10</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_2</span><span class="tok-p">,</span> <span class="tok-n">shf_4_10</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_2</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_10</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_3</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_11</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_3</span><span class="tok-p">,</span> <span class="tok-n">shf_4_11</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_3</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_11</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_4</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_12</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_4</span><span class="tok-p">,</span> <span class="tok-n">shf_4_12</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_4</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_12</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_5</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_13</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_5</span><span class="tok-p">,</span> <span class="tok-n">shf_4_13</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_5</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_13</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_6</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_14</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_6</span><span class="tok-p">,</span> <span class="tok-n">shf_4_14</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_6</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_14</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_7</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_15</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_7</span><span class="tok-p">,</span> <span class="tok-n">shf_4_15</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_7</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_15</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">10</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_16</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_24</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_16</span><span class="tok-p">,</span> <span class="tok-n">shf_4_24</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_16</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_24</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_17</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_25</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_17</span><span class="tok-p">,</span> <span class="tok-n">shf_4_25</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_17</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_25</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_18</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_26</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_18</span><span class="tok-p">,</span> <span class="tok-n">shf_4_26</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_18</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_26</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_19</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_27</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_19</span><span class="tok-p">,</span> <span class="tok-n">shf_4_27</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_19</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_27</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_20</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_28</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_20</span><span class="tok-p">,</span> <span class="tok-n">shf_4_28</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_20</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_28</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_21</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_29</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_21</span><span class="tok-p">,</span> <span class="tok-n">shf_4_29</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_21</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_29</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">11</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_22</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_30</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_22</span><span class="tok-p">,</span> <span class="tok-n">shf_4_30</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_22</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_30</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_23</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_4_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_shuffle_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_31</span><span class="tok-p">,</span> <span class="tok-n">shm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_3_23</span><span class="tok-p">,</span> <span class="tok-n">shf_4_31</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_4_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_4_23</span><span class="tok-p">,</span> <span class="tok-n">rnd_3_31</span><span class="tok-p">,</span> <span class="tok-n">blm_4</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">blm_5</span> <span class="tok-o">=</span> <span class="tok-n">BLENDV_MASK</span><span class="tok-p">[</span><span class="tok-mi">4</span><span class="tok-p">];</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_0</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_0</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_16</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_16</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_0</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_0</span><span class="tok-p">,</span> <span class="tok-n">shf_5_16</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_16</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_0</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_16</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">0</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_0</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">16</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_16</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_1</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_1</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_17</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_17</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_1</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_1</span><span class="tok-p">,</span> <span class="tok-n">shf_5_17</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_17</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_1</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_17</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">1</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_1</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">17</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_17</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_2</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_2</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_18</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_18</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_2</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_2</span><span class="tok-p">,</span> <span class="tok-n">shf_5_18</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">12</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_18</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_2</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_18</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">2</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_2</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">18</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_18</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_3</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_3</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_19</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_19</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_3</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_3</span><span class="tok-p">,</span> <span class="tok-n">shf_5_19</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_19</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_3</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_19</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">3</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_3</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">19</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_19</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_4</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_4</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_20</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_20</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_4</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_4</span><span class="tok-p">,</span> <span class="tok-n">shf_5_20</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_20</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_4</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_20</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">4</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_4</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">20</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_20</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_5</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_5</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_21</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_21</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_5</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_5</span><span class="tok-p">,</span> <span class="tok-n">shf_5_21</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_21</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_5</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_21</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">5</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_5</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">21</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_21</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_6</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_6</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_22</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_22</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_6</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_6</span><span class="tok-p">,</span> <span class="tok-n">shf_5_22</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">13</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_22</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_6</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_22</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">6</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_6</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">22</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_22</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_7</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_7</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_23</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_23</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_7</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_7</span><span class="tok-p">,</span> <span class="tok-n">shf_5_23</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_23</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_7</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_23</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">7</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_7</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">23</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_23</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_8</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_8</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_24</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_24</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_8</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_8</span><span class="tok-p">,</span> <span class="tok-n">shf_5_24</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_24</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_8</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_24</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">8</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_8</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">24</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_24</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_9</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_9</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_25</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_25</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_9</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_9</span><span class="tok-p">,</span> <span class="tok-n">shf_5_25</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_25</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_9</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_25</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">9</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_9</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">25</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_25</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_10</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_10</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_26</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_26</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_10</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_10</span><span class="tok-p">,</span> <span class="tok-n">shf_5_26</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">14</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_26</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_10</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_26</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">10</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_10</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">26</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_26</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_11</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_11</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_27</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_27</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_11</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_11</span><span class="tok-p">,</span> <span class="tok-n">shf_5_27</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_27</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_11</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_27</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">11</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_11</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">27</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_27</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_12</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_12</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_28</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_28</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_12</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_12</span><span class="tok-p">,</span> <span class="tok-n">shf_5_28</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_28</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_12</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_28</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">12</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_12</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">28</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_28</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_13</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_13</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_29</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_29</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_13</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_13</span><span class="tok-p">,</span> <span class="tok-n">shf_5_29</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_29</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_13</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_29</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">13</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_13</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">29</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_29</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_14</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_14</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_30</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_30</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_14</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_14</span><span class="tok-p">,</span> <span class="tok-n">shf_5_30</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">_mm_prefetch</span><span class="tok-p">(</span><span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-mi">15</span><span class="tok-o">*</span><span class="tok-n">src_stride</span><span class="tok-p">,</span> <span class="tok-n">_MM_HINT_NTA</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_30</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_14</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_30</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">14</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_14</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">30</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_30</span><span class="tok-p">;</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_15</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_15</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">shf_5_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_permute2x128_si256</span><span class="tok-p">(</span><span class="tok-n">rnd_4_31</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_31</span><span class="tok-p">,</span> <span class="tok-mh">0x01</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_15</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">rnd_4_15</span><span class="tok-p">,</span> <span class="tok-n">shf_5_31</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-n">__m256i</span> <span class="tok-n">rnd_5_31</span> <span class="tok-o">=</span> <span class="tok-n">_mm256_blendv_epi8</span><span class="tok-p">(</span><span class="tok-n">shf_5_15</span><span class="tok-p">,</span> <span class="tok-n">rnd_4_31</span><span class="tok-p">,</span> <span class="tok-n">blm_5</span><span class="tok-p">);</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">15</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_15</span><span class="tok-p">;</span>
  <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-mi">31</span><span class="tok-o">*</span><span class="tok-n">dst_stride</span><span class="tok-p">)</span> <span class="tok-o">=</span> <span class="tok-n">rnd_5_31</span><span class="tok-p">;</span>
<span class="tok-p">}</span>

<span class="tok-kt">void</span> <span class="tok-nf">transpose_Vec256</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">n</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">n</span><span class="tok-p">();</span>
  <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">rb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">rb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-mi">64</span><span class="tok-p">;</span> <span class="tok-n">rb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
    <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">cb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">cb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-mi">64</span><span class="tok-p">;</span> <span class="tok-n">cb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
      <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">src_origin</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">data</span><span class="tok-p">()</span>  <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">rb</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-n">cb</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-mi">64</span><span class="tok-p">;</span>
      <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">prf_origin</span> <span class="tok-o">=</span> <span class="tok-n">next_block</span><span class="tok-p">(</span><span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">rb</span><span class="tok-p">,</span> <span class="tok-n">cb</span><span class="tok-p">);</span>
            <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dst_origin</span> <span class="tok-o">=</span> <span class="tok-n">dst</span><span class="tok-o">-&gt;</span><span class="tok-n">data</span><span class="tok-p">()</span> <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">cb</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-n">rb</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-mi">64</span><span class="tok-p">;</span>

      <span class="tok-n">transpose_Vec256_kernel</span><span class="tok-p">(</span><span class="tok-n">src_origin</span><span class="tok-p">,</span>         <span class="tok-n">dst_origin</span><span class="tok-p">,</span>         <span class="tok-n">prf_origin</span><span class="tok-p">,</span>      <span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">);</span>
      <span class="tok-n">transpose_Vec256_kernel</span><span class="tok-p">(</span><span class="tok-n">src_origin</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-p">,</span>    <span class="tok-n">dst_origin</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-p">,</span>      <span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-n">n</span><span class="tok-o">*</span><span class="tok-mi">16</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">);</span>
      <span class="tok-n">transpose_Vec256_kernel</span><span class="tok-p">(</span><span class="tok-n">src_origin</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-p">,</span>      <span class="tok-n">dst_origin</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-p">,</span>    <span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-n">n</span><span class="tok-o">*</span><span class="tok-mi">32</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">);</span>
      <span class="tok-n">transpose_Vec256_kernel</span><span class="tok-p">(</span><span class="tok-n">src_origin</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-p">,</span> <span class="tok-n">dst_origin</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-p">,</span> <span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-n">n</span><span class="tok-o">*</span><span class="tok-mi">48</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">);</span>
    <span class="tok-p">}</span>
  <span class="tok-p">}</span>
<span class="tok-p">}</span>
</pre></td></tr></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>Switching from general purpose instruction set to AVX turned out to be quite complex.
But it comes with a benefit of further reduction in running time by 15-30% compared to 64-bit version:</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 40%;">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">&nbsp;</th>
<th class="tableblock halign-right valign-top">cpe (N=320)</th>
<th class="tableblock halign-right valign-top">cpe (N=2112)</th>
<th class="tableblock halign-right valign-top">cpe (N=46400)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Vec32</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.85</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.85</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.89</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Vec64</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.76</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.74</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.80</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Vec256</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.44</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.49</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.68</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>This trend should continue with increase in vector size, provided that
performance is not stuck in memory access and that latency of longer-vector instructions is the same
as of their shorter counterparts.
Unfortunately, vector sizes can&#8217;t be increased indefinitely.
In the majority of <em>horizontal</em> vector instructions, such as shuffle, every element of the output register
depends on every element of the input register.
Such logic requires O(log(n)) stages of digital circuitry, which increases latency and hence defeats the purpose
of increasing vector size.
We already saw that shuffle in fact is not a purely 256-bit horizontal SIMD instruction,
but a pair of simultaneously executing 128-bit shuffles.
For CPUs, it is quite unrealistic that vector instructions will be increased from 512 (provided by AVX-512) to longer values.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_buffering_output_0_35">Buffering output [0.35]</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Although performance has improved, thorough observation of PMU counters
reveals that memory subsystem is again the bottleneck and not the SIMD code.
According to <code>resource_stalls.sb</code> counter, there are almost 16 stall cycles per each 64 bytes of input data.
Values in the table were collected by transposing three matrices of N=46400.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 40%;">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">&nbsp;</th>
<th class="tableblock halign-left valign-top">[unit]</th>
<th class="tableblock halign-right valign-top">Total</th>
<th class="tableblock halign-right valign-top">Per 64 bytes</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resource_stalls.sb</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1,600,415,891</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">15.858</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">offcore_requests.demand_rfo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">101,634,381</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.007</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">offcore_requests_outstanding.cycles_with_demand_rfo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">3,645,717,723</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">36.125</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Memory and the execution are the most common culprits of suboptimal performance.
One of them is always the bottleneck.
Fixing memory issues makes execution the bottleneck, and optimizing execution uncovers more issues with memory.
This process can be repeated many times up to the point when both are working close to their theoretical limits.</p>
</div>
<div class="paragraph">
<p>The fact that we get nearly sixteen SB stalls per each cache line is unexpected.
Each two invocations of 32x32 transposer fully assemble 32 cache lines,
which should definitely fit into SB (Skylake has 56 entries).
And since they are assembled in full, we expect that they will be flushed out without
sending any load requests to RAM first.
And yet there are SB stalls.
To figure out what is going on, we can look to offcore_requests.demand_rfo counter.
Its value, normalized by number of processed cache lines, reveals that there is one RFO message per each cache line.
RFO messages are sent to other cores to agree on the ownership of cache lines.
There can be only one exclusive owner of given cache line at any given time, or otherwise
different cores may have different versions of the same cache line, ruining the correct
behaviour of the programs which concurrently modify the data.
Once the responses from all other cores are received, the requesting core becomes the sole owner of the cache line
and has the right to modify it locally.
In case of our transposer it is sure that the region of <code>dst</code> where we are writing to is not yet in L1d or L2,
leading us to the conclusion that current core is not yet the owner of the respective cache lines,
and that&#8217;s why it has to send the RFOs before the modification can be applied.
Apparently, waiting for the RFO responses is the bottleneck of our SIMD transposer.</p>
</div>
<div class="paragraph">
<p>Such type of issues can be circumvented if we write the result of 64x64 transpose not directly into <code>dst</code> matrix,
but into temporary L1d-local buffer.
After transpose of the 64x64 block is complete, we can move data from the buffer into <code>dst</code> using <em>non-temporal hint</em>.
The idea that we have to write data twice is counterintuitive, but it reduces waiting times for inter-core communication.
During the first stage we write data into temporary buffer, which is reused for each 64x64 block and that&#8217;s why
it settles down in L1d, with all of its cache lines being privately owned by the current core.
The latter means that when some cache line of this buffer needs to be updated, there is no need to send RFO and wait for responses
from other cores: cache line is marked in L1d as privately owned by this core with no possibility of other cores having a copy of it.
In the second stage we have to copy data from buffer into <code>dst</code>.
Using ordinary move instructions would generate RFO message for each cache line
and wait while all other cores respond that they have relinquished ownership if they had one.
Waiting for RFO responses is what is currently limiting performance of the transposer.
Using non-temporal move instructions, on the other hand, relaxes the ownership rules.
No RFOs are sent and, since cache lines are modified in full, they can be moved directly to memory.</p>
</div>
<div class="paragraph">
<p>Of course, non-temporal moves do not come without limitations, or otherwise they would be used ubiquitously.
First, to create any useful effect, they have to be issued in burst covering the whole cache line.
Second, you have to be sure that no other threads are accessing the data concurrently with non-temporal move.
The latter is a common contract in matrix-oriented code.</p>
</div>
<div class="paragraph">
<p>The changes we need to make to our program are small: preallocate 64x64 buffer,
instruct 32x32 SIMD transposer to write result into this buffer, and finally
use a burst of <code>_mm256_stream_si256</code> instructions to copy data from the buffer into <code>dst</code>.
32x32 transpose kernel remains the same.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><table class="linenotable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23</pre></div></td><td class="code"><pre><span></span><span class="tok-kt">void</span> <span class="tok-nf">transpose_Vec256Buf</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">Mat</span><span class="tok-o">&amp;</span> <span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">Mat</span><span class="tok-o">*</span> <span class="tok-n">dst</span><span class="tok-p">)</span> <span class="tok-p">{</span>
  <span class="tok-k">const</span> <span class="tok-kt">int64_t</span> <span class="tok-n">n</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">n</span><span class="tok-p">();</span>
  <span class="tok-kt">uint8_t</span> <span class="tok-n">buf</span><span class="tok-p">[</span><span class="tok-mi">64</span><span class="tok-o">*</span><span class="tok-mi">64</span><span class="tok-p">]</span> <span class="tok-n">__attribute__</span> <span class="tok-p">((</span><span class="tok-n">aligned</span> <span class="tok-p">(</span><span class="tok-mi">64</span><span class="tok-p">)));</span>
  <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">rb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">rb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-mi">64</span><span class="tok-p">;</span> <span class="tok-n">rb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
    <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int64_t</span> <span class="tok-n">cb</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">cb</span> <span class="tok-o">&lt;</span> <span class="tok-n">n</span><span class="tok-o">/</span><span class="tok-mi">64</span><span class="tok-p">;</span> <span class="tok-n">cb</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
      <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">src_origin</span> <span class="tok-o">=</span> <span class="tok-n">src</span><span class="tok-p">.</span><span class="tok-n">data</span><span class="tok-p">()</span>  <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">rb</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-n">cb</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-mi">64</span><span class="tok-p">;</span>
      <span class="tok-k">const</span> <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">prf_origin</span> <span class="tok-o">=</span> <span class="tok-n">next_block</span><span class="tok-p">(</span><span class="tok-n">src</span><span class="tok-p">,</span> <span class="tok-n">rb</span><span class="tok-p">,</span> <span class="tok-n">cb</span><span class="tok-p">);</span>
            <span class="tok-kt">uint8_t</span><span class="tok-o">*</span> <span class="tok-n">dst_origin</span> <span class="tok-o">=</span> <span class="tok-n">dst</span><span class="tok-o">-&gt;</span><span class="tok-n">data</span><span class="tok-p">()</span> <span class="tok-o">+</span> <span class="tok-p">(</span><span class="tok-n">cb</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-n">rb</span><span class="tok-p">)</span><span class="tok-o">*</span><span class="tok-mi">64</span><span class="tok-p">;</span>

      <span class="tok-n">transpose_Vec256_kernel</span><span class="tok-p">(</span><span class="tok-n">src_origin</span><span class="tok-p">,</span>         <span class="tok-n">buf</span><span class="tok-p">,</span>          <span class="tok-n">prf_origin</span><span class="tok-p">,</span>      <span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-mi">64</span><span class="tok-p">);</span>
      <span class="tok-n">transpose_Vec256_kernel</span><span class="tok-p">(</span><span class="tok-n">src_origin</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-p">,</span>    <span class="tok-n">buf</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-p">,</span>       <span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-n">n</span><span class="tok-o">*</span><span class="tok-mi">16</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-mi">64</span><span class="tok-p">);</span>
      <span class="tok-n">transpose_Vec256_kernel</span><span class="tok-p">(</span><span class="tok-n">src_origin</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-p">,</span>      <span class="tok-n">buf</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-o">*</span><span class="tok-mi">64</span><span class="tok-p">,</span>    <span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-n">n</span><span class="tok-o">*</span><span class="tok-mi">32</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-mi">64</span><span class="tok-p">);</span>
      <span class="tok-n">transpose_Vec256_kernel</span><span class="tok-p">(</span><span class="tok-n">src_origin</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-o">*</span><span class="tok-n">n</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-p">,</span> <span class="tok-n">buf</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-o">*</span><span class="tok-mi">64</span><span class="tok-o">+</span><span class="tok-mi">32</span><span class="tok-p">,</span> <span class="tok-n">prf_origin</span><span class="tok-o">+</span><span class="tok-n">n</span><span class="tok-o">*</span><span class="tok-mi">48</span><span class="tok-p">,</span> <span class="tok-n">n</span><span class="tok-p">,</span> <span class="tok-mi">64</span><span class="tok-p">);</span>

      <span class="tok-k">for</span> <span class="tok-p">(</span><span class="tok-kt">int</span> <span class="tok-n">row</span> <span class="tok-o">=</span> <span class="tok-mi">0</span><span class="tok-p">;</span> <span class="tok-n">row</span> <span class="tok-o">&lt;</span> <span class="tok-mi">64</span><span class="tok-p">;</span> <span class="tok-n">row</span><span class="tok-o">++</span><span class="tok-p">)</span> <span class="tok-p">{</span>
        <span class="tok-n">__m256i</span> <span class="tok-n">lane0</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">buf</span> <span class="tok-o">+</span> <span class="tok-mi">64</span><span class="tok-o">*</span><span class="tok-n">row</span><span class="tok-p">);</span>
        <span class="tok-n">__m256i</span> <span class="tok-n">lane1</span> <span class="tok-o">=</span> <span class="tok-o">*</span><span class="tok-p">(</span><span class="tok-k">const</span> <span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">buf</span> <span class="tok-o">+</span> <span class="tok-mi">64</span><span class="tok-o">*</span><span class="tok-n">row</span> <span class="tok-o">+</span> <span class="tok-mi">32</span><span class="tok-p">);</span>
        <span class="tok-n">_mm256_stream_si256</span><span class="tok-p">((</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-n">n</span><span class="tok-o">*</span><span class="tok-n">row</span><span class="tok-p">),</span> <span class="tok-n">lane0</span><span class="tok-p">);</span>
        <span class="tok-n">_mm256_stream_si256</span><span class="tok-p">((</span><span class="tok-n">__m256i</span><span class="tok-o">*</span><span class="tok-p">)(</span><span class="tok-n">dst_origin</span> <span class="tok-o">+</span> <span class="tok-n">n</span><span class="tok-o">*</span><span class="tok-n">row</span> <span class="tok-o">+</span> <span class="tok-mi">32</span><span class="tok-p">),</span> <span class="tok-n">lane1</span><span class="tok-p">);</span>
      <span class="tok-p">}</span>
    <span class="tok-p">}</span>
  <span class="tok-p">}</span>
<span class="tok-p">}</span>
</pre></td></tr></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>With such an improvement, we get nearly zero RFO, while SB stalls fall down by 80%.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 40%;">
<colgroup>
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">&nbsp;</th>
<th class="tableblock halign-left valign-top">[unit]</th>
<th class="tableblock halign-right valign-top">Total</th>
<th class="tableblock halign-right valign-top">Per 64 bytes</th>
<th class="tableblock halign-right valign-top">Change</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">resource_stalls.sb</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">268,383,671</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.659</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">-83.2%</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">offcore_requests.demand_rfo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">events</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">195,150</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.002</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">-99.8%</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">offcore_requests_outstanding.cycles_with_demand_rfo</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">cycles</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9,126,733</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.090</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">-99.7%</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>This translates to improvement by up to 40% compared to SIMD version without buffering:</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 40%;">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">&nbsp;</th>
<th class="tableblock halign-right valign-top">cpe (N=320)</th>
<th class="tableblock halign-right valign-top">cpe (N=2112)</th>
<th class="tableblock halign-right valign-top">cpe (N=46400)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">&#8230;&#8203;</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Vec64</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.76</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.74</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.80</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Vec256</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.44</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.49</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.68</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Vec256Buf</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.37</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.35</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.40</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Vec256Buf is the final, most efficient transpose algorithm presented in this article.
Let&#8217;s do short review of the ideas it is based on:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>We require that matrices are padded: their physical size in each dimension must be equal
to cache line size (64) multiplied by some odd number.
This ensures that there will be no performance issues caused by cache aliasing.
We also require that beginning of the matrices is aligned by cache line size.
Combined, these two requirements allow us to naturally treat each row of the matrix as an array of cache lines.</p>
</li>
<li>
<p>Input is divided into 64x64 blocks, which are processed one by one.
While we are processing a block, we instruct CPU to preload the next one.
Since single block is small, it fits entirely into L1d.
When we move to the next block, we can be sure that it is already preloaded into L1d
and hence that we will not be slowed down by long latency of RAM.</p>
</li>
<li>
<p>Each 64x64 block is subdivided into four 32x32 blocks.
This allows us to store rows in AVX2 registers, which are 256-bit long.</p>
</li>
<li>
<p>Each 32x32 block is transposed using 5-stage block decomposition algorithm.
Many small block matrices are transposed simultaneously with the help of AVX2 shuffle and blend instructions.</p>
</li>
<li>
<p>The result of 32x32 transposer is written into proper position of 64x64 reusable buffer.
After 64x64 block is fully assembled, we write it into <code>dst</code> using non-temporal hint.
This ensures that current core won&#8217;t wait while other cores relinquish ownership of the target memory region.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Compare this monstrosity with the initial eight-liner.
This is the cost we have to pay to "explain" the CPU our high-level intentions.
Without our help, CPU understands very little: its scope is limited to a small window inside never-ending sequence
of machine code instructions.
Looking into such window, it is hard even for a human being to reverse engineer and optimize the overall algorithm,
and especially hard to do this automatically and on the fly as CPU mode of operation requires.
High complexity is the cost we have to pay for performance when we really want it.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Below is the evaluation of all presented algorithms for a selection of <code>N</code> values which already satisfy "dealiasing" rules.
As before, values are the average number of cycles per element; the lower&#8201;&#8212;&#8201;the better.
Vec256Buf, the most sophisticated transposer, outperforms Naive by a factor of more than x25 for large <code>N</code>.
It may be more reasonable to compare it with the Blocks transposer, since the latter is not much more complicated
than the Naive transposer and yet it doesn&#8217;t rely on architecture-dependent instructions.
In this case performance differs by approximately x4&#8201;&#8212;&#8201;still a significant gap.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 70%;">
<colgroup>
<col style="width: 11.1111%;">
<col style="width: 11.1111%;">
<col style="width: 11.1111%;">
<col style="width: 11.1111%;">
<col style="width: 11.1111%;">
<col style="width: 11.1111%;">
<col style="width: 11.1111%;">
<col style="width: 11.1111%;">
<col style="width: 11.1112%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-right valign-top">N</th>
<th class="tableblock halign-right valign-top">Naive</th>
<th class="tableblock halign-right valign-top">Reverse</th>
<th class="tableblock halign-right valign-top">Blocks</th>
<th class="tableblock halign-right valign-top">BlocksPrf</th>
<th class="tableblock halign-right valign-top">Vec32</th>
<th class="tableblock halign-right valign-top">Vec64</th>
<th class="tableblock halign-right valign-top">Vec256</th>
<th class="tableblock halign-right valign-top">Vec256Buf</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>320</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.33</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.28</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.49</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.36</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.85</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.76</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.44</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.37</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>576</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">3.24</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.48</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.49</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.35</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.84</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.74</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.43</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.36</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>704</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4.09</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.70</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.48</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.36</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.85</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.74</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.42</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.35</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>1088</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4.00</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.78</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.50</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.36</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.84</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.74</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.43</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.35</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>1472</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4.01</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.80</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.50</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.36</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.85</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.74</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.45</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.35</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>2112</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">3.90</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.61</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.46</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.35</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.85</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.74</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.49</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.35</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>2880</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4.11</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.95</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.52</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.36</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.86</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.75</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.54</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.37</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>4160</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.62</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4.05</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.42</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.35</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.88</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.81</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.63</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.39</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>5824</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.68</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.08</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.59</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.37</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.88</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.79</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.64</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.39</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>8256</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.51</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.16</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.61</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.37</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.87</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.81</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.64</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.39</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>11584</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.75</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.28</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.61</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.37</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.87</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.79</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.65</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.39</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>16448</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.60</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.36</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.52</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.36</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.87</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.81</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.65</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.40</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>23232</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.62</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.27</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.62</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.38</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.89</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.79</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.67</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.40</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>30784</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.73</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.83</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.52</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.38</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.89</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.80</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.67</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.40</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>46400</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.95</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.01</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.64</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.41</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.89</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.80</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.68</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.40</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>65600</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">8.14</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">10.89</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.38</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.35</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.93</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.97</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.69</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.43</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>77888</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9.23</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">10.67</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.66</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.41</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.90</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.84</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.69</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.42</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>92736</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">10.80</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">12.32</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.66</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.42</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.90</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.82</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.69</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.42</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock"><strong>111808</strong></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">12.40</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">13.74</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.65</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1.43</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.91</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.83</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.69</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.42</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>It should be obvious that Vec256Buf is not the ultimate solution.
There is plenty of room for further improvement by using newer instruction extensions.
For example, replacing AVX2 32-byte shuffling and blending instructions with their respective
64-byte counterparts from AVX-512 allows to transpose 64x64 submatrix with a single invocation
of block algorithm that is constructed of log<sub>2</sub>(64)=6 levels.
Entirely different way around is to use scatter/gather instructions provided by AVX-512.
They can load and store elements of SIMD registers from noncontiguous addresses.
Similarly, AVX-512 provides prefetch instructions with the same capability.
Third direction may be the application of AMX extension.
It specifically targets matrix operations and in particular is capable of loading and storing submatrices
into tile registers using strided memory access.</p>
</div>
<div class="paragraph">
<p>But the key point is that matrix transpose serves as a prime example of the problem which appears
trivial but nevertheless requires tremendous effort to be solved efficiently for CPU.
Root causes of this unpleasant surprise can be traced to high memory latency and limited
parallelization capabilities of CPUs.
Neither of these issues is going to be alleviated in the future.
With each newer semiconductor process we can see increase in number of cores and memory sizes,
but memory latency is limited by physical phenomena and cannot be reduced any further.
Increasing CPU parallelism is possible but wastes precious chip area.
Since CPU is tailored to run predominantly serial programs, this extra area would
be rarely used, making it an expensive feature.
Finally, optimizing compilers are also not a silver bullet.
They work well when they recognize one of the standard problems they were trained for
and fail otherwise.
In general, GPU and other specialized accelerators are much more suitable for matrix-heavy problems.
However, more often than not it is required to run code for such problems only occasionally.
In such scenarios it is more cost-effective to run thoroughly optimized CPU version of the code,
thus justifying all the sophistication explained in this article.</p>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2024-02-11 12:37:58 UTC
</div>
</div>
</body>
</html>