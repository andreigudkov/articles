<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.4">
<meta name="author" content="Andrei Gudkov">
<title>Storage subsystem performance: analysis and recipes</title>
<style>
/*! normalize.css v2.1.2 | MIT License | git.io/normalize */article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}audio,canvas,video{display:inline-block}audio:not([controls]){display:none;height:0}[hidden],template{display:none}script{display:none!important}html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}a{background:transparent}a:focus{outline:thin dotted}a:active,a:hover{outline:0}h1{font-size:2em;margin:.67em 0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}mark{background:#ff0;color:#000}code,kbd,pre,samp{font-family:monospace,serif;font-size:1em}pre{white-space:pre-wrap}q{quotes:"\201C" "\201D" "\2018" "\2019"}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}svg:not(:root){overflow:hidden}figure{margin:0}fieldset{border:1px solid #c0c0c0;margin:0 2px;padding:.35em .625em .75em}legend{border:0;padding:0}button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}button,input{line-height:normal}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="search"]{-webkit-appearance:textfield;-moz-box-sizing:content-box;-webkit-box-sizing:content-box;box-sizing:content-box}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}textarea{overflow:auto;vertical-align:top}table{border-collapse:collapse;border-spacing:0}*,*::before,*::after{box-sizing:border-box;margin:0}body{font-family:Helvetica,Arial,sans-serif;font-size:16px;color:#222;line-height:1.5;max-width:55em;margin:0 auto}#content,#footnotes{padding-left:.5em;padding-right:.5em}strong{font-weight:bold}em{font-style:italic}:not(pre)>code{font-family:Courier,monospace;line-height:1.0}a{color:#0061c5;text-decoration:none}a:hover{text-decoration:underline}hr{border-width:0 0 1px 0;border-style:solid;border-color:#678}ul,ol{list-style-position:outside;padding-left:0;margin-left:2em}ul li ul,ul li ol,ol li ul,ol li ol{margin-left:1.414em}ul>li{list-style-type:square;font-size:80%}ul>li>*{font-size:125%}ol>li{font-weight:bold}ol>li>*{font-weight:normal}ol.arabic{list-style-type:decimal}ol.decimal{list-style-type:decimal-leading-zero}ol.loweralpha{list-style-type:lower-alpha}ol.upperalpha{list-style-type:upper-alpha}ol.lowerroman{list-style-type:lower-roman}ol.upperroman{list-style-type:upper-roman}ol.lowergreek{list-style-type:lower-greek}.dlist dt{color:#325d72;font-weight:bold}.dlist dt:not(:first-child){margin-top:1em}.dlist dd{margin-left:2em}td.hdlist1{color:#325d72;padding-right:.5em;vertical-align:top}td.hdlist2{padding-bottom:.5em}h1{font-size:28px;font-weight:normal;letter-spacing:-1px;color:white;background-color:#325d72;text-align:center;margin:0 0 .5em 0;padding:.05em .5em}@media print{h1{color:#325d72;background-color:white;font-weight:bold}}h1::after{content:':';width:0;overflow:hidden;display:inline-block;vertical-align:middle}.author{color:#325d72}.email::before{content:"<";color:#325d72}.email::after{content:">";color:#325d72}.author+br,.email+br{display:none}#author{padding-left:.5em}#toc{margin:1em 0 2em 0;padding-left:.5em}#toctitle{font-size:19px;font-weight:bold;color:#325d72;margin:.5em 0}#toc>ul{line-height:1.4;font-size:15px;margin:0 0 0 .5em}#toc ul li{list-style-type:none}#toc li{margin:0}.big{font-size:120%}.small{font-size:75%}.underline{text-decoration:underline}.overline{text-decoration:overline}.line-through{text-decoration:line-through}.aqua{color:#00bfbf}.aqua-background{background-color:#00fafa;border-radius:2px;padding:0 3px}.black{color:"black"}.black-background{background-color:"black";border-radius:2px;padding:0 3px}.blue{color:#0000bf}.blue-background{background-color:#0000fa;border-radius:2px;padding:0 3px}.fuchsia{color:#bf00bf}.fuchsia-background{background-color:#fa00fa;border-radius:2px;padding:0 3px}.gray{color:#606060}.gray-background{background-color:#7d7d7d;border-radius:2px;padding:0 3px}.green{color:#006000}.green-background{background-color:#007d00;border-radius:2px;padding:0 3px}.lime{color:#00bf00}.lime-background{background-color:#00fa00;border-radius:2px;padding:0 3px}.maroon{color:#600000}.maroon-background{background-color:#7d0000;border-radius:2px;padding:0 3px}.navy{color:#000060}.navy-background{background-color:#00007d;border-radius:2px;padding:0 3px}.olive{color:#606000}.olive-background{background-color:#7d7d00;border-radius:2px;padding:0 3px}.purple{color:#600060}.purple-background{background-color:#7d007d;border-radius:2px;padding:0 3px}.red{color:#bf0000}.red-background{background-color:#fa0000;border-radius:2px;padding:0 3px}.silver{color:#909090}.silver-background{background-color:#bcbcbc;border-radius:2px;padding:0 3px}.teal{color:#006060}.teal-background{background-color:#007d7d;border-radius:2px;padding:0 3px}.white{color:#bfbfbf}.white-background{background-color:#fafafa;border-radius:2px;padding:0 3px}.yellow{color:#bfbf00}.yellow-background{background-color:#fafa00;border-radius:2px;padding:0 3px}table.tableblock{border:1px solid #91a7b3;margin-left:auto;margin-right:auto}table.tableblock>caption.title{text-align:left;margin-bottom:.5em}table.tableblock>colgroup>col{width:inherit!important}table.tableblock>tbody>tr>td{border-style:solid;border-color:#91a7b3;border-width:0 1px;padding:0 5px 2px 5px}table.tableblock>tbody>tr:nth-of-type(2n){background-color:#f3f5f7}p.tableblock{text-align:inherit}table.tableblock>thead>tr>td,table.tableblock>thead>tr>th,table.tableblock>tfoot>tr>td,table.tableblock>tfoot>tr>th{color:#325d72;font-weight:bold;line-height:1.35;padding:2px 5px;border:1px solid #91a7b3}table.tableblock>thead>tr>th,table.tableblock>thead>tr>td{border-bottom-width:2px}table.tableblock>tfoot>tr>th,table.tableblock>tfoot>tr>td{border-top-width:2px}th.halign-left,td.halign-left{text-align:left}th.halign-right,td.halign-right{text-align:right}th.halign-center,td.halign-center{text-align:center}th.valign-top,td.valign-top{vertical-align:top}th.valign-bottom,td.valign-bottom{vertical-align:bottom}th.valign-middle,td.valign-middle{vertical-align:middle}div.listingblock{padding:.5em;border-style:solid;border-color:#678;border-width:0 0 0 2px;background-color:#f3f5f7;overflow:auto}div.listingblock .title{text-align:right}div.listingblock pre{font-family:Menlo,Consolas,Monaco,"Lucida Console",monospace;font-size:87.5%;white-space:pre;background-color:#f3f5f7!important;margin:0}div.listingblock td.linenos{border-right:1px solid #91a7b3;padding-right:.67em}div.listingblock table.pyhltable div.linenodiv{color:#678;text-align:right}div.listingblock table.pyhltable td.code{padding-left:.67em}div.imageblock>div.content>img{max-width:98%}.text-indent{padding-left:2em}img.inlinemath{image-rendering:optimizequality;margin-top:.5ex}h2,h3,h4{font-weight:normal;color:#325d72;margin:0}h2{font-size:27px;letter-spacing:-1px;border-bottom:1px solid #91a7b3}h3{font-size:24px;letter-spacing:-0.75px}h4{font-size:21px;letter-spacing:-0.5px}.title{color:#325d72;font-weight:bold}#footer{font-size:80%;color:white;background-color:#325d72}@media print{#footer{color:#325d72;background-color:white;font-weight:bold}}#footer-text{text-align:center;padding:.5em}#footer-badges{display:none}span.footnote{vertical-align:super;font-size:80%}#footnotes>hr{display:none}#footnotes::before{display:block;border-bottom:1px solid #678;margin:.5em 0;content:"Notes";font-size:19px;font-weight:bold;color:#325d72}#footnotes .footnote{margin-left:.5em;font-size:15px}hr:not(:first-child){margin-top:1.5em}hr:not(:last-child){margin-bottom:1.5em}.imageblock:not(:last-child),.listingblock:not(:last-child),.tableblock:not(:last-child){margin-bottom:1em}p+*{margin-top:1em}.paragraph+*{margin-top:1em}p+.ulist,p+.olist,p+.dlist,p+.hdlist,.paragraph+.ulist,.paragraph+.olist,.paragraph+.dlist,.paragraph+.hdlist,.paragraph+.listingblock{margin-top:.5em!important}li *+.ulist,li *+.olist,li *+.dlist,li *+.hdlist{margin-top:.1em!important}.title:not(:first-child){margin-top:1.5em}.content+.title{margin-top:.5em!important}.title+*{margin-top:1.5em}.title+p,.title+.paragraph,.title+.ulist,.title+.olist,.title+.dlist,.title+.hdlist{margin-top:.5em!important}.ulist:not(:last-child){margin-bottom:1em}.olist:not(:last-child){margin-bottom:1em}li:not(:first-child){margin-top:.1em}.dlist:not(:last-child){margin-bottom:1em}.dlist:not(:first-child){margin-top:1em}.sect3:not(:last-child){margin-bottom:18px}.sect3:not(:first-child){margin-top:18px}h4:not(:last-child){margin-bottom:9px}.sect2:not(:last-child){margin-bottom:22px}.sect2:not(:first-child){margin-top:22px}h3:not(:last-child){margin-bottom:11px}.sect1:not(:last-child){margin-bottom:40px}.sect1:not(:first-child){margin-top:40px}#preamble:not(:last-child){margin-bottom:40px}h2:not(:last-child){margin-bottom:13px}#header:not(:last-child),#content:not(:last-child),#footnotes:not(:last-child){margin-bottom:2em}
</style>
<style>
.listingblock .pygments .hll { background-color: #ffffcc }
.listingblock .pygments  { background: #f8f8f8; }
.listingblock .pygments .tok-c { color: #408080; font-style: italic } /* Comment */
.listingblock .pygments .tok-err { border: 1px solid #FF0000 } /* Error */
.listingblock .pygments .tok-k { color: #008000; font-weight: bold } /* Keyword */
.listingblock .pygments .tok-o { color: #666666 } /* Operator */
.listingblock .pygments .tok-ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.listingblock .pygments .tok-cm { color: #408080; font-style: italic } /* Comment.Multiline */
.listingblock .pygments .tok-cp { color: #BC7A00 } /* Comment.Preproc */
.listingblock .pygments .tok-cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.listingblock .pygments .tok-c1 { color: #408080; font-style: italic } /* Comment.Single */
.listingblock .pygments .tok-cs { color: #408080; font-style: italic } /* Comment.Special */
.listingblock .pygments .tok-gd { color: #A00000 } /* Generic.Deleted */
.listingblock .pygments .tok-ge { font-style: italic } /* Generic.Emph */
.listingblock .pygments .tok-gr { color: #FF0000 } /* Generic.Error */
.listingblock .pygments .tok-gh { color: #000080; font-weight: bold } /* Generic.Heading */
.listingblock .pygments .tok-gi { color: #00A000 } /* Generic.Inserted */
.listingblock .pygments .tok-go { color: #888888 } /* Generic.Output */
.listingblock .pygments .tok-gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.listingblock .pygments .tok-gs { font-weight: bold } /* Generic.Strong */
.listingblock .pygments .tok-gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.listingblock .pygments .tok-gt { color: #0044DD } /* Generic.Traceback */
.listingblock .pygments .tok-kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.listingblock .pygments .tok-kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.listingblock .pygments .tok-kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.listingblock .pygments .tok-kp { color: #008000 } /* Keyword.Pseudo */
.listingblock .pygments .tok-kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.listingblock .pygments .tok-kt { color: #B00040 } /* Keyword.Type */
.listingblock .pygments .tok-m { color: #666666 } /* Literal.Number */
.listingblock .pygments .tok-s { color: #BA2121 } /* Literal.String */
.listingblock .pygments .tok-na { color: #7D9029 } /* Name.Attribute */
.listingblock .pygments .tok-nb { color: #008000 } /* Name.Builtin */
.listingblock .pygments .tok-nc { color: #0000FF; font-weight: bold } /* Name.Class */
.listingblock .pygments .tok-no { color: #880000 } /* Name.Constant */
.listingblock .pygments .tok-nd { color: #AA22FF } /* Name.Decorator */
.listingblock .pygments .tok-ni { color: #999999; font-weight: bold } /* Name.Entity */
.listingblock .pygments .tok-ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.listingblock .pygments .tok-nf { color: #0000FF } /* Name.Function */
.listingblock .pygments .tok-nl { color: #A0A000 } /* Name.Label */
.listingblock .pygments .tok-nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.listingblock .pygments .tok-nt { color: #008000; font-weight: bold } /* Name.Tag */
.listingblock .pygments .tok-nv { color: #19177C } /* Name.Variable */
.listingblock .pygments .tok-ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.listingblock .pygments .tok-w { color: #bbbbbb } /* Text.Whitespace */
.listingblock .pygments .tok-mb { color: #666666 } /* Literal.Number.Bin */
.listingblock .pygments .tok-mf { color: #666666 } /* Literal.Number.Float */
.listingblock .pygments .tok-mh { color: #666666 } /* Literal.Number.Hex */
.listingblock .pygments .tok-mi { color: #666666 } /* Literal.Number.Integer */
.listingblock .pygments .tok-mo { color: #666666 } /* Literal.Number.Oct */
.listingblock .pygments .tok-sa { color: #BA2121 } /* Literal.String.Affix */
.listingblock .pygments .tok-sb { color: #BA2121 } /* Literal.String.Backtick */
.listingblock .pygments .tok-sc { color: #BA2121 } /* Literal.String.Char */
.listingblock .pygments .tok-dl { color: #BA2121 } /* Literal.String.Delimiter */
.listingblock .pygments .tok-sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.listingblock .pygments .tok-s2 { color: #BA2121 } /* Literal.String.Double */
.listingblock .pygments .tok-se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.listingblock .pygments .tok-sh { color: #BA2121 } /* Literal.String.Heredoc */
.listingblock .pygments .tok-si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.listingblock .pygments .tok-sx { color: #008000 } /* Literal.String.Other */
.listingblock .pygments .tok-sr { color: #BB6688 } /* Literal.String.Regex */
.listingblock .pygments .tok-s1 { color: #BA2121 } /* Literal.String.Single */
.listingblock .pygments .tok-ss { color: #19177C } /* Literal.String.Symbol */
.listingblock .pygments .tok-bp { color: #008000 } /* Name.Builtin.Pseudo */
.listingblock .pygments .tok-fm { color: #0000FF } /* Name.Function.Magic */
.listingblock .pygments .tok-vc { color: #19177C } /* Name.Variable.Class */
.listingblock .pygments .tok-vg { color: #19177C } /* Name.Variable.Global */
.listingblock .pygments .tok-vi { color: #19177C } /* Name.Variable.Instance */
.listingblock .pygments .tok-vm { color: #19177C } /* Name.Variable.Magic */
.listingblock .pygments .tok-il { color: #666666 } /* Literal.Number.Integer.Long */
</style>
</head>
<body class="article">
<div id="header">
<h1>Storage subsystem performance: analysis and recipes</h1>
<div class="details">
<span id="author" class="author">Andrei Gudkov</span><br>
<span id="email" class="email"><a href="mailto:gudokk@gmail.com">gudokk@gmail.com</a></span><br>
</div>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_introduction">Introduction</a></li>
<li><a href="#_theory">Theory</a>
<ul class="sectlevel2">
<li><a href="#_io_stack_overview">IO stack overview</a></li>
<li><a href="#_performance_metrics">Performance metrics</a></li>
<li><a href="#_access_patterns">Access patterns</a></li>
<li><a href="#_performance_related_concepts">Performance-related concepts</a>
<ul class="sectlevel3">
<li><a href="#_caching">Caching</a></li>
<li><a href="#_speculative_reads">Speculative reads</a></li>
<li><a href="#_writeback">Writeback</a></li>
<li><a href="#_write_combining_and_reordering">Write combining and reordering</a></li>
<li><a href="#_read_modify_write">Read-modify-write</a></li>
<li><a href="#_parallelism">Parallelism</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_hard_disk_drive_hdd">Hard disk drive (HDD)</a>
<ul class="sectlevel2">
<li><a href="#_theory_of_operation">Theory of operation</a>
<ul class="sectlevel3">
<li><a href="#_internals">Internals</a></li>
<li><a href="#_sector_size">Sector size</a></li>
<li><a href="#_addressing">Addressing</a></li>
</ul>
</li>
<li><a href="#_performance">Performance</a>
<ul class="sectlevel3">
<li><a href="#_sequential_access">Sequential access</a></li>
<li><a href="#_random_access">Random access</a></li>
<li><a href="#_hybrid_access">Hybrid access</a></li>
<li><a href="#_comparison">Comparison</a></li>
</ul>
</li>
<li><a href="#_experiments">Experiments</a>
<ul class="sectlevel3">
<li><a href="#_effect_of_lba_address_on_sequential_access_performance">Effect of LBA address on sequential access performance</a></li>
<li><a href="#_random_access_response_time_distribution">Random access response time distribution</a></li>
<li><a href="#_chunk_size">Chunk size</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_solid_state_drive_ssd">Solid-state drive (SSD)</a>
<ul class="sectlevel2">
<li><a href="#_theory_of_operation_2">Theory of operation</a>
<ul class="sectlevel3">
<li><a href="#_floating_gate_transistor">Floating gate transistor</a></li>
<li><a href="#_block">Block</a></li>
<li><a href="#_controller">Controller</a></li>
<li><a href="#_ssd_assembly">SSD assembly</a></li>
</ul>
</li>
<li><a href="#_performance_2">Performance</a></li>
<li><a href="#_experiments_2">Experiments</a>
<ul class="sectlevel3">
<li><a href="#_single_threaded_random_access">Single threaded random access</a></li>
<li><a href="#_concurrent_random_access">Concurrent random access</a></li>
<li><a href="#_sequential_access_2">Sequential access</a></li>
<li><a href="#_read_chunk_size">Read chunk size</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_higher_levels">Higher levels</a>
<ul class="sectlevel2">
<li><a href="#_interfaces">Interfaces</a>
<ul class="sectlevel3">
<li><a href="#_sata">SATA</a></li>
<li><a href="#_sas">SAS</a></li>
<li><a href="#_nvme">NVMe</a></li>
<li><a href="#_usb_mass_storage">USB mass storage</a></li>
<li><a href="#_comparison_2">Comparison</a></li>
</ul>
</li>
<li><a href="#_schedulers">Schedulers</a></li>
<li><a href="#_page_cache">Page cache</a></li>
<li><a href="#_filesystems">Filesystems</a></li>
</ul>
</li>
<li><a href="#_tips_and_tricks">Tips and tricks</a>
<ul class="sectlevel2">
<li><a href="#_optimizations">Optimizations</a>
<ul class="sectlevel3">
<li><a href="#_disable_atime">Disable atime</a></li>
<li><a href="#_squeeze_all_effective_space_out_of_filesystem">Squeeze all effective space out of filesystem</a></li>
<li><a href="#_use_tmpfs_or_ramfs_to_reduce_io">Use tmpfs or ramfs to reduce IO</a></li>
<li><a href="#_use_right_syscall_to_flush_buffers">Use right syscall to flush buffers</a></li>
<li><a href="#allocate-separate-partition-hdd">Allocate separate partition for fast data (HDD)</a></li>
<li><a href="#_use_reasonable_read_buffer_size">Use reasonable read buffer size</a></li>
<li><a href="#_specify_large_block_size_to_code_dd_code">Specify large block size to <code>dd</code></a></li>
<li><a href="#_pad_data_to_avoid_rmw">Pad data to avoid RMW</a></li>
<li><a href="#_align_partitions_by_physical_sector_to_avoid_rmw">Align partitions by physical sector to avoid RMW</a></li>
<li><a href="#_use_idle_scheduling_class_for_maintenance_jobs">Use idle scheduling class for maintenance jobs</a></li>
<li><a href="#_understand_performance_limitations_of_b_tree_indices">Understand performance limitations of B-tree indices</a></li>
<li><a href="#_sort_addresses_when_reading_in_bulk_hdd">Sort addresses when reading in bulk (HDD)</a></li>
</ul>
</li>
<li><a href="#_ad_hoc_tasks">Ad-hoc tasks</a>
<ul class="sectlevel3">
<li><a href="#_how_to_identify_drive_model">How to identify drive model</a></li>
<li><a href="#_use_hard_links_to_modify_large_directory_structures">Use hard links to modify large directory structures</a></li>
<li><a href="#when-file-content-is-deleted">Understand when file content is deleted</a></li>
<li><a href="#_recover_deleted_but_opened_file">Recover deleted but opened file</a></li>
<li><a href="#_prefer_code_tail_f_code_to_code_tail_f_code">Prefer <code>tail -F</code> to <code>tail -f</code></a></li>
<li><a href="#_do_not_underestimate_copy_time">Do not underestimate copy time</a></li>
<li><a href="#_dealing_with_text_file_busy_error">Dealing with &#8220;text file busy&#8221; error</a></li>
<li><a href="#_create_extent_files_instantly_with_code_fallocate_code">Create extent files instantly with <code>fallocate</code></a></li>
<li><a href="#_use_progress_meters">Use progress meters</a></li>
<li><a href="#_how_to_resize_ext4">How to resize ext4</a></li>
<li><a href="#_invalidate_page_cache_before_running_performance_tests">Invalidate page cache before running performance tests</a></li>
<li><a href="#_monitoring_io_kernel_interface">Monitoring IO: kernel interface</a></li>
<li><a href="#_monitoring_io_command_line_utilities">Monitoring IO: command line utilities</a></li>
</ul>
</li>
<li><a href="#_miscellaneous">Miscellaneous</a>
<ul class="sectlevel3">
<li><a href="#_distinguish_between_binary_and_decimal_units">Distinguish between binary and decimal units</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_appendix_drvperf">Appendix: drvperf</a></li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_introduction">Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Storage (disk) subsystem is poorly understood even by folks who work in bigdata area,
where optimizing performance by mere 5% may save a fortune.
Very few interviewees are able to answer questions about how data gets from application to storage
device and what factors affect performance of this process.
Extra complexity comes from the fact that there currently coexist two types of devices: HDD and SSD.
Each of them has unique set of benefits and drawbacks.
This results in terribly inefficient software designs sometimes.
Hardware planning also becomes more like a game of guessing rather than logical reasoning.
Under- and over- provisioning may easily happen, latter leading to project failure in worst case.</p>
</div>
<div class="paragraph">
<p>Goal of this article is to provide overall coverage of storage subsystem with main focus on performance.
It is split into theoretical and practical parts.
Theoretical part is dedicated to the components of IO stack with particular attention
to modern data storage devices: HDD and SSD.
Theory of operation provides the basis for explaining performance advantages and limitations of
corresponding device; real-world test results are included as well.
Practical part lists various methods of performance improvement and also gives hands-on advices
about everyday tasks.</p>
</div>
<div class="paragraph">
<p>Reader is expected to have previous experience of programming and system administration in Linux environment.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_theory">Theory</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_io_stack_overview">IO stack overview</h3>
<div class="paragraph">
<p>Let&#8217;s look at what happens when application makes IO request, such <code>read(2)</code>.
Each time request is issued, it passes through following layers:</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="the-stack.svg" alt="the stack" width="30%">
</div>
<div class="title">IO susbystem stack</div>
</div>
<div class="paragraph">
<p>First, request in the form of call to C function goes from userspace process into libc.
Server oriented Linux distributions use GNU libc (glibc), but others may use different
implementation, such as Bionic libc used in Android.
(g)libc role is to provide convenient means of interacting with kernel: it checks arguments
for invalid values, manipulates with registers to satisfy kernel calling conventions
and sets <code>errno</code> on return.
Without (g)libc, software engineers would have to write architecture-dependent assembler code
to perform syscalls themselves.</p>
</div>
<div class="paragraph">
<p>Next, IO syscall, which has the same name as corresponding (g)libc function, enters kernel space
and is routed to code of filesystem that is responsible for passed file descriptor.
Filesystem maps hierarchical structure of directories and regular files onto linear address space
of block device.
Depending on syscall, filesystem may trigger a bunch of IO requests to lower levels.
For example, reading portion of data from file may trigger read request and also a <em>write</em> request
to file&#8217;s metadata to update access time.</p>
</div>
<div class="paragraph">
<p>Next level is page cache.
Filesystem contents is extensively cached in RAM, so chances that requested data is found in page
cache are high in typical system.
If requested data is found here, it is returned immediately without propagating IO request
to lower levels.
Otherwise, it goes to IO scheduler, also called an elevator.</p>
</div>
<div class="paragraph">
<p>Scheduler is responsible for order in which requests are sent to storage device.
Usually this order is chosen to provide fairness among all processes with respect to IO priorities.
When device is ready to serve next request, scheduler pops next request from head of its queue
and passes it to driver.</p>
</div>
<div class="paragraph">
<p>Drivers may be organized into multi-tiered stack for better code reuse.
For example, topmost driver may be responsible for encoding IO requests into SCSI commands,
while bottomost driver may implement protocol of sending these SCSI commands to specific model
of disk controller.
Ultimate goal of driver stack is to provide transparent means of communicating with variety
of hardware controllers and interfaces.</p>
</div>
<div class="paragraph">
<p>Workstations and cheap servers have controller integrated into chipset, while more expensive
servers have separate dedicated controller chip soldered to motherboard;
standalone expander cards inserted into PCIe slots also exist.
Controller&#8217;s task is to provide reliable delivery of commands to connected hardware and their
responses back.
It converts commands between software and wire representation, computes checksums,
triggers hot-plug events and so on.
Some controllers may have their own small cache and IO scheduler, this is particularly true about
controllers capable of hardware RAID.</p>
</div>
<div class="paragraph">
<p>Ultimately, request reaches storage device.
Once it completes serving it, response is sent back and also passes through all these layers from
bottom to top until data and response code are delivered to userspace application.
This finalizes IO operation.</p>
</div>
<div class="paragraph">
<p>Many more layers may be plugged into these IO stack, for example, by using NFS or multi-tiered RAIDs.
Non-essential layers may also be removed: e.g. it is possible to bypass page cache by issuing
IO request with so called &#8220;direct&#8221; flag set on it.
Anyway, above stack is the core of IO subsystem of all sorts of computers ranging from smartphones
to enterprise servers.</p>
</div>
</div>
<div class="sect2">
<h3 id="_performance_metrics">Performance metrics</h3>
<div class="paragraph">
<p>As with all request-oriented systems, computer storage performance is measured by using
two common metrics: throughput and response time.</p>
</div>
<div class="paragraph">
<p><strong>Throughput</strong> comes in two flavours: either as read/write speed in megabytes per second (MB/s)
or as number of completed IO operations per second (IOPS).
First flavour is better known because it is reported by a variety of command line utilities
and GUI programs.
It is usually determinant in systems where operations are large enough, such as video streaming.
Conversely, IOPS is used primarily in contexts where operations are relatively small and are
independent from one another, such as retrieving rows of relational database by their primary keys.
Whether it is MB/s or IOPS, throughput is a macro metric in its nature: it doesn&#8217;t say anything
about each separate operation.
Whether you have single large operation or a group of smaller ones, whether IO occupies 100% of
application time or only fraction of it, whether identical operations take the same amount of time
to complete or whether they are orders of magnitude different&#8201;&#8212;&#8201;all these cases may produce
the same throughput value.</p>
</div>
<div class="paragraph">
<p>Using throughput as a single metric may be sufficient for offline batch processing systems.
High value means good hardware utilization, while low value means waste of resources (hardware).
But things become different if you are building online system performing multiple independent
requests in parallel.
Because you need to satisfy constraints for each separate request, <strong>response time</strong>&#8201;&#8212;&#8201;time spent
doing single IO operation (ms)&#8201;&#8212;&#8201;becomes a key factor.
Of course, we want it to be as small as possible.</p>
</div>
<div class="paragraph">
<p>In ideal world, response time is directly derived from throughput and request size:
<img src="math-ff2ab68ea431acdc.svg" class="inlinemath" style="height:4.547ex;vertical-align:-1.813ex;" alt="\mathrm{ResponseTime}=\frac{\mathrm{RequestSize}}{\mathrm{Throughput}}"/>.
However, there are a lot of subtleties in practice:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>response time can never be zero because storage device needs some time to <em>start</em> accessing data</p>
</li>
<li>
<p>operations may influence one another, so order of operations is important</p>
</li>
<li>
<p>if multiple operations are issued simultaneously, they may end up waiting in queue</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Result is that response time values are scattered along wide range even if requests are identical
in type and size.
As such, we need some way to combine them into single number.
Most often, <strong>average response time</strong> over fixed time window is used for that purpose
(&#8220;single operation completed in 15 ms on average during last minute&#8221;).
Unfortunately, this naive metric is too general to reveal any significant information.
Consider three different systems, each completing the same sequence of 10 operations.
Lists of response time values are presented below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>(1) 25 25 25 25 25 25 25 25 25 25
(2) 45 45 45 45 45 5 5 5 5 5
(3) 205 5 5 5 5 5 5 5 5 5</pre>
</div>
</div>
<div class="paragraph">
<p>All these systems have the same average response time of 25 time units, so they are completely
indistinguishable.
But it is obvious that some of them are preferable to the others.
Which one is the best?
This depends on the application.
If these operations are independent (such as in file server) then I&#8217;d say that (3) is the best one:
system managed to complete almost all operations very fast&#8201;&#8212;&#8201;in under 5 time units each;
one slow operation is not a big problem.
On the other hand, if these operations are part of some larger entity and are simultaneously issued
(so that total time is limited by the slowest operation), then (1) is the best.</p>
</div>
<div class="paragraph">
<p>Metric that takes into consideration all such sorts of reasoning is <strong>percentile</strong>.
Actually, it is not a single metric but rather a family of metrics with given parameter <img src="math-4b68ab3847feda7d.svg" class="inlinemath" style="height:1.375ex;vertical-align:-0.0ex;" alt="X"/>.
<img src="math-4b68ab3847feda7d.svg" class="inlinemath" style="height:1.375ex;vertical-align:-0.0ex;" alt="X"/>-percentile is defined as such response time that <img src="math-6bbb8af3273cfb4f.svg" class="inlinemath" style="height:1.406ex;vertical-align:-0.031ex;" alt="X\%"/> of operations are faster
than this value and <img src="math-674fad3b884d45b7.svg" class="inlinemath" style="height:1.734ex;vertical-align:-0.359ex;" alt="(100-X)\%"/> percent of operations are slower.
<img src="math-4b68ab3847feda7d.svg" class="inlinemath" style="height:1.375ex;vertical-align:-0.0ex;" alt="X"/> is usually selected to be 90%, 95%, 99% or 100% (last value is simply the slowest
operation in given time window).
Three above systems have 90-percentile response times of 25, 45 and 5 time units respectively.
Idea of percentile metric is that if you are satisfied with 90-percentile response time,
then you are definitely satisfied with all response times less or equal to 90-percentile.
Specific values are of no importance.</p>
</div>
<div class="paragraph">
<p>Variations are also in use.
It may be convenient to keep track of multiple percentiles simultaneously: 95, 99, 100.
Or, if there is agreed target response time value <img src="math-e632b7095b0bf32c.svg" class="inlinemath" style="height:1.359ex;vertical-align:-0.0ex;" alt="T"/>, then it makes sense to use &#8220;inverse&#8221;
of percentile&#8201;&#8212;&#8201;percent of operations that are slower than <img src="math-e632b7095b0bf32c.svg" class="inlinemath" style="height:1.359ex;vertical-align:-0.0ex;" alt="T"/>.</p>
</div>
<div class="paragraph">
<p>Because all described metrics are connected together, it is not possible to optimize one without
sacrifying others.
Some projects define their goal as optimization of single metric, while others have a combination
of constraints.
Examples of such goals:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Web server (public service):
be able to handle at least 1000 IOPS by keeping 95-percentile response time at 500 ms level.
Goal here is to provide virtually instant service to almost all of 1000 users who visit site
at peak hour.</p>
</li>
<li>
<p>Video projector (realtime system):
be able to read each separate frame in no more than 40 ms.
Goal here is to prevent buffer underflow so that video sequence is not interrupted
(assuming 25 frames per second).</p>
</li>
<li>
<p>Backup storage (batch system):
provide uninterruptible read speed (throughput) of at least 70 MB/s.
Required so that typical backup image of 250 GB can be restored in one hour in case of
critical data loss.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_access_patterns">Access patterns</h3>
<div class="paragraph">
<p>What most people fail to understand is that throughput figure (MB/s) given in device specification
will virtually never be achieved.
The reason for that comes out from the fact that response time of each single request is limited by
<strong>access time</strong>&#8201;&#8212;&#8201;extra price that must be paid for each single request, no matter how small it is.
General formula for computing response time of single request is following:</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="math-22e093e4f7841894.svg" alt="\mathrm{ResponseTime\;[s]} = \mathrm{QueueWaitTime\;[s]} + \mathrm{AccessTime\;[s]} + \frac{\mathrm{RequestSize\;[MB]}}{\mathrm{Throughput\;[MB/s]}}" width="65%">
</div>
</div>
<div class="paragraph">
<p>Access time is a combination of all sorts of delays not dependent on request size:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Each request must pass through number of software layers before it is sent to device:
userspace &#8594; kernel &#8594; scheduler &#8594; driver(s).
Response also passes these layers but in reverse direction.</p>
</li>
<li>
<p>Transfer by wire also has some latency, which is particularly large for remote NASes.</p>
</li>
<li>
<p>Finally, device may need to make some preparations after receiving request and before
it will be able to stream data.
This factor is the major contributor when HDDs are used: HDD must perform mechanical seek,
which takes order of milliseconds.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>But once random access price is paid, data comes out with fixed rate limited only by throughput
of device and connecting interface.
Chart below displays relation between request size and effective throughput for three hypothetical
devices with the same designed throughput (200 MB/s) but different access times.
Even to get 50% of efficiency (100 MB/s), someone needs to make requests of 32 KiB, 256 KiB
and 2 MiB respectively.
Issuing small requests is very inefficient, this can be easily observed when copying directory
containing large number of small files.
Consider following analogy: it is unfeasible to enter a store in order to purchase bottle of beer
for $1 if entrance fee is $100.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="the-throughput-efficiency.svg" alt="the throughput efficiency" width="90%">
</div>
<div class="title">Dependence of effective throughput on request size and access time (model)</div>
</div>
<div class="paragraph">
<p>Such sigmoidal behaviour led to idea of differentiaing access patterns into two classes:
sequential and random.
Most of real world applications generate IO requests belonging to strictly one of these two classes.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Sequential access</dt>
<dd>
<p>Sequential access arises when requests are large enough so that access time plays little role;
response time is dominated by time it takes to transfer data from storage media to application
or vice versa.
Throughput (MB/s) is the main metric for such type of pattern.
This is optimal mode of operation for storage devices&#8201;&#8212;&#8201;throughput efficiency is somewhere
in the right part of above graph and is close to 100%.
All following tasks cause sequential access:</p>
<div class="ulist">
<ul>
<li>
<p>making a copy of large file (in contrast to copying a lot of small files)</p>
</li>
<li>
<p>making backup of raw block device or partition</p>
</li>
<li>
<p>loading database index file into memory</p>
</li>
<li>
<p>running web server to stream HD-quality movies</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Random access</dt>
<dd>
<p>Random access is the opposite of sequential access.
Requests are so small that second summand of above formula&#8201;&#8212;&#8201;access time&#8201;&#8212;&#8201;plays primary role.
IOPS and response time percentile metrics become more important.
Random access is less efficient but is inevitable in wide range of applications, such as:</p>
<div class="ulist">
<ul>
<li>
<p>making copies of large number of small files (in contrast to making a copy of single large file)</p>
</li>
<li>
<p>issuing SQL SELECTs, each one returning single row by its primary key</p>
</li>
<li>
<p>running web server to provide access to small static files: scripts, stylesheets, thumbnails</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>It should be obvious that some operations are not purely sequential and are not purely random.
Middle of the graph is exactly the place where both random and sequential components make
significant contribution to response time.
Neither of them may be ignored.
Such access pattern may be a result of careful request size selection to keep balance between
response time and throughput, for example in multiclient server environments.</p>
</div>
</div>
<div class="sect2">
<h3 id="_performance_related_concepts">Performance-related concepts</h3>
<div class="paragraph">
<p>Concepts presented below are not exclusive for mass storage devices, but are the same
for all types of memories found at all levels of application stack, ranging from DRAM chips
to high-level pieces of software such as distributed key-value databases as a whole.
These concepts are implemented at least twice in respect to storage stack:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>in OS kernel</p>
</li>
<li>
<p>in intermediate electronics, such as port expanders and hardware RAID controllers (if present)</p>
</li>
<li>
<p>in storage device itself</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_caching">Caching</h4>
<div class="paragraph">
<p>Storage devices are very slow compared to other types of memory.
Look at the image below, it visualizes memory hierarchy found in modern computers.
When moving from top to bottom, memory sizes grow in capacity, but tradeoff is more expensive access time.
Largest drop occurs between storage devices and RAM: it is order of 1,000 for SSD and 10,000 for HDD.
This should not come as a surprise because storage device is the only one in this hierarchy
capable of persistency (ability to store data without power for prolonged amount of time),
which doesn&#8217;t come for free.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="the-memory-hierarchy.svg" alt="the memory hierarchy" width="80%">
</div>
<div class="title">Memory hierarchy</div>
</div>
<div class="paragraph">
<p>If we collect statistics about accesses to storage, then we may see that only small fraction of data
is accessed often, while large bulk of remaining data is virtually never accessed.
Actual distribution varies depending on purpose of application, but the general concept of dividing
data into &#8220;hot&#8221; and &#8220;cold&#8221; proves to be correct for vast majority of applications.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="the-access-distribution.svg" alt="the access distribution" width="80%">
</div>
<div class="title">Dependence of request frequency on address (model)</div>
</div>
<div class="paragraph">
<p>This observation leads to idea of caching storage data&#8201;&#8212;&#8201;making copy of &#8220;hot&#8221; data in much faster RAM.
Caching reduces access times to nearly zero for &#8220;hot&#8221; data and it also helps to increase throughput
but to a lesser extent.
Caching is so natural and effective that it is implemented in all nodes on the path from
storage device to user application:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>storage devices have internal caches</p>
</li>
<li>
<p>hardware RAID controllers and port expanders may have caches</p>
</li>
<li>
<p>operating system maintains disk cache in RAM</p>
</li>
<li>
<p>applications may implement their own domain specific caches</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Usually disk cache found in OS is the most important cache because it is the largest one.
Hardware cache sizes are fixed during production and rarely exceed 1 GB,
while disk cache is limited only by available RAM, which is much easier to expand.
Operating system is free to use all RAM not directly requested by applications to transparently
maintain cache.
It is not uncommon to see disk caches of hundreds of gigabytes.
This even led to software architercture pattern when application depends on the fact that all data
is cached as part of normal workflow and application would fail to meet performance goals otherwise.</p>
</div>
</div>
<div class="sect3">
<h4 id="_speculative_reads">Speculative reads</h4>
<div class="paragraph">
<p>Idea of speculative reads is to anticipate read requests from upper layer.
When requests are scattered randomly all over the storage address space, no prediction is possible.
But when order of previously accessed addresses is regular, it is easy to make prediction on what
addresses will be accessed next.
Set of recognized patterns varies between different types of memory and implementations.
In data storaging, almost always <strong>read-ahead</strong> is implemented: it recognizes basic pattern when
addresses are accessed in sequence in ascending order.
For example, if data from addresses <img src="math-f9dbaba128a60b43.svg" class="inlinemath" style="height:1.734ex;vertical-align:-0.391ex;" alt="i,"/> <img src="math-87f52930a1cb70fd.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="i+1,"/> <img src="math-4cb394a6a554fbf3.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="i+2,"/> <img src="math-ef39591411d94c0d.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.031ex;" alt="i+3"/> was requested,
then read-ahead may decide to speculatively read data from locations <img src="math-c0e943b62821ff6a.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="i+4,"/> <img src="math-dec1bfb08eaf6b35.svg" class="inlinemath" style="height:1.781ex;vertical-align:-0.391ex;" alt="i+5,"/>&#8230;&#8203; into cache.
Conversely, speculative read of addresses which are accessed in descending order is called <strong>read-behind</strong>.</p>
</div>
<div class="paragraph">
<p>Read-ahead and read-behind are typically implemented in all layers of storage stack where there
is cache present.
As with caching, most notable implementation is found in OS.
OS has much more information compared to hardware: it is able to relate requested addresses with files,
it knows exactly which process issued request and its priority.
As such, mechanism of speculative reads fits naturally into operating system&#8217;s IO scheduler.
Being a software implementation, it is also very flexible: it allows to modify settings for each
separate file programmatically.
Software engineer may wish to increase volume of data read in advance for some file or, vice versa,
to entirely disable speculative reads.
Storage devices also have read-ahead implementation but it is limited to just reading couple of hundreds
kilobytes of further located data (regarding last request) if device becomes idle.
This may help in embedded systems without sophisticated IO scheduler.</p>
</div>
<div class="paragraph">
<p>Speculative reads may be executed online or in background.
In the first case, initial request size is extended by read-ahead/read-behind value.
For example, if you issue request to read 100 KB then it may be transparently extended to 250 KB:
first 100 KB is returned to you as was requested, while remaining part is stored in cache.
This takes a bit longer to complete but still comes out beneficial for devices with long access time,
such as HDDs.
Background speculative reads work similar.
The only difference is that they are done by additional requests, which are issued in background,
when there are no requests of higher priority to serve.</p>
</div>
</div>
<div class="sect3">
<h4 id="_writeback">Writeback</h4>
<div class="paragraph">
<p>Reads from memory are always synchronous: you can&#8217;t return control to requesting application
without actual data.
When application asks OS kernel to read some piece of data, application is blocked while
data is physically being fetched from storage or from cache.
But writing is another story.
Think of write as of <em>delayed</em> request.
Operating system does not interact with storage during <code>write()</code> system calls by default.
Instead, it makes a copy of user supplied data into kernel buffer and returns control
to application immediately.
Copying to RAM is very fast compared to access to storage and has to be done anyway
because hardware (DMA) requires that buffer has specific alignment and location in physical memory.
Actual write happens at some point later in background (typically in no more that in couple of seconds
after requested), concurrently with unblocked application.
Hence, the terms <strong>writeback</strong> and <strong>write buffering</strong>.
Such approach reduces running time to 50% in best case for applications which would be otherwise
active for one half of time and blocked by write IO for another half.
Without buffered writes, software engineers would have to use multiple threads or nonblocking IO
explicitly to achieve the same result, which is longer to code and harder to debug.</p>
</div>
<div class="paragraph">
<p>Problem with writeback is that if something goes wrong&#8201;&#8212;&#8201;for example, power is lost&#8201;&#8212;&#8201;then
buffered but not yet executed writes are also lost.
In order to resolve this issue, operating systems provide means of explicit synchronization.
These are presented by system calls of different granularity levels which block application
execution until all previously issued writes are actually executed.</p>
</div>
<div class="paragraph">
<p>It is important to remember that writes and reads are <em>very different</em>&#8201;&#8212;&#8201;they are by no means symmetric.
In particular, if <code>write()</code> of 5 GB of data seems to complete in one second, it doesn&#8217;t mean that
your disk has speed of 5 GB/s.
Data is not written yet at this moment, but only buffered in kernel.</p>
</div>
</div>
<div class="sect3">
<h4 id="_write_combining_and_reordering">Write combining and reordering</h4>
<div class="paragraph">
<p>Buffering of writes is beneficial to performance not only because of increased concurrency,
but it also makes possible for OS to make device-dependent optimizations, which are transparent
to user application.</p>
</div>
<div class="paragraph">
<p>First optimization is <strong>write combining</strong>.
Its idea is that applications tend to use small buffers to write large sequential chunks of data.
Instead of issuing each small request to storage device separately, OS groups them into larger requests.
This results in that access time cost needs to be paid much rarely on average than it would be paid otherwise.</p>
</div>
<div class="paragraph">
<p>Another optimization is <strong>write reordering</strong>.
Whether and how it is used depends on type of storage device.
For example, access time for hard disk drive depends on absolute delta between previously accessed
address and currently accessed address among other things.
If three requests come in sequence to addresses <img src="math-16712ce9fa88f344.svg" class="inlinemath" style="height:1.781ex;vertical-align:-0.391ex;" alt="5,"/> <img src="math-7b3565ecfca35b55.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="100,"/> <img src="math-60263a314085db8e.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="10,"/> then it is
beneficial to reorder them to <img src="math-16712ce9fa88f344.svg" class="inlinemath" style="height:1.781ex;vertical-align:-0.391ex;" alt="5,"/> <img src="math-60263a314085db8e.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="10,"/> <img src="math-ad57366865126e55.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.031ex;" alt="100"/>.
This reduces cumulative delta from <img src="math-9098a1cbdd7aab21.svg" class="inlinemath" style="height:2.0ex;vertical-align:-0.5ex;" alt="\lvert{100-5}\rvert + \lvert{100-10}\rvert = 185"/> to
<img src="math-5c5295550cd316da.svg" class="inlinemath" style="height:2.0ex;vertical-align:-0.5ex;" alt="\lvert{5-10}\rvert + \lvert{10-100}\rvert = 95"/>.</p>
</div>
<div class="paragraph">
<p>Write reording creates problem of <strong>write hazards</strong>.
Applications may depend on strict ordering of request execution to guarantee logical consistency of data.
For example, consider two applications&#8201;&#8212;&#8201;one producer and one consumer&#8201;&#8212;&#8201;which communicate
by means of filesystem.
When producer application wants to send some data, it makes two requests: first it writes new data
and then writes pointer to new data into predefined location.
If order of requests is reversed by OS, then it is possible that consumer application reads
new pointer value first.
It makes further read by dereferencing this pointer and sees some garbage because actual data
has not been written yet.
Applications which depend on strong ordering must explicitly enforce it by using synchronization
system calls.
Issuing such syscall between two writes would remove write hazard in above example.</p>
</div>
</div>
<div class="sect3">
<h4 id="_read_modify_write">Read-modify-write</h4>
<div class="paragraph">
<p>Memory devices rarely allow to access single bits and even bytes.
Such limitation is the result of internal device organization and also simplifies protocols
and ECC handling.
Minimal unit for mass storage devices is sector, which is 512 or 4096 bytes depending on device model.
When OS sends request to device, it specifies index of first sector, number of sectors to read/write
and pointer to data buffer.
Devices do not have any API to access sectors partially.</p>
</div>
<div class="paragraph">
<p>Such strong limitation is not convenient for user applications, so OS transparently transforms
arbitrary byte-aligned user requests into larger sector-aligned requests.
This doesn&#8217;t create substantial performance problems for reads: access time cost must be paid anyway,
and it is much larger than time required to read extra bytes.</p>
</div>
<div class="paragraph">
<p>Situation with writes is much worse than that.
When application makes write request with address or size not aligned by sector size, OS performs
transparently infamous read-modify-write sequence (RMW for short):</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>first, sectors which must be written only partially, are read (#5 and #9 in image below)</p>
</li>
<li>
<p>then data from these sectors which needs to be preserved is masked and merged into main buffer</p>
</li>
<li>
<p>and only then all affected sectors are written to storage device (#5, #6, #7, #8, #9)</p>
</li>
</ol>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="the-rmw.svg" alt="the rmw" width="80%">
</div>
</div>
<div class="paragraph">
<p>It doesn&#8217;t seem to create a performance degradation at first glance because only first and last sectors
may be written partially, no matter how large request is.
Inner sectors are always written in full and, thus, don&#8217;t need to be read first.
But the problem is that above steps can&#8217;t be performed simultaneously&#8201;&#8212;&#8201;only step by step.
This results in that effective write response time becomes poor, it may be twice as long compared to
read response time in worst case.</p>
</div>
<div class="paragraph">
<p>If write request is issued exactly by sector boundaries, then RMW doesn&#8217;t happen and there is
no performance penalty.
Note that high-level hardware (RAID arrays) and software (OS cache and filesystem) may increase minimal
RMW-avoidable unit from sector size to even higher value.</p>
</div>
</div>
<div class="sect3">
<h4 id="_parallelism">Parallelism</h4>
<div class="paragraph">
<p>Most of elementary memory devices are able to serve only single request at a time and demonstrate
poor sequential speed.
Combining multiple such devices together results in better performance.
This technique is commonly employed in all types of memory:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>SSDs are equipped with 4-16 flash chips, each chip has 2-8 logical units (LUNs)</p>
</li>
<li>
<p>DRAM modules contain 8-16 DRAM chips per rank</p>
</li>
<li>
<p>DRAM controller has 2 channels</p>
</li>
<li>
<p>RAID arrays are built atop of 2-8 slave devices</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Such organization is beneficial to performance of both sequential and random access patterns.</p>
</div>
<div class="paragraph">
<p>Sequential access is faster due to <strong>interleaving</strong>.
Idea is to organize addressing in such way that parts of single large request are served
simultaneously by different elementary memory devices.
If these parts turn out to be identical in size, then effective sequential speed is multiplied
by number of attached elementary devices.
The more devices are connected together, the faster cumulative sequential speed is.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="the-interleaving.svg" alt="the interleaving" width="90%">
</div>
<div class="title">Perfect interleaving</div>
</div>
<div class="paragraph">
<p>Random access benefits from the fact that large number of small requests may be served in truly
parallel way provided they are evenly distributed across elementary devices.
Without such organization, requests would have to wait in queue for prolonged periods of time,
thus worsen response time from software perspective.</p>
</div>
<div class="paragraph">
<p>Good thing about parallelism is that its scalability is virtually infinite: by combining more and
more elementary memory devices together it is possible to achieve hundreds and thousands times
faster sequential speed or to be able to serve hundreds and thousands simultaneous requests
in truly parallel way.
Bad thing is that <em>access time</em> can&#8217;t be lowered by such technique.
Even if you managed to get x1000 improvement in throughput, response time of small requests
is still the same as if only single elementary device was present.
Access time and, as a result, overall response time of standalone request can be improved only
by switching to faster technology.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_hard_disk_drive_hdd">Hard disk drive (HDD)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Hard disk drives first appeared in 1950s.
Rapid advancement in technology led to increase in capacity by orders of magnitude, so that modern HDD is able to store up
to 10 TB of data with price as low as 2.5 cents per gigabyte.
Serious disadvantage of hard disk drives is that their operation relies on mechanical principles.
Actually, HDD is the last of devices in computer which is not based on purely electronical or eletrical phenomena
(cooling fans are not taken into account because they are not involved in processing of data).
As a result, hard disk drives have following disadvantages:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>HDDs are prone to mechanical failure.
Vibrations or elevated temperatures found in highly packed server rooms make HDDs to fail after only couple of years
of service.</p>
</li>
<li>
<p>Precise mechanical operation requires using heavy metallic components&#8201;&#8212;&#8201;it&#8217;s impossible to use HDDs in mobile devices.
Typical 3.5" HDD weights about 0.7 kg.</p>
</li>
<li>
<p>And, most important, very long access time, which didn&#8217;t improve greatly since the advent of HDDs.
This is primary reason why HDDs are steadily being obsoleted by SSDs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>However, HDD&#8217;s killer feature&#8201;&#8212;&#8201;very low price per gigabyte&#8201;&#8212;&#8201;makes them number one choice in applications
where price is more important than performance, such as multimedia storages, batch processing systems and backuping.</p>
</div>
<div class="sect2">
<h3 id="_theory_of_operation">Theory of operation</h3>
<div class="sect3">
<h4 id="_internals">Internals</h4>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-platter.svg" alt="hdd platter" width="100%">
</div>
<div class="title">HDD platter (left) and platter assembly (right)</div>
</div>
<div class="paragraph">
<p>HDD stores information in magnetic form on the surface of <strong>platter</strong>.
It is covered with thin layer of ferromagnetic material and is perfectly polished.
While platter surface seems like a single entity from macroscopic point of view,
it is divided logically into very small magnetic regions (~30x30 nm).
Each such magnetic region has its own direction of magnetization, which is used to encode either &#8220;0&#8221; or &#8220;1&#8221;.
Direction of magnetization may be changed locally during write operation,
which is performed by applying strong external magnetic field exactly to the target region.
Each of regions is also may be read by passing coil atop of it: direction of current generated by electormagnetic induction
depends on direction of region magnetization.
Read and write operations are perfected to such extent, that each of them takes only couple of nanoseconds to act on each
separate magnetic region.
This translates to speeds of more than 100 MB/sec.</p>
</div>
<div class="paragraph">
<p>To increase HDD capacity, each platter has two magnetic surfaces, and a group of identical platters
is mounted together to form <strong>platter assembly</strong>.
Maximal number of platters in assembly depends on disk form factor and height: 3.5" disks may have 1-7 platters,
while 2.5" disks&#8201;&#8212;&#8201;only 1-3.
Thus, 3.5" disk may have maximum of 14 surfaces.
Number of platters is rarely specified in datasheets, so you may want to consult
<a href="http://rml527.blogspot.ru/2010/09/hdd-platter-capacity-database.html">The HDD Platter Capacity Database</a>.
This resource was created by enthusiast with sole purpose to provide means of getting info about platter configuration without
need to disassemble HDD.
You may note that some drives have weird setups.
For example, it is common for one of platters to have only single functional surface in a two-platter HDD
(total of 3 surfaces).
Another example is HDD where area of some surfaces is used only partially.
Usually such setups are created in order to fulfil marketing demand for full range of capacities:
instead of manufacturing completely different drives per each capacity value,
it may be cheaper to produce HDD with large single fixed capacity
and than to create submodels of it with reduced capacites by locking some surfaces in firmware.</p>
</div>
<div class="paragraph">
<p>All platters are mounted on a single spindle and are synchronously rotated with fixed speed.
Once HDD is powered on, electrical motor spins up platter assembly to nominal speed and this rotation continues
until HDD is powered down or goes into power-saving mode.
This speed, which is called &#8220;rotational speed&#8221;, is measured in revolutions per minute (RPM) and
makes significant effect on both sequential and random access performance.
Typical PC or server 3.5" disk drive has rotational speed of 7200 RPM, which is a de-facto standard.
Laptop 2.5" HDDs are usually slower&#8201;&#8212;&#8201;only 5400 RPM, but some models are as fast as their larger associates&#8201;&#8212;&#8201;7200 RPM.
Very fast drives also exist&#8201;&#8212;&#8201;for most demanding applications&#8201;&#8212;&#8201;which rotate at 10,000 RPM and even 15,000 RPM.
Often, such drives are called &#8220;raptors&#8221;, name given after popular series of such disks&#8201;&#8212;&#8201;Western Digital Raptor.
The problem of fast drives is that they have order of magnitude less capacity and because of their high cost.
Currently, they are becoming nearly extinct because of introduction of SSDs, which are cheaper and even faster.</p>
</div>
<div class="paragraph">
<p>Read and write are performed by magnetic heads, one head per platter surface.
All heads are mounted on the very end of <strong>actuator arm</strong>.
Actuator arm may be rotated on demand by arbitrary angles.
This operation positions all heads simultaneously onto concentric <strong>tracks</strong>.
Positioning is relatively fast&#8201;&#8212;&#8201;it takes only couple of milliseconds to complete, which is achieved with accelerations of
hundreds of <code>g</code>, and is also very precise&#8201;&#8212;&#8201;modern disk drives have more than 100,000 concentric tracks per inch.
Actuator arm repositioning can be easily heard: it creates audible clicking sound during operation.</p>
</div>
<div class="paragraph">
<p>Each <strong>track</strong> is logically is divided by number of <strong>sectors</strong> where each sector stores 512 or 4096 bytes of useful
information depending on model.
Each sector is a basic unit of both read and write operations.
Such configuration makes it possible to access whole platter surface: actuator arm selects concentric tracks
and constant rotation of platter assembly guarantees that each of sectors is periodically passed under head.</p>
</div>
<div class="paragraph">
<p>For addressing purposes, virtual combination of vertically stacked tracks (one track per surface) is called <strong>cylinder</strong>.
For example, if HDD has 5 platters each with 2 surfaces, than each cylinder consists of 10 vertically stacked tracks,
one per surface.
When you reposition actuator arm&#8201;&#8212;&#8201;you reposition it to some cylinder.
This makes each of heads to be located above corresponding track of this cylinder.</p>
</div>
<div class="paragraph">
<p>Besides platter assembly, HDD is equipped with electronical controller.
It acts like a glue between host and magnetic storage and is responsible for all sorts of logical operations:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>it governs actuator arm repositioning</p>
</li>
<li>
<p>computes and verifies error-correcting code (ECC)</p>
</li>
<li>
<p>caches and buffers data</p>
</li>
<li>
<p>maps addresses of &#8220;bad&#8221; sectors into spare ones</p>
</li>
<li>
<p>reorders requests to achieve better performance (NCQ)</p>
</li>
<li>
<p>implements communication protocol: SATA or SAS</p>
</li>
</ul>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-controller.svg" alt="hdd controller" width="80%">
</div>
<div class="title">HDD controller components</div>
</div>
</div>
<div class="sect3">
<h4 id="_sector_size">Sector size</h4>
<div class="paragraph">
<p>Each track is divided into <strong>sectors</strong>.
All sectors carry user data of fixed length, 512 or 4096 bytes.
Important to note that sector is a <em>minimal unit</em> of data which may be read or written to.
It is not possible to read or write only part of sector, but it is possible to request disk drive to read/write
sequence of adjacent sectors.
For example, prototype for blocking read function inside driver could be like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="c"><span class="tok-kt">int</span> <span class="tok-nf">hw_read</span><span class="tok-p">(</span><span class="tok-kt">size_t</span> <span class="tok-n">IndexOfFirstSectorToRead</span><span class="tok-p">,</span> <span class="tok-kt">size_t</span> <span class="tok-n">NumberOfSectorsToRead</span><span class="tok-p">,</span> <span class="tok-kt">void</span><span class="tok-o">*</span> <span class="tok-n">Buffer</span><span class="tok-p">);</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>User data is not the only part of sector.
As displayed in image below, each sector carries additional bytes which are used internally by HDD.
Intersector gap and sync marker areas help HDD to robustly synchronize to the beginning of next sector.
ECC area stores error-correcting code for user data.
ECC becomes more and more important with growing HDD capacities.
Even with ECC, modern HDDs do not guarantee to be error-free.
Typical HDD allows 1 error bit per 10<sup>15</sup> bits on average (1 error per 125 TB).
This figure is of little interest for surveillance video storage but may become a cover-your-ass problem for storing
numerical financial data.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-sector-layout.svg" alt="hdd sector layout" width="100%">
</div>
<div class="title">Sector layout</div>
</div>
<div class="paragraph">
<p>One important characterisitc of HDD that influences performance is sector size.
Sector size is always specified as number of user data bytes available per each sector.
Standard value is 512 bytes.
Such HDDs have been used for decades and are still mass produced.
512 byte block is also a default unit for most of unix commands dealing with files and filesystems, such as <code>dd</code> and <code>du</code>.
After introduction of HDDs with other sector sizes, it became necessary to distinguish HDDs by sector size.
Common way is to use term <strong>512n</strong> which stands for &#8220;512 native&#8221;.
Also, &#8220;physical&#8221; is used interchangeably with &#8220;native&#8221;.
Thus, terms &#8220;512n&#8221;, &#8220;512 physical&#8221;, &#8220;512 native&#8221; all identify HDD with 512 byte sectors.</p>
</div>
<div class="paragraph">
<p>Intense rivalry for higher HDD capacity forced manufacturers to increase standard sector size from 512 bytes to larger value.
The reason behind this decision is that system data occupies nearly constant size:
intesector gap, sync marker and sector address are required only once per sector and do not depend on the amount of data
stored in user data section.
Overhead of system data becomes lower with increase in sector size, which adds a bit more capacity to HDD.
So, new standard value for sector size emerged&#8201;&#8212;&#8201;4096 bytes.
Such sector size is called &#8220;Advanced Format&#8221; (AF) and is a relatively new advancement, since 2010.
AF is implemented by most of HDDs with multi-terabyte capacites and is advertised as <strong>4Kn</strong> (or 4096n).</p>
</div>
<div class="paragraph">
<p>Third option exists to provide backward compatability to operating systems which do not support AF:
4096 byte physical sector with 512 logical sectors.
When using such drives, OS talks to HDD in terms of 512-byte logical sectors.
But internally each 8 such logical sectors are grouped together and form single physical 4096 sector.
Such drives are advertised as <strong>512e</strong> (&#8220;512 emulation&#8221;).
Modern operating systems are aware of the fact that logical and physical sector sizes may be different and always issue
requests in units of physical sector sizes (4 KiB for 512e).
This doesn&#8217;t create any performance penalties.
But if some older OS, thinking that drive really has 512 byte sectors, makes write request in violation with 4 KiB alignment rule,
then drive accepts this request but performs read-modify-write (transparently) because only part of physical 4 KiB sector is
modified.
No need to say that this severely cripples performance.</p>
</div>
<div class="paragraph">
<p>In linux, it is possible to find out information on sector sizes by looking into sysfs.
Replace <code>sda</code> with the name of device you want to know about.
This should be raw block device name and not a partition.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ cat /sys/class/block/sda/queue/logical_block_size
512
$ cat /sys/class/block/sda/queue/physical_block_size
4096</pre>
</div>
</div>
<div class="paragraph">
<p>It may be worth to note that some enterprise class drives have a bit larger sector size than canonical values:
512+8, 512+16, 4096+16, 4096+64 or 4096+128 bytes.
Such extensions are part of SCSI protocol and are called DIF/DIX (Data Integrity Field/Data Integrity Extensions) or
PI (Protection Information).
Extra bytes are used to store checksums computed by software or by RAID controller rather by disk drive itself.
Such approach allows to make verification of data integrity more robust.
It verifies storage subsystem as a whole, including not only HDDs but also all intermediate electronics and buses.</p>
</div>
</div>
<div class="sect3">
<h4 id="_addressing">Addressing</h4>
<div class="paragraph">
<p>Next thing we need to know is sector addressing.
Addressing scheme is very important for performance as it allows OS to optimize its requests,
and that&#8217;s why addressing scheme is kept similar among all HDDs.</p>
</div>
<div class="paragraph">
<p>Very old HDDs were addressed by using vector of three numbers&#8201;&#8212;&#8201;(cylinder, head, sector)&#8201;&#8212;&#8201;called CHS for short.
Single HDD had fixed number of cylinders, fixed number of heads, and fixed number of sectors per track.
All these values were reported to BIOS during system initalization.
If you wanted to access some sector, then you would send its three coordinates to disk drive.
Cylinder value specified position where to move actuator arm to, head value activated corresponding head,
and, after that, HDD waited until track with given track number passed under activated head while platter assembly
was being constantly rotated.</p>
</div>
<div class="paragraph">
<p>Access to adjacent sectors belonging to single track is cheap,
while access to sectors belonging to tracks of different cylinders is expensive.
Filesystem make use of this fact to optimize allocation algorithms.
In particular, they try to make files to occupy nearby sectors rather than far away sectors.</p>
</div>
<div class="paragraph">
<p>CHS scheme became obsolete, partly because using fixed number of sectors per track is not optimal in terms of HDD capacity.
Outer sectors are longer compared to inner sectors (left image), but store the same amount of information.
This inspired manufacturers to create technology which is called ZBR&#8201;&#8212;&#8201;zone bit recording.
Idea of ZBR is to group cylinders by sector density.
Number of sectors per track is fixed inside single group, but grows gradually when moving from inner group to outer group.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-zbr.svg" alt="hdd zbr" width="70%">
</div>
<div class="title">No ZBR (left), ZBR with two zones (right)</div>
</div>
<div class="paragraph">
<p>Downside of such approach is that addressing scheme becomes very complex.
OS needs to know number of cylinder groups and number of sectors per track for each such group.
Instead of exposing these internals to OS, manufacturers adopted simpler addressing scheme called LBA (logical block addressing).
When OS works with LBA enabled disk drives, it needs to specify only flat sector address in range <img src="math-6857e77bbb82aae7.svg" class="inlinemath" style="height:1.703ex;vertical-align:-0.313ex;" alt="[0\;..\;{N{-}1}"/>],
where <img src="math-8ce86a6ae65d3692.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.0ex;" alt="N"/> is total number of sectors.
HDD controller translates this flat address into geometrical coordinates: cylinder, cylinder group, track, sector,&#8201;&#8212;&#8201;and this translation algorithm is transparent to OS.</p>
</div>
<div class="paragraph">
<p>Though exact algorithm to map LBA to geometrical coordinates is not known to OS, mapping is somewhat predictable.
This is done intentionally to allow filesystems to continue to employ their optimizations.
In particular:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Access to nearby LBA addresses is cheaper than access to far away addresses.
For example, it is better to access sectors <img src="math-f9dbaba128a60b43.svg" class="inlinemath" style="height:1.734ex;vertical-align:-0.391ex;" alt="i,"/> <img src="math-87f52930a1cb70fd.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="i+1,"/> <img src="math-cf735962ecf8773c.svg" class="inlinemath" style="height:1.375ex;vertical-align:-0.016ex;" alt="i+2"/> rather than <img src="math-f9dbaba128a60b43.svg" class="inlinemath" style="height:1.734ex;vertical-align:-0.391ex;" alt="i,"/>
<img src="math-72c8fcd5bc964f66.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="i+1000,"/> <img src="math-6c0d1b16f7ca7a49.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.031ex;" alt="i+2000."/></p>
</li>
<li>
<p>LBA addresses always start with outermost cylinder and end with innermost one.
Zero LBA address is mapped to some sector in the outermost cylinder, while LBA address <img src="math-a5b94796d549d06b.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.0ex;" alt="N-1"/> is mapped
to some sector in the innermost cylinder.</p>
</li>
</ol>
</div>
<table class="tableblock frame-all grid-all" style="width: 1%;">
<caption class="title">Example of addressing for non-ZBR geometry (C,H,S) = (10, 2, 50)</caption>
<colgroup>
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7037%;">
<col style="width: 3.7038%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Linear&#8594;</th>
<th class="tableblock halign-right valign-top"></th>
<th class="tableblock halign-right valign-top">000</th>
<th class="tableblock halign-right valign-top">001</th>
<th class="tableblock halign-right valign-top">002</th>
<th class="tableblock halign-right valign-top"><span class="small">&#8230;&#8203;</span></th>
<th class="tableblock halign-right valign-top">049</th>
<th class="tableblock halign-right valign-top"></th>
<th class="tableblock halign-right valign-top">050</th>
<th class="tableblock halign-right valign-top">051</th>
<th class="tableblock halign-right valign-top">052</th>
<th class="tableblock halign-right valign-top"><span class="small">&#8230;&#8203;</span></th>
<th class="tableblock halign-right valign-top">099</th>
<th class="tableblock halign-right valign-top"></th>
<th class="tableblock halign-right valign-top">100</th>
<th class="tableblock halign-right valign-top">101</th>
<th class="tableblock halign-right valign-top">102</th>
<th class="tableblock halign-right valign-top"><span class="small">&#8230;&#8203;</span></th>
<th class="tableblock halign-right valign-top">149</th>
<th class="tableblock halign-right valign-top"></th>
<th class="tableblock halign-right valign-top"><span class="small">&#8230;&#8203;</span></th>
<th class="tableblock halign-right valign-top"></th>
<th class="tableblock halign-right valign-top">950</th>
<th class="tableblock halign-right valign-top">951</th>
<th class="tableblock halign-right valign-top">952</th>
<th class="tableblock halign-right valign-top"><span class="small">&#8230;&#8203;</span></th>
<th class="tableblock halign-right valign-top">999</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cylinder</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Head</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Sector</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">49</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">49</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">49</p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock"><span class="small">&#8230;&#8203;</span></p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">49</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Transition to ZBR created serious impact on HDD performance.
Because disk drive spins at fixed rate, head reads more sectors per second when positioned
at outward cylinder compared to when it is positioned at inward cylinder in the same amount of time.
Combined with rule #2 from above, this leads to important characteriscitc of HDDs:
working with low LBA addresses is faster than working with high addresses.</p>
</div>
<div class="paragraph">
<p>All modern HDDs use LBA atop of ZBR.
It is possible occasionly to see CHS values, for example in BIOS or in <code>hdparm</code> output.
But keep in mind that these values are not true values: this is emulation provided only for backward compatability.
It has nothing to do with actual internal geometry and provides access to only limited subset of HDD sectors.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_performance">Performance</h3>
<div class="paragraph">
<p>Once completed with basics of HDD operation, we can move on to measuring performance.
Each IO operation is a read or a write of a sequence of sectors specified by LBA address
of first sector.
Mechanical nature of HDD makes IO operation to be a multistep process:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Random access.
Obvious enough that in order to start accessing sectors, head must be positioned to the first
requested sector.
When request is received, actuator arm position relative to center of platter assembly is in
some random state in general case.
The same is true about instantaneous angular position of platter assembly.
So, HDD performs random access, which in turn consists of two steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Seek.
Controller sends command to move actuator arm to a position so that head is above track where
first requested sector is located.
Actuator arm is moved inwards or outwards depending on its current position.</p>
</li>
<li>
<p>Rotational delay.
Because platters are being constantly rotated, target sector will eventually pass under head.
No special action is required&#8201;&#8212;&#8201;only idle waiting.
This time may be as long as it takes to make one full revolution, but is only half revolution
on average.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Now HDD reads or writes all requested sectors one by one, while platter assembly continues
to be rotated.
Situation is possible when some of requested sectors are located in adjacent cylinder.
HDD has to reposition actuator arm once again (track-to-track seek) in this case.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Each of these steps takes considerable amount of total time and cannot be ignored in general.
Let&#8217;s see at how these steps affect performance of different access patterns.</p>
</div>
<div class="sect3">
<h4 id="_sequential_access">Sequential access</h4>
<div class="paragraph">
<p>Sequential access pattern is natural to HDD.
It arises when we want to read or write very large number of adjacent sectors starting with
given address: <img src="math-131afce2c2f1894b.svg" class="inlinemath" style="height:1.844ex;vertical-align:-0.391ex;" alt="A_{i},A_{i+1},...,A_{i+n-1}"/>.
<img src="math-1b16b1df538ba12d.svg" class="inlinemath" style="height:0.906ex;vertical-align:-0.016ex;" alt="n"/> here is order of hundreds and thousands.</p>
</div>
<div class="paragraph">
<p>Becase number of sectors to access is huge, positioning to the first sector (<img src="math-34b2eb62a09c0d69.svg" class="inlinemath" style="height:1.766ex;vertical-align:-0.313ex;" alt="A_{i}"/>) takes
small time compared to read/write itself.
So, we assume that head is already positioned to it.
Taking into consideration LBA to CHS mapping, now the only thing we need to do is to wait
some time while platter assembly is being rotated: all required sectors will be read/written
during this rotation.
Faster rotation speed and higher density of sectors per track make sequential access faster.</p>
</div>
<div class="paragraph">
<p>The only bad thing that may happen is when subsequent sector has different cylinder coordinate
(e.g. CHS transition (0,1,49)&#8594;(1,0,0)).
Actuator arm has to be repositioned in this case.
Repositioning is performed to adjacent cylinder (to the inside of the platter), and time required
to perform such move is called <strong>track-to-track seek time</strong>.
Typical value of track-to-track seek time is only couple of milliseconds and is often specified
in HDD&#8217;s datasheet.
Anyway, track-to-track seeks do not contribute very much to sequential access performance and may
be ignored.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-sequential-operations.svg" alt="hdd sequential operations" width="70%">
</div>
<div class="title">Order of sector access during sequential access</div>
</div>
<div class="paragraph">
<p>Good thing about sequential access is that almost all time goes direcly into reading or
writing sectors.
Thus, throughput efficiency is at its maximum.
This is the optimum mode of operation for HDD by design.
Manufacturers usually provide sequential access speed as <strong>sustained data transfer rate</strong>
in datasheets.
Typical value for modern 3.5'' 7200 RPM disk drive is 150..200 MB/sec.
But you must be aware of two common marketing tricks leading to this figure.
First one is well-known: rate is specified in decimal megabytes per second rather than
in binary ones.
Difference between them is about 5%: not so great, but and also is not too small to ignore if
you&#8217;re building high load file storage.
Second trick is subtle one.
This transfer rate is the <em>maximal</em> rate and was measured by manufacturer at lowest address:
remember that largest number of sectors is in the outermost cylinder because of ZBR.
Average rate is much smaller, you will need to subtract 20%-25% of advertised rate to get it.
Minimal transfer rate, which is achieved in innermost cylinder, is even worth that that&#8201;&#8212;&#8201;down 40%.</p>
</div>
<div class="paragraph">
<p>In general, sequential speed depends on following disk characteristics:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Rotational speed.
Disk drive with 7200 RPM is faster than 5400 RPM one.</p>
</li>
<li>
<p>Sector count per single track. This in turn depends on:</p>
<div class="ulist">
<ul>
<li>
<p>HDD model introduction date.
Newer generations of HDD have higher density of sectors, both longitudal
(more sectors per track) and radial (more tracks per platter surface).
Recently introduced HDD model will be faster than model introduced five years ago.</p>
</li>
<li>
<p>LBA address. Working with low addresses is faster than working with high addresses.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Track-to-track seek time.
HDD with smaller value is a bit faster.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_random_access">Random access</h4>
<div class="paragraph">
<p>While hard disk drives are good at sequential access, they really suck at random access
at the same time.
Random access arises in situations when we need to make a lot of small (couple of sectors)
reads or writes scattered all over the address space.</p>
</div>
<div class="paragraph">
<p>Time it takes to read/write sectors is neglible because number of sectors involved is small
by the definition of random access.
The true problem is that to read/write this small number of sectors, head must be properly
positioned to them.
This repositioning (called <strong>access time</strong>) takes all the time, and that&#8217;s why throughput (MB/s)
drops down greatly compared to sequential access.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s look at what happens during random access.
HDD carries out two operations in sequence:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>First, it moves actuator arm to target track.
Time required to perform such move is called <strong>seek time</strong>.</p>
</li>
<li>
<p>Next, it waits until platter is being rotated so that target sector is just beneath the head.
Corresponding metric is called <strong>rotational latency</strong>.</p>
</li>
</ul>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-random-operations.svg" alt="hdd random operations" width="55%">
</div>
<div class="title">Operations performed during random access</div>
</div>
<div class="paragraph">
<p>Seek time depends on number of tracks between current track and destination track.
The closer they are together&#8201;&#8212;&#8201;the faster is the seek.
For example, worst case is moving from the innermost track to the outermost track or vice versa.
Conversely, seek to adjacent track is relatively cheap.
In practice, seek time is not necesseraly linearly dependent on track count between source and
destination because HDDs move actuator arm with some acceleration.
In order to compare disk drives, three different characteristics are in use:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Track-to-track</strong> seek time (~0.2..2 ms).
This is time it takes to move from one track to its neighbour and presents the lowest bound for
all possible seek times.
This characteristic is not important for random access but plays some role in sequential access
as was explained in previous section.</p>
</li>
<li>
<p><strong>Full stroke</strong> seek time (~15..25 ms).
Time it takes to move from outermost track to innermost one or vice versa.
This is the worst possible seek time.</p>
</li>
<li>
<p><strong>Average seek time</strong> (~4..6 ms).
This time is computed as average time it takes to move actuator arm across 1/3 of all tracks.
This number (1/3) is derived from the fact that if we have two uniformly distributed indpenedent
random variables (source and destination tracks respectively), then average difference between
them is 1/3 of their range.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Rotational latency is less predictable but is strictly bounded.
After seek is done, platter angular position is random relatively to target sector, and HDD needs to wait while
platter is being rotated.
Best case is if target sector turned out to be just exactly under head after seek.
Worst case is if target sector has just passed away from head, and wait for full revolution is required until it will pass
again under the head.
It can be said that wait for half rotation is done on average.
Thus, rotational latency depends solely on disk rotational speed (RPM).
Table below lists well known rotational speeds along with translations of RPM values into how long it takes
to make single revolution and how many such revolutions are performed per second.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 40%;">
<colgroup>
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-right valign-top">RPM</th>
<th class="tableblock halign-center valign-top" colspan="2">Half revolution</th>
<th class="tableblock halign-center valign-top" colspan="2">Full revolution</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">&nbsp;</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">ms</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">IOPS</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">ms</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">IOPS</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">15,000</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2.0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">500</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4.0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">250</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">10,000</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">3.0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">333</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">6.0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">166</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">7,200</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4.2</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">240</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">8.3</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">120</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">5,900</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">196</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">10.2</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">98</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">5,400</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">5.6</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">180</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">11.1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">90</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">4,200</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">7.1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">140</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">14.3</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">70</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>For example, half revolution takes 4.2 ms for 7200 RPM disk drive.
If you are querying database for random records, you can&#8217;t get performance of more than 240 requests per second on average.
This value is limited by rotational speed even if records are packed close together (seek is cheap in this case).
If you&#8217;ve got better performance then this means that records come out mostly from page cache rather than from HDD,
and you will get into trouble when dataset becomes large enough so that it doesn&#8217;t fit in page cache anymore.</p>
</div>
<div class="paragraph">
<p>Seek time and rotational latency are rivals in the field of who contributes more to the access time.
Graph below demonstrates dependence between LBA address delta and range of possible access times.
HDD is assumed to be 7200 RPM and to execute full stroke seek in 15 ms.
Light green line represents lowest possible access time, which is the result of seek time only.
Mid-green and dark green lines are cases when additional wait was required for half and full revolutions respectively.
For example, if we send requests to 1 TB drive, previous request was at 100 GB position and new request is at 500 GB,
then delta between them is 40%.
Thus, according to the graph below, access time is bounded between 10..18 ms.
Repetition of this test will produce different values from 10 to 18 ms, but average access time will be 14 ms.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-access-time-model.svg" alt="hdd access time model" width="80%">
</div>
<div class="title">HDD access time model</div>
</div>
<div class="paragraph">
<p>It can be observed that if address delta is relatively small, then rotational latency is definitely the major contributor:
sometimes it is zero, but sometimes it is full rotation, while seek time is always negligible.
Conversely, if address delta is large, then seek time takes all the credit.
Finally, if address delta is not too big not too small, then there is no definite winner.</p>
</div>
<div class="paragraph">
<p>To realize slowness of random access, let&#8217;s compare it to CPU speed.
If we are given a 3 GHz CPU, then we may assume that it performs <img src="math-d51c51a00006a08e.svg" class="inlinemath" style="height:1.797ex;vertical-align:-0.031ex;" alt="{3}{\cdot}{10^9}"/> basic operations (like additions)
per second in sequence, and this is the worst case&#8201;&#8212;&#8201;without taking into account out-of-order execution and other
hardware optimizations.
While disk drive serves single random access read of 5 ms, CPU manages to complete
<img src="math-6bf1be81430134c7.svg" class="inlinemath" style="height:1.797ex;vertical-align:-0.031ex;" alt="{0.005}\cdot{{3}{\cdot}{10^9}}=15\,000\,000"/> operations.
HDD random access is more than seven orders of magnitude slower than CPU arithmetic unit.</p>
</div>
</div>
<div class="sect3">
<h4 id="_hybrid_access">Hybrid access</h4>
<div class="paragraph">
<p>Some applications have a concept of data chunk.
They read/write data in chunks of fixed size, which can be configured.
Choice of chunk size depends on what is more important: throughput or response time.
You can&#8217;t get both of them.
If throughput is more important than response time, then chunk size is chosen to be very large,
so that access pattern becomes sequential.
This is natural and most effective mode for HDDs.
If response time is more important than throughput, then chunk size is chosen to be very small&#8201;&#8212;&#8201;not more than couple of dozens of sectors.
Access pattern becomes almost random in this case.
As explained above, HDDs are very poor at performing random access: its speed (MB/s) may fall down
orders of magnitude compared to throughput-oriented chunk size.</p>
</div>
<div class="paragraph">
<p>Now imagine web application that streams video files to multiple clients simultaneously.
Of course, we would like to make chunk size as large as possible (tens and hundreds of megabytes).
Unfortunetely, such large chunk size has its own drawbacks.
First of all, we need in-memory buffer of corresponding size for each separate client,
which may be expensive.
Second, and more important, is that clients may be blocked for long periods of time: if, while
reading data for client <strong>A</strong>, new client <strong>B</strong> makes request, then <strong>B</strong> is stalled until read for <strong>A</strong>
is complete.
Conversely, using very small chunk size leads to perfect fairness and small memory footprint but
uses HDDs extremely ineffectively (pure random access).</p>
</div>
<div class="paragraph">
<p>So, what we need to do, is to find a balance between response time and effective throughput.
For example, we may want to choose such chunk size that values of both summands in general
formula are equal:</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="math-22ae76cbc6eaf163.svg" alt="\mathrm{ResponseTime\;[s]} = \mathrm{AccessTime\;[s]} + \frac{\mathrm{RequestSize\;[MB]}}{\mathrm{Throughput\;[MB/s]}}" width="50%">
</div>
</div>
<div class="paragraph">
<p>If HDD has average sustained speed of 100 MB/s and average access time is 5 ms, then chunk size is
chosen to be <img src="math-3e2f8b3944f14bcf.svg" class="inlinemath" style="height:2.016ex;vertical-align:-0.5ex;" alt="100\,000\,000\;\mathrm{B/s}\cdot 0.005\;\mathrm{s} = 500\;\mathrm{KB}"/>.
Single operation will take 10 ms on average with such chunk size, half of which is spent on
random access and another half is spent on read/write itself.
Throughput is also not so bad: <img src="math-791e86498333bdc0.svg" class="inlinemath" style="height:4.125ex;vertical-align:-1.406ex;" alt="500\;\mathrm{KB}\cdot{\frac{1}{0.010\;\mathrm{s}}} = 50\;\mathrm{MB/s}"/>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_comparison">Comparison</h4>
<div class="paragraph">
<p>In the image below, throughput efficiency of three different access patterns is compared.
Time flows left to right; each separate long bar reprerensts single request.
Blue color represents &#8220;useful&#8221; periods of time, when data is actually read or written.
Red color means periods of time wasted on seeks and rotational delays.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-throughput-efficiency.svg" alt="hdd throughput efficiency" width="90%">
</div>
<div class="title">Throughput efficiency of different access patterns</div>
</div>
<div class="paragraph">
<p>Sequential access demonstrates nearly perfect throughput, above 90%.
Almost all time is spent in reading or writing sectors excluding initial random access and rare
track-to-track seeks.
Random access is the opposite: all time is burnt in disk random accesses, and only very small
fraction of time is spent in working directly with data.
Throughput efficiency is just above zero.
Hybrid access displays 50% throughput efficiency by design: a balance between throughput and IOPS.</p>
</div>
<div class="paragraph">
<p>As a rule of thumb, use HDD for mostly sequential access and do not use it for random access.
The only one exception is when response time of tens of milliseconds is acceptable and you are
sure that system would never be required to provide more than one hundred IOPS.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_experiments">Experiments</h3>
<div class="sect3">
<h4 id="_effect_of_lba_address_on_sequential_access_performance">Effect of LBA address on sequential access performance</h4>
<div class="paragraph">
<p>This test demonstrates effect of LBA address on sequential performance.
Test was performed by reading 128 MiB chunks of data at different addresses: first probe was made at address 0,
next&#8201;&#8212;&#8201;at 1% of disk capacity, and so on with last probe made at 100% of capacity.
Horizontal axis is the address value in gigabytes, vertical axis is the achieved speed.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-sequential-read-speed.svg" alt="hdd sequential read speed" width="90%">
</div>
<div class="title">Effect of LBA address on sequential access performance</div>
</div>
<div class="paragraph">
<p>You can clearly see that speed drops down more than twice at highest address (which is mapped to innermost HDD track).
Sometimes it may be even worth to repartition HDD with regard to above effect:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If you are going to work with data which will be often accessed sequentially, then create dedicated partition for it in
the very <em>beginning</em> of HDD.
Examples of such data include: web server access logs, database files and streaming video.</p>
</li>
<li>
<p>If you have data which will be accessed rarely (access pattern doesn&#8217;t matter), then create dedicated partition
for it in the very <em>end</em> of HDD so that this data won&#8217;t use valuable low addresses.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_random_access_response_time_distribution">Random access response time distribution</h4>
<div class="paragraph">
<p>Next graph is dedicated to random access timings.
Each point corresponds to one of 2000 random reads performed, each read is one sector long
(512 bytes) and is properly aligned.
Horizontal axis is LBA address delta between source and destination positions, vertical axis
is operation response time.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-random-read-time.svg" alt="hdd random read time" width="90%">
</div>
<div class="title">Random access response time distribution</div>
</div>
<div class="paragraph">
<p>Points lying at the lower bound of the cloud are the best cases.
They correspond to situations when after actuator arm has been positioned at proper track,
required sector turned out to be just beneath the head.
That is, no additional wait for platter rotation is required.
Note that seek time becomes longer with larger LBA address delta.
Seek time corresponding to max delta is fullstroke seek time (21 ms).</p>
</div>
<div class="paragraph">
<p>Conversely, upper bound points correspond to the worst possible situation.
After arm has been positioned to target track, required sector has just moved past head,
and drive has to wait for one full platter rotation until required sector passes under head
once again.
According to above graph, rotational delay is spread in range [0, 11] milliseconds and is
independent of deltas.
Knowledge of full rotation time (11 ms) immediately leads us to the conclusion that this is
5400 RPM HDD that was tested.</p>
</div>
<div class="paragraph">
<p>Important to remember that short delta random accesses are limited in performance by rotational
speed while large delta random acceses are limited by seek time.</p>
</div>
</div>
<div class="sect3">
<h4 id="_chunk_size">Chunk size</h4>
<div class="paragraph">
<p>This tests demonstrates influence of chunk size on two types of throughput: MB/s and IOPS.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hdd-chunk-size.svg" alt="hdd chunk size" width="100%">
</div>
<div class="title">Influence of HDD chunk size on performance</div>
</div>
<div class="paragraph">
<p>If HDD was an ideal device, then access time would be zero and throughput (MB/s) would be constant
(dashed line) disregarding chunk size.
Unfortunately, access time is well above zero due to mechanical nature of HDD, which results
in throughput being much worse on average.
Effective throughput converges to optimum only when chunk size is very large, starting with 8 MiB
for specimen used in this test.
HDD is working in purely sequential access mode and achieves it top performance (MB/s)
in this region.
Almost all time spent inside HDD goes directly into reading or writing sectors.</p>
</div>
<div class="paragraph">
<p>Left part of graph displays the opposite.
Chunk size is very small, and HDD is working in purely random access mode.
As such, HDD manages to complete a lot of independent operations per second, but cumulative write speed (MB/s) falls down
dramatically compared to purely sequential access.
For example, we get nearly top IOPS with 16 KiB chunks but write speed is only ~2 MB/s, which is 1/15 of top possible speed.
Time spent on writing sectors is negligible in this case&#8201;&#8212;&#8201;most of time is actually &#8220;burnt&#8221; on seeks and rotational delays.
It is easy to encounter such situation in real world during copying a directory with a lot of small files in it.
When this is happenning, you may observe your HDD working like mad judging by blinking LEDs, you may hear hellish sounds
from inside (seeks), but copy speed is just above zero MB/s.</p>
</div>
<div class="paragraph">
<p>Point of intersection corresonds to kind of golden mean, where both MB/s and IOPS are neither too bad nor too good.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_solid_state_drive_ssd">Solid-state drive (SSD)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>SSDs are slowly but steady taking the place of HDDs as a mainstream storage device.
Free of mechanical parts, SSD clears the gap between sequential and random types of access: both are served equally fast.
Unfortunately, cost per gigabyte is still about eight times higher for SSD compared to HDD even after more than decade of
intensive development.
This makes SSDs and HDDs to organically coexist in data storage device market, with each one being ideally suitable for
specific set of applications.</p>
</div>
<div class="sect2">
<h3 id="_theory_of_operation_2">Theory of operation</h3>
<div class="sect3">
<h4 id="_floating_gate_transistor">Floating gate transistor</h4>
<div class="paragraph">
<p>Solid state drives, USB sticks and all common types of memory cards are all based on <strong>flash memory</strong>.
Basic unit of flash memory is a floating gate IGFET transistor.
Idea behind it is to add additional layer, which is called &#8220;floating gate&#8221; (FG), to otherwise ordinal IGFET transistor.
Floating gate is surrounded by insulating layers from both sides.
Such structure allows to trap excessive electrons inside FG.
Under normal circumstances, neither they can escape FG nor other electrons can penetrate insulator layer and get into FG.
Hence the ability of transistor to store information: it is said that transistor stores binary &#8220;0&#8221; or &#8220;1&#8221; depending
on whether its FG is charged with additional electrons or not.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-fgigfet.svg" alt="ssd fgigfet" width="90%">
</div>
<div class="title">FG IGFET transistor (left) and source-to-drain conductiveness table (right)</div>
</div>
<div class="paragraph">
<p>Reading transistor state is performed by checking whether source-to-drain path conducts electricity or not.
If gate is unconnected to power source, then this path is definitely unconductive, disregarding whether FG is charged or not:
one of p-n junctions is in reverse and won&#8217;t allow current to flow through it.</p>
</div>
<div class="paragraph">
<p>Things become interesting when gate is connected to power source (<img src="math-7dcb574f43317093.svg" class="inlinemath" style="height:1.703ex;vertical-align:-0.313ex;" alt="V_{\mathrm{read}}"/>).
If FG is uncharged, then control gate&#8217;s electrical field attracts some of electrons to the very top of p-substrate like a magnet.
These electrons can&#8217;t move up because they do not have enough energy to pass insulator layer, so they form narrow region
which is conductive and exists until power is removed from gate.
Current is free to flow along this region in both directions by means of these electrons, thus shortening source-to-drain path.
But if FG is charged, then FG charge partially shields p-substrate from gate`s electrical field.
Remaining electrical field is not enough to attract substantial number of electrons, and source-to-drain path remains unconductive.</p>
</div>
<div class="paragraph">
<p>It is noteworthy to mention that if gate voltage is raised to even higher
level (<img src="math-36fb54c002d0748c.svg" class="inlinemath" style="height:1.984ex;vertical-align:-0.594ex;" alt="V_{\mathrm{pass}}"/>), then FG won&#8217;t be able to provide enough
shielding anymore, and source-to-drain path will become conductive unconditionally.
Such operation is of no use for standalone transistor, but is essential when
transistors are arranged into series, as would be explained later.</p>
</div>
<div class="paragraph">
<p>To change the charge of FG, someone needs to apply high voltage (tens of volts) between gate and other terminals.
It forces electrons to pass lower insulator layer.
Depending on direction of voltage applied, either electrons will move out of FG to the p-substrate, or
vice versa, move from p-substrate into the FG.
Top insulator layer is made of material which is inpenetrable even at high voltages.
Adding electrons to FG is called &#8220;programming&#8221; in terms of flash memory and removing electrons from FG is called &#8220;erasing&#8221;.
Unfortunately, each program or erase operation worsens quality of insulator layer.
Some of electrons get trapped in insulator layer rather than in FG, and there are no simple controlled means of removing
them out of there.
Electrons trapped in insulator layer create residual charge and also facilitate leakage of electrons to/from FG.
After number of program/erase cycles, insulator layer &#8220;wears off&#8221; and transistor is unsuitable for data storage anymore.
Manufacturers specify endurance of flash memory as guaranteed number of program/erase cycles before failure (<strong>P/E cycles</strong>).</p>
</div>
<div class="paragraph">
<p>Modern flash memory is able to store more than single bit per transistor (which are called <strong>cells</strong> in terms of flash memory).
This is achieved by distinguising between more than two levels of charge, for example, by testing source-to-drain conductivity
with different gate voltages.
Currently, following types are produced:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>SLC, &#8220;single-level cell&#8221;&#8201;&#8212;&#8201;senses 2 levels of charge, stores 1 bit</p>
</li>
<li>
<p>MLC, &#8220;multi-level cell&#8221;&#8201;&#8212;&#8201; senses 4 levels of charge, stores 2 bits.
Technically, &#8220;multi-&#8221; may mean any number of levels larger than two, but in practice it almost always refers to four levels.</p>
</li>
<li>
<p>TLC, &#8220;triple-level cell&#8221;&#8201;&#8212;&#8201;senses 8 levels of charge, stores 3 bits</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Storing more bits per cell has obvious benefit of having higher device capacity.
Tradeoff is that when more bits per cell are stored, things become complicated.
It is easy to implement read/program operations with only two levels of charge: every charge that is higher than predefined
threshold value is treated as binary 1 and every charge below this threshold value is binary 0.
Ability to work with three levels or more requires intermediate charges to fit exactly into specified windows.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-charge-levels.svg" alt="ssd charge levels" width="40%">
</div>
<div class="title">Charge levels of SLC vs TLC</div>
</div>
<div class="paragraph">
<p>This makes all operations slower.
For example, now programming has to be done in small steps in order not to miss desired window:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add some small charge to FG</p>
</li>
<li>
<p>Verify its actual level</p>
</li>
<li>
<p>Go to first step if it has not achieved required level yet</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Storing more levels per transistor also severely reduces number of P/E cycles:
even slowest leakage of electrons through insulator layer would move charge out of its window and be sensed as
incorrect value.
Typical SLC endures up to 100,000 P/E cycles, MLC&#8201;&#8212;&#8201;3,000, and TLC is even worse than that&#8201;&#8212;&#8201;only 1,000.
To alleviate this problem, flash memory contains extra cells to store ECC, but this helps only to some extent.</p>
</div>
</div>
<div class="sect3">
<h4 id="_block">Block</h4>
<div class="paragraph">
<p>Cells are organized into two-dimensional arrays called <strong>blocks</strong>.
All cells share p-substrate, that&#8217;s why only three leads per cell are shown.
Each row of cells forms single <strong>page</strong>, which has somewhat similar role as sector in HDD:
it is a basic unit of reading and programming in most cases.
Typical page consists of 4K-32K cells plus some cells to store per-page ECC (not shown).
Typical block has 64-128 such pages.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-block.svg" alt="ssd block" width="90%">
</div>
<div class="title">Flash memory block (128×16384×2)</div>
</div>
<div class="paragraph">
<p>Block supports the same three operations as standalone cell: read, program and erase.
But now, because cells are coupled into series, it becomes even trickier to perform them.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Read</dt>
<dd>
<p>Both read and program operations act on single pages.
Selection of particular page is done by connecting word lines of all <em>other</em> pages to <img src="math-36fb54c002d0748c.svg" class="inlinemath" style="height:1.984ex;vertical-align:-0.594ex;" alt="V_{\mathrm{pass}}"/>.
This opens all cells in them no matter whether their floating gates are charged or not.
As such, circuitry turns into as if only single page was present.</p>
<div class="paragraph">
<p>Next step depends on operation.
If read is done, then selected page&#8217;s word line is connected to <img src="math-7dcb574f43317093.svg" class="inlinemath" style="height:1.703ex;vertical-align:-0.313ex;" alt="V_{\mathrm{read}}"/>
(<img src="math-1835c9f70d67da67.svg" class="inlinemath" style="height:1.984ex;vertical-align:-0.594ex;" alt="V_{\mathrm{read}} \less V_{\mathrm{pass}}"/>) and source line is grounded.
Now data for reading is sensed via bit lines: path between source line and each of bit lines is conductive if and only if
corresponding cell in selected page doesn&#8217;t hold charge.
If cell is MLC/TLC, than either different levels of current are sensed, or reading is performed in number of steps with
different <img src="math-7dcb574f43317093.svg" class="inlinemath" style="height:1.703ex;vertical-align:-0.313ex;" alt="V_{\mathrm{read}}"/> values.
It takes about 25 μs to read single page in SLC, and couple of times longer in MLC/TLC.</p>
</div>
<div class="paragraph">
<p>If we put aside floating gates for a moment, then it can be observed that such configuration resembles multi-input NAND gate:
<img src="math-115dd3e365dad987.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="\mathrm{NAND}(x_1,x_2,...,x_n)=\mathrm{NOT}(\mathrm{AND}(x_1,x_2,...,x_n))"/>.
Word lines act as inputs and each separate bit line is an output.
Bit line is tied to ground ("0") if and only if all gates are "1".
Similarity to NAND gate gave name to such type of memory organization&#8201;&#8212;&#8201;<strong>NAND flash</strong>.
Another type of flash memory, which won&#8217;t be described here, is NOR flash: it is faster for reading and provides random access
to each separate cell.
But because its higher cost, it is mainly used to store and execute firmware code.</p>
</div>
</dd>
<dt class="hdlist1">Program</dt>
<dd>
<p>Programming is also performed by first selecting page by applying <img src="math-36fb54c002d0748c.svg" class="inlinemath" style="height:1.984ex;vertical-align:-0.594ex;" alt="V_{\mathrm{pass}}"/> to all other pages.
Next, bit lines corresponding to bits in page we want to program are grounded, and word line of selected page is pulled
to <img src="math-a179365f73080b31.svg" class="inlinemath" style="height:1.984ex;vertical-align:-0.594ex;" alt="V_{\mathrm{prg}}"/> (<img src="math-24bef44d8633686d.svg" class="inlinemath" style="height:1.984ex;vertical-align:-0.594ex;" alt="V_{\mathrm{prg}} \gtr V_{\mathrm{pass}} \gtr V_{\mathrm{read}}"/>).
<img src="math-a179365f73080b31.svg" class="inlinemath" style="height:1.984ex;vertical-align:-0.594ex;" alt="V_{\mathrm{prg}}"/> voltage is high enough to force electrons to overcome insulator layer and move into floating gates.
Floating gates in other cells are nearly unaffected because voltage difference is only <img src="math-36fb54c002d0748c.svg" class="inlinemath" style="height:1.984ex;vertical-align:-0.594ex;" alt="V_{\mathrm{pass}}"/>, which is too low.
Program operation is much slower than read operation: about 200 μs for SLC and, as with read, couple of times longer for MLC/TLC.
But anyway, it is order of magnitude faster than HDD&#8217;s access time.</p>
<div class="paragraph">
<p>It is reasonable to ask: why unit of programming is whole page and not single cell?
Problem is that because program operation uses high voltage, charges of nearby cells are disturbed.
If multiple program operations were allowed to be issued to single page (to different cells), than this would potentially toggle
a glitch in cells we didn&#8217;t want to be programmed.
Some SLC flash allows partial programming, that is, it is possible to first program cells 0, 5 and then 2, 3, 6 in a single page.
But even such, number of partial programmings beore erasure is strictly limited.
The only reasonable case for using partial programming is to split large physical page into number of smaller logical pages,
thus allowing single program operation to be issued once per logical page.</p>
</div>
</dd>
<dt class="hdlist1">Erase</dt>
<dd>
<p>The key difference of erase is that it is a <strong>whole block</strong> operation: all cells of one or more blocks are erased simultaneously.
Such limitation simplifies memory production and also removes the necessity to care about disturbing charge of nearby cells.
Inability to erase single pages presents second major drawback of flash memory (wearing is being the first one).
Erase is performed very rough: all word lines are grounded and high voltage source is connected to body (substrate),
which is shared by all cells.
This forces electrons to move out from FG into p-substrate, thus zeroing charge of all floating gates.</p>
<div class="paragraph">
<p>Erase is even slower than programming: 1500 μs for SLC and, once again, couple of times longer for MLC/TLC.
But in practice it is not a big problem because, under normal circumstances, erases are performed in background.
If, for some reason, erase of only subset of pages needs to be done, then we have to perform read-modify-write sequence:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>first, all pages, which must be preserved, are read into temporary buffer one by one</p>
</li>
<li>
<p>then block is erased as a whole</p>
</li>
<li>
<p>then buffered pages are programmed back one by one</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Not only this is extremely slow, but also wears cells prematurely.
In order to avoid such situations, SSD employ smart algorithms described in next section.</p>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>As a summary, here is lifecycle of flash block consisting of four pages each with 8 SLC cells.
Note that once page is programmed, it remains in such state until erase is performed, which acts on whole block.
There is no overwrite operation.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-block-lifecycle.svg" alt="ssd block lifecycle" width="90%">
</div>
<div class="title">Block lifecycle</div>
</div>
<div class="paragraph">
<p>And here are typical performance figures of NAND flash.
Most of produced server grade flash memory is of MLC type.
SSDs intended to be used in workstations may be MLC or TLC.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 60%;">
<colgroup>
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">&nbsp;</th>
<th class="tableblock halign-left valign-top">Read page</th>
<th class="tableblock halign-left valign-top">Program page</th>
<th class="tableblock halign-left valign-top">Erase block</th>
<th class="tableblock halign-left valign-top">P/E cycles</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SLC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">25 μs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">200 μs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1500 μs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">100,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">MLC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">50 μs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">600 μs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">3000 μs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">3,000 - 10,000</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TLC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">75 μs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">900 μs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">4500 μs</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,000 - 5,000</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="_controller">Controller</h4>
<div class="paragraph">
<p>Like HDDs, SSDs carry controller, which acts as a bridge between memory chips and outer interface.
But flash memory specifics&#8201;&#8212;&#8201;erasures at block level and wearing&#8201;&#8212;&#8201;make controller to implement
complex algorithms to overcome these limitations.
As a result, performance and life span of SSD depend on controller to such great extent that
controller model is typically specified in SSD datasheet.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s start with the following situation.
Suppose that your have some sector that is updated very often.
For example, it backs database row that stores current ZWR/USD exchange rate.
If sectors had one-to-one mapping to pages of flash memory as in HDD, that would require
read-modify-write sequence at block level with each edit.
That would wear out frequently written pages very soon (only after couple of thousands of writes),
while long tale of rarely used pages would remain in nearly virgin state.
Such approach is also undesirable because performance of read-modify-write is very poor&#8201;&#8212;&#8201;about that of HDD&#8217;s access time.</p>
</div>
<div class="paragraph">
<p>To solve these problems, logical sectors do not have fixed mapping to flash memory pages.
Instead, there is additional level of addressing called <strong>flash translation layer</strong> (FTL),
maintained by controller.
Controller stores table which maps logical sectors to (chip, block, page) tuples,
and this table is updated during SSD lifetime.
When <code>write()</code> request is served, instead of performing read-modify-write sequence,
controller searches for free page, writes data into it and updates table entry for written sector.
If you make 1000 writes to the same sector, then 1000 different pages will be occupied by data,
with table entry pointing to the page with latest version.</p>
</div>
<div class="paragraph">
<p>Example of FTL mapping is displayed below.
There are 16 logical sectors: some of them are valid and are mapped to physical pages,
while others are not currently valid.
Pages not referenced by FTL are either free pages or contain old versions of sectors.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-ftl-mapping.svg" alt="ssd ftl mapping" width="75%">
</div>
<div class="title">FTL mapping</div>
</div>
<div class="paragraph">
<p>Existance of FTL creates in turn another problem&#8201;&#8212;&#8201;dealing with pages which store deprecated versions.
To reclaim them, controller performs <strong>garbage collecting</strong> (GC).
Normally, it is run in background, when there are no pending requests from OS to serve.
If some block is full and all its pages contain deprecated versions, block may be safely erased and all its
pages may be added to the pool of free pages for further use.
It also makes sense to erase block if it contains small fraction of live pages&#8201;&#8212;&#8201;by copying them
into free pages and updating FTL table accordingly.</p>
</div>
<div class="paragraph">
<p>Obviously enough, FTL is effective only if there are plenty of free pages.
Without having free pages, attempt to write to some sector will cause blocking
read-modify-write sequence, which is extremely bad from performance and longevity points of view.
But the problem is that SSD is <em>always</em> full by default.
All those &#8220;free space left&#8221; metrics are part of filesystems and are of no knowledge to SSD.
From SSD persepective, once data is written to some sector, this sector is &#8220;live&#8221; forever,
even if it doesn&#8217;t hold valid data anymore from filesystem&#8217;s point of view.
Two special features are implemented in modern SSDs to resolve this problem.</p>
</div>
<div class="paragraph">
<p>First feature is called <strong>provisioning area</strong>.
SSDs come with a bit more capacity than actually is specified in datasheets (typically +5..10%).
Provisioning area size is not counted into total capacity of SSD.
Main purpose of adding provisioning area is to be sure that SSD will never be 100% full internally,
thus nearly eliminating read-modify-write.
The only one exception is when rate of writes is so high that background GC is not able to keep pace with it.
Second purpose of provisioning area is to ensure that spare blocks exist as a replacement to be used for weared off blocks.</p>
</div>
<div class="paragraph">
<p>Second feature in use is a <strong>TRIM</strong> command (word is not acronym but an ATA command name), also known as <strong>erase</strong> and <strong>discard</strong>.
If filesystem supports it, then it may send TRIM along with range of sectors to notify SSD that data in
these sectors is not required anymore.
On receiving such request, controller will remove entries from FTL table and mark pages corresponding to specified sectors
as ready for garbage collecting.
After that is done, reading trimmed sectors without writing to them first will return unpredictable data
(for security reasons, controller will usually return zeroes without accessing any pages at all).</p>
</div>
<div class="paragraph">
<p>Image below demonstrates tecnhniques described above:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Initally, all pages of block <code>#0</code> are in erased state. No data is stored in sectors <code>0x111111</code>, <code>0x222222</code>, <code>0x333333</code>&#8201;&#8212;&#8201;there are null pointers in FTL table for them.</p>
</li>
<li>
<p>Next, some data was written into sectors <code>0x111111</code> and <code>0x222222</code>.
Controller found that pages in block <code>#0</code> are free and used them to store this data.
After write was completed, it also updated FTL table to point to these pages.</p>
</li>
<li>
<p>Next, sector <code>0x333333</code> was written to and also sectors <code>0x111111</code> and <code>0x222222</code> were overwritten.
Controller wrote new versions of sectors in free pages and updated table.
Pages containing old versions were marked as ready for GC.</p>
</li>
<li>
<p>Next, filesystem decided that sector <code>0x222222</code> is not required anymore and sent TRIM command.
Controller removed entry for this sector and marked page as ready for GC.</p>
</li>
<li>
<p>Finally, background GC was performed.
Valid versions were moved to empty block <code>#1</code>, table was updated appropriately and block <code>#0</code> was erased.</p>
</li>
</ol>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-ftl-example.svg" alt="ssd ftl example" width="100%">
</div>
<div class="title">FTL in action</div>
</div>
<div class="paragraph">
<p>Besides garbage collecting, controller is also responsible for <strong>wear leveling</strong>.
Its ultimate goal is to distribute erases among all blocks as evenly as possible.
Controller stores set of metrics for each block: number of times each block was erased and last erase timestamp.
Not only are these metrics used to select where to direct next write request to, but controller also performs
background wear leveling.
Idea behind it is that if some block is close to its P/E limit, then writes to this block should be avoided, but there
is no problem to use it for reading.
So, controller searches for pages with &#8220;stable&#8221; data in them: pages which were last written long time ago and
have not been modified since that time (e.g. pages which back OS installation or multimedia collection).
Next, pages of highly weared block and &#8220;stable&#8221; pages are exchanged.
This requires erase on nearly dead block one more time, but is beneficial in the long run.
It reduces chances that highly weared block will be overwritten soon, hence prolonging its life.</p>
</div>
</div>
<div class="sect3">
<h4 id="_ssd_assembly">SSD assembly</h4>
<div class="paragraph">
<p>Thousands of flash blocks are produced together on a single die in the form of <strong>flash chip</strong>,
and multiple such chips are the base of SSD.
Other important components are the controller chip and RAM chip.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-assembly.svg" alt="ssd assembly" width="70%">
</div>
<div class="title">SSD assembly</div>
</div>
<div class="paragraph">
<p>One important question to answer is where FTL is stored.
FTL and related data structures (list of free pages, wear counters) must be preserved
when SSD is offline, so they have to occupy some region of flash memory.
But it would be unfeasable to access this region directly during SSD operation because
each program or erase request would require additional write to this region to update FTL,
which is bad for performance and causes premature wearing.
That&#8217;s why most of SSDs cache FTL in RAM.
When SSD is powered on, FTL and related data structures are loaded from flash memory into RAM,
where they are read from and updated to during SSD operation.
Periodically cached state is synchronized to persistent flash memory; this also happens when
host sends flush or shutdown command.
As with write buffering, this approach solves performance problem but creates problem of
inconsistency: if SSD is powered down without proper shutdown command, cached FTL state
won&#8217;t be persisted.
To resolve this issue, enterprise grade SSDs may carry a couple of capacitors with enough energy
to flush FTL (and data) from RAM buffer into flash memory in case of sudden power loss.
SSDs produced for mass market usually lack this feature.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_performance_2">Performance</h3>
<div class="paragraph">
<p>What makes SSDs different from HDDs is extremely high concurrency.
It benefits to both sequential access performance and, more important, to random access performance.</p>
</div>
<div class="paragraph">
<p>As we remember, HDD is able to serve only single request at a time simply because it has only one actuator arm.
But SSDs have multiple chips, and each chip is also split internally into number of logical units (LUNs).
Each LUN is an independent entity and may serve one request at a time.
The more chips and LUNs SSD has, the more concurrent random access requests it may serve as a whole.</p>
</div>
<div class="paragraph">
<p>The same reasoning applies for sequential access, too.
If only single chip was used for sequential access, than performance would be rather poor.
For example, if sector size is 4096 bytes and flash memory is of MLC type (50 μs read time and 500 μs write time), then
resulting speed is limited by 81.92 MB/s and 8.192 MB/s respectively.
But because controller interleaves sequential sectors among multiple chips/LUNs, speed is multiplied by concurrency factor.</p>
</div>
<div class="paragraph">
<p>By varying number of chips/LUNs it is possible to produce SSDs with performance characteristics in wide range of values:
from only 100 MB/s / 1000 IOPS for workstation grade SSDs to gigabytes per second / 1 million IOPS for high-end enterprise SSDs.</p>
</div>
<div class="paragraph">
<p>Following sections provide examples.</p>
</div>
</div>
<div class="sect2">
<h3 id="_experiments_2">Experiments</h3>
<div class="paragraph">
<p>First thing you want to know is that testing SSDs is much harder than HDDs.
Additional layer of indirection&#8201;&#8212;&#8201;FTL&#8201;&#8212;&#8201;and all the algorithms used to maintain it create
significant differences between SSDs.
Following rules must be followed in order to get robust test results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>SSDs shipped from factory have all blocks erased.
Trying to test such drive may result in unusually high or low performance.
To make test results valid, you need to explicitly fill SSD with data before running tests.
Either fill it all, or, ideally, make sure that proportion between occupied and erased blocks
equals to that of production environment.
And do not use <code>/dev/zero</code> as data input source because some home grade SSDs compress data on the fly.</p>
</li>
<li>
<p>SSDs run some operations in background (erasures and static wear leveling), and both of them
influence test results.
Repeat tests multiple times and be ready that identical test runs will demonstrate considerable
differences in performance.</p>
</li>
<li>
<p>Thinly provisioned SSDs may demonstrate good performance when test only starts but with
dramatical degradation further, when there are no free blocks left.
Run tests for prolonged amount of time.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In general, bear in mind that synthetic benchmarks is poor choice for testing SSDs (in contrast to HDDs).
Tests below try to demonstrate orders of magnitude and general performance patterns universal to all SSDs.</p>
</div>
<div class="sect3">
<h4 id="_single_threaded_random_access">Single threaded random access</h4>
<div class="paragraph">
<p>First test demonstrates how good SSDs are at random access.
Test was performed by making 1 million single sector requests scattered uniformly along address space.
Horizontal axis is test completion in percents, vertical&#8201;&#8212;&#8201;number of IOPS achieved.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-random-operations.svg" alt="ssd random operations" width="90%">
</div>
<div class="title">SSD single threaded random access performance</div>
</div>
<div class="paragraph">
<p>Let&#8217;s start analysis with looking at random reads line.
As expected, it is very stable along the course of test, nearly a straight line.
Converting 7000 IOPS to its inverse results in 143 μs average response time, which is couple
of times larger compared to anticipated flash memory read time (25-75 μs), but is satisfactory
if transfer and in-kernel delays are taken into account.
7000 IOPS is already 35x (!) improvement against HDD random access, which is a good indicator by itself
to switch to SSDs in applications where random access is the primary method of accessing storage.</p>
</div>
<div class="paragraph">
<p>Bottom line represents <em>synchronous</em> writes.
Term &#8220;synchronous&#8221; means that each IO syscall returns control to program ony after data has been actually
written to physical media.
Such mode disallows write buffering in all nodes on the path from application to storage media.
Neither OS nor drive are allowed to respond back before write actually takes place.
As such, this mode is perfect for performing unbiased comparison of reads and writes as they happen in "raw"
flash memory, without any higher level optimizations.
It is clearly seen from chart above that synchronous writes have two drawbacks.
First one is that they are much slower than reads.
This is expected because flash memory theoretical write time is about 10 times slower than reads.
Anyway, average value of ~900 IOPS is still much better compared to HDD, though this difference
is not so dramatical as for random reads.
Second drawback comes from the fact that write response time is not stable during test progression
but is widely dispersed between 400 and 1300 IOPS.
This the result of SSD performing online block erasures time to time.</p>
</div>
<div class="paragraph">
<p>In practice, SSDs demonstrate a bit different write performance and roughness patterns depending on a variety of factors:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>controller model</p>
</li>
<li>
<p>flash memory technology: SLC, MLC or TLC</p>
</li>
<li>
<p>availability of free pages (which in turn depends on provisioning area size and rate of writes)<br>
Giving some spare time between writes allows SSD to erase blocks in background, hence reducing &#8220;roughness&#8221;.</p>
</li>
<li>
<p>age of device</p>
</li>
<li>
<p>whether the same data is written or not<br>
Some of devices compare new and previous versions of data and, if they are equal, ignore write
in order not to issue unnecessary P/E cycle.
This results in &#8220;write&#8221; performance being equal to read performance.
Do not forget to randomize data when running synthetic benchmarks.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>What makes above graph weird at first glance is ultra-fast normal (non-synchronous) writes.
Non-synchronous write means that both OS and drive are allowed to respond with completion of request immediately,
even if data is not written to physical media yet (request is queued or is in progress).
With requirement for immediate consistency dropped, SSDs are able to achieve tens and hundreds times higher write IOPS.
Such extraordinary leap in performance is the result of high internal concurrency.
SSDs are equipped with multiple (4-16) flash chips and each chip consists of multiple (2-8) independently working
logical units (LUNs), thus bringing overall concurrency factor up to 128.
When new write request arrives, controller issues it to currently non-busy chip/LUN, while previously issued writes
are still running in other chips/LUNs.
This was not possible with single-threaded <em>synchronous</em> writes, because only one chip/LUN worked at any given time,
thus nullifying concurrency concept.
That&#8217;s why we see such dramatical difference in synchronous vs non-synchronous writes.</p>
</div>
<div class="paragraph">
<p>Note that HDDs also make use of non-synchronous mode.
They reorder write requests in queue with purpose to reduce rotational latency and seek time.
But in practice, these optimizations are too small to be noticable, rarely exceeding 20%.</p>
</div>
</div>
<div class="sect3">
<h4 id="_concurrent_random_access">Concurrent random access</h4>
<div class="paragraph">
<p>By knowing that non-synchronous writes are fast due to concurrency leads to question: can we use
concurrency for reads?
The answer is yes, if we issue multiple read requests at a time.
This next test demonstrates how IOPS depend on number of concurrently issued requests
(something that is also referred to as &#8220;queue depth&#8221;).
As in previous test, each request is one physical sector in length, properly aligned.
But now horizontal axis is the number of parallel requests made at a time, 1 to 40.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-concurrency.svg" alt="ssd concurrency" width="90%">
</div>
<div class="title">SSD concurrent random access performance</div>
</div>
<div class="paragraph">
<p>You may see that read IOPS grow steadily with increase in number of parallel requests and finally
saturate at 76,000 IOPS.
This gives ten-fold speedup compared to only one read made at a time.
As with writes, such outstanding performance is possible because of SSD concurrency: random read
requests are distrubuted nearly uniformly among chips and LUNs.
So, not only flash memory is faster than HDD by itself, but SSDs can also serve requests in truly
parallel fashion.
Combined, these two properties result in 380x (!!) better random read performance compared to HDD.</p>
</div>
<div class="paragraph">
<p>Conversely, write performance doesn&#8217;t depend on number of concurrent requests at all.
Without having to block responses until data is written onto medium, internal parallelism
already becomes saturated with only few parallel requests.
Requests are delivered faster to drive&#8217;s buffer than it is able to actually serve them,
thus keeping buffer always full.
As soon as some chip/LUN becomes free, it takes and starts working on next write request
from buffer.</p>
</div>
</div>
<div class="sect3">
<h4 id="_sequential_access_2">Sequential access</h4>
<div class="paragraph">
<p>Compared to previous graphs, sequential access performance doesn&#8217;t have any pecularities.
Percentage values along horizontal axis denote request offsets relative to drive&#8217;s capacity.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-sequential-operations.svg" alt="ssd sequential operations" width="90%">
</div>
<div class="title">SSD sequential access performance</div>
</div>
<div class="paragraph">
<p>Once again, sequential performance is so good because of concurrency.
Single chip is slow, but when reads and writes are interleaved among chips/LUNs,
throughput skyrockets to hundreds of megabytes per second.
Sequential access speed is usually limited by one of interconnecting interfaces, not by concurrency
level or flash memory read/write latency.
This limiting interface is either flash chip interface (defines how fast SSD controller is able to
stream data to/from single chip) or SSD outer interface (SATA/SAS/PCIe).</p>
</div>
<div class="paragraph">
<p>Also note one more good thing about SSDs: you don&#8217;t need to care about proper disk partitioning&#8201;&#8212;&#8201;performance is uniform along whole address space.</p>
</div>
</div>
<div class="sect3">
<h4 id="_read_chunk_size">Read chunk size</h4>
<div class="paragraph">
<p>Final graph demonstrates how throughput (MB/s) and IOPS depend on read chunk size.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="ssd-chunk-size.svg" alt="ssd chunk size" width="100%">
</div>
<div class="title">SSD read chunk size</div>
</div>
<div class="paragraph">
<p>Leftmost value corresponds to purely random access, values to the right of 16 MiB
(point of throughput saturation) correspond to purely sequential access, and point
of intersection is when access type is unbiased.
Compare this latter value (64 KiB) to analogous value for HDD (256 KiB).</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_higher_levels">Higher levels</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_interfaces">Interfaces</h3>
<div class="paragraph">
<p>Software engineers are usually not aware of such low-level things as what particular interface is used to communcate to hardware.
This may turn out to be fatal in achieving performance goals because there are a lot of different interfaces and their versions
in use and each of them has its own limitations and set of problems.
Two technical characteristics which are of particular interest to software engineers are <strong>transfer rate</strong> and max <strong>number
of parallel requests</strong>.</p>
</div>
<div class="sect3">
<h4 id="_sata">SATA</h4>
<div class="paragraph">
<p>SATA is the most common interface and is found in all workstations and most servers since about 2005.
SATA stands for &#8220;serial ATA&#8221; and emphasizes the fact that it was designed as a replacement for older (P)ATA &#8220;parallel ATA&#8221;
interface.
Each SATA device is connected to the motherboard or dedicated controller with separate 7-wire cable.
It may transfer one bit at a time in both directions simultaneously.
SATA uses much higher operating frequences than (P)ATA, therefore achieving faster transfer rates even though it has less wires
than older (P)ATA (40 or 80).</p>
</div>
<div class="paragraph">
<p>A number of backward and forward compatible SATA revisions were issued since its inception in 2003 differing primarily in speed.
Because of that, revision is often specified not as a true revision number, but as wire throughput (Gbit/s) or as
transfer rate (MB/s).
For example, terms SATA-III, SATA 3.0, SATA 6Gb/s and SATA 600 are the names of the same interface.
Note the discrepancy between wire throughput (6 Gb/s) and transfer rate (600 MB/s).
As most serial protocols, SATA uses variant of 8b/10b encoding in order to transfer both data and clock signals over the same
wire among other reasons.
As such, only 80% of throughput is used to transfer useful payload.
Wire throughput may be converted to transfer rate by dividing by 10 (not by 8), e.g. for SATA-III:
<img src="math-68dde004e29b3b24.svg" class="inlinemath" style="height:4.75ex;vertical-align:-1.875ex;" alt="\frac{6000\;\mathrm{Mb/s} \cdot 0.8}{8\;\mathrm{bits/byte}} = 600\;\mathrm{MB/s}"/>.</p>
</div>
<div class="paragraph">
<p>SATA supports NCQ (Native Command Queueing), which allows to send 32 parallel requests simultaneously.
This is more than enough for hard disk drives and also is enough for SSDs and hardware RAIDs if they are used for
one request at a time mode.
But this figure is too small to unleash the true potential of highly-parallel SSDs.</p>
</div>
<div class="paragraph">
<p>One good thing about SATA is that almost all SATA controllers&#8201;&#8212;&#8201;both integrated in chipset and separate PCIe devices&#8201;&#8212;&#8201;implement standard software interface&#8201;&#8212;&#8201;AHCI (Advanced Host Controller Interface).
This interface makes SATA controller to look like an ordinal PCIe device from driver&#8217;s perspective.
It defines device addressing scheme and standard memory-mapped API to send and receive requests to SATA devices.
Thus, only single driver is required to support all models of SATA controllers implementing AHCI (this driver is called
<code>ahci</code> in linux).</p>
</div>
</div>
<div class="sect3">
<h4 id="_sas">SAS</h4>
<div class="paragraph">
<p>SAS stands for &#8220;Serial attached SCSI&#8221;.
As serial ATA came as a replacement to parallel ATA, serial SAS came as a replacement to parallel SCSI.
Conceptually is similar to SATA, but is much more complex, flexible and targets mainly high-end enterprise environments.
SAS makes it possible to do such things as connecting both SAS and SATA drives to SAS controller,
or to use multiple hardware paths to single group of drives (for redundancy and/or multiplied transfer rate).</p>
</div>
<div class="paragraph">
<p>SAS doesn&#8217;t have such level of unification as SATA does.
For example, there are different connector types may be found in the wild.
And there is no standard software API for SAS controllers&#8201;&#8212;&#8201;each manufacturer provides its own driver.
In order to measure performance, each setup of SAS must be considered on case by case basis.</p>
</div>
<div class="paragraph">
<p>As SATA, it has numerous revisions and the same naming confusion.
For example, SAS-3 and SAS 12 Gb/s are the same interface with 1.2 GB/s max data transfer rate.
Theoretically, SAS protocol itself allows to send up to 2<sup>64</sup> parallel requests but in practice this figure is limited by
controller or underlying interface.
So, SAS may be a better choice for applications which send requests in parallel to SSDs (but not so good as NVMe&#8201;&#8212;&#8201;see below).</p>
</div>
<div class="paragraph">
<p>In general, SAS enhancements over SATA target mainly system administrators; there is little difference between SAS and SATA
from software developer&#8217;s perspective.</p>
</div>
</div>
<div class="sect3">
<h4 id="_nvme">NVMe</h4>
<div class="paragraph">
<p>As mentioned earlier, SATA has severe limitation of 32 parallel requests at any given time, which is not enough to saturate SSDs.
In order to overcome this limitation, NVMe (&#8220;Non-Volatile Memory Express&#8221;) interface was developed.
It supports up to 2<sup>32</sup> parallel requests, allowing potentially to achieve millions of random access IOPS.
More than that, NVMe was designed to work efficiently in multcore environments by grouping requests into queues, with each queue
belonging to RAM of single CPU, thus achieving perfect data locality.</p>
</div>
<div class="paragraph">
<p>NVMe devices are inserted directly into PCIe slots, which also reduces transfer latency compared to wired SATA.
PCIe provides low-level generic transport, while NVMe provides command set and software protocol over PCIe.
As with SATA, only single <code>nvme</code> driver is required for all implementing devices.</p>
</div>
</div>
<div class="sect3">
<h4 id="_usb_mass_storage">USB mass storage</h4>
<div class="paragraph">
<p>Besides USB itself, its creators also designed separate higher level command protocols called &#8220;classes&#8221; for all sorts of
USB devices such as webcams, keyboards and printers.
When device is plugged in, it advertises itself to host as <code>xy</code> class device, and kernel uses this class driver to talk to
connected device.
This, once again, eliminates the necessity for each manufacturer to create its own set of protocols and drivers.</p>
</div>
<div class="paragraph">
<p><strong>USB mass storage device class</strong> is such class for communicating with block devices: USB sticks, smartphones, cameras,
SATA enclosuers, card readers.
Devices may implement this command protocol natively (USB stick) or have special chip to convert USB mass storage commands into
native device interface, which is SATA for SATA drive enclosers and SD card protocol for SD card readers.</p>
</div>
<div class="paragraph">
<p>Because primary goal of USB is to provide unified connectivity to variety of peripheral devices and not performance, it is not
very fast or low latency compared to above specialized protocols.
Maximal transfer rate is limited by USB itself, which is only 35 MB/s for USB 2.0 and 400 MB/s for USB 3.0.
Command protocol used for USB 2.0 (&#8220;Bulk Only Tranport&#8221;, BOT) has also very limited feature set.
In partucular, it is not able to handle multiple requests in parallel&#8201;&#8212;&#8201;only single request at a time.
With introduction of USB 3.0, a replacmenet command protocol for mass storage class devices emerged&#8201;&#8212;&#8201;USB Attached SCSI (UAS).
It fixes problems of BOT: it allows parallel requests and supports TRIM command.</p>
</div>
</div>
<div class="sect3">
<h4 id="_comparison_2">Comparison</h4>
<div class="paragraph">
<p>Table below lists common interfaces and their performance limitations:</p>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">&nbsp;</th>
<th class="tableblock halign-right valign-top">Max transfer rate</th>
<th class="tableblock halign-right valign-top">Parallel requests</th>
<th class="tableblock halign-left valign-top">Scope of usage</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SATA 1.0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">150 MB/s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">obsolete</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SATA 2.0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">300 MB/s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">32</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">laptops, workstations and most servers</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SATA 3.0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">600 MB/s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">32</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">laptops, workstations and most servers</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SAS-1</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">300 MB/s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">up to 2<sup>64</sup></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">obsolete</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SAS-2</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">600 MB/s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">up to 2<sup>64</sup></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">enterprise servers</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SAS-3</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1200 MB/s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">up to 2<sup>64</sup></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">enterprise servers</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">NVMe (over PCIe 2.0 x8)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4000 MB/s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2<sup>32</sup>, efficiently</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">highly parallel apps talking to SSDs</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">USB BOT (over USB 2.0)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">35 MB/s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">connecting home appliances</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">USB UAS (over USB 3.0)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">400 MB/s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">2<sup>64</sup></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">connecting home appliances</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Compare above values to charactersitics of typical devices:</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 80%;">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">&nbsp;</th>
<th class="tableblock halign-right valign-top">Typical speed</th>
<th class="tableblock halign-right valign-top">Typical parallelism</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">HDD</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">200 MB/s (read/write)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SSD (SATA)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">500 MB/s (read/write)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">32 (limited by SATA)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SSD (NVMe)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">1500 MB/s (read), 800 MB/s (write)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">128</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_schedulers">Schedulers</h3>
<div class="paragraph">
<p>Next level is software scheduler, also called &#8220;elevator&#8221;.
Scheduler is part of kernel and governs access to single raw block device.
It is possible to set different schedulers for different block devices.
Scheduler is responsible for managing block device request queue: it may reorder and/or group requests in order to optimize
performance, to satisfy fairness among different processes, to enforce IO priorities and deadlines.</p>
</div>
<div class="paragraph">
<p>Linux kernel is shipped with three standard schedulers.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">cfq</dt>
<dd>
<p>This is the default scheduler and is the most complicated one.
CFQ stands for &#8220;complete fairness queueing&#8221; and, as its name suggests, tries to achieve fairness among all requesting processes.
CFQ was mainly designed for HDDs, so it is aware of high random access cost and performs various optimizations.
For example, if there are multiple sequential streams, then it prefers to send requests from one stream first rather than
mixing requests from different streams, and does so even at the cost of some idling (by waiting for anticipating request
from upper level).</p>
<div class="paragraph">
<p>CFQ supports different scheduling classes.
Each process/thread may have different class, which may set by <code>ionice(1)</code> from command line or with <code>ioprio_set(2)</code> from
a program itself.
There are three different scheduling classes: <strong>Realtime</strong>, <strong>Best-effort</strong> (default) and <strong>Idle</strong>.
They are strongly ordered meaning that if, for example, there is pending request from a Realtime class process, then it will
be served first disregarding requests from other classes.
Processes from Realtime and Best-effort classes also have priority value 0 to 7.
They are supposed to work similar to CPU-time scheduling priorities, but in practice they are of no use.</p>
</div>
<div class="paragraph">
<p>CFQ has number of configurable options.
Their description may be found in <code>Documentation/block/cfq-iosched.txt</code> file of linux kernel.</p>
</div>
</dd>
<dt class="hdlist1">deadline</dt>
<dd>
<p>Deadline doesn&#8217;t support classes and doesn&#8217;t honor fairness among processes.
Instead, it tries to ensure that each request is dispatched to device in no more than fixed amount of time since
request was submitted to scheduler.
This time is different for reads and writes, with reads treated as more important
(configurable, see <code>Documentation/block/deadline-iosched.txt</code>).
This is because reads are usually synchronous: you can&#8217;t return control to application without data, while writes are usually
asynchronous: control returns as soon as kernel makes a copy of user data, and actual write happens in background at some point
later.</p>
</dd>
<dt class="hdlist1">noop</dt>
<dd>
<p>As name suggests (&#8220;no operation&#8221;), does nothing extraodinary.
Implements simple FIFO queue.</p>
<div class="paragraph">
<p>Noop is used in configurations when it is advantegous to send requests as they arrive to scheduler, with actual scheduling
happening somewhere at lower level.
This is the case with SSDs and also with RAID controllers.
In both cases, lower level has more information about internal organization of downstream hardware and is able to make better
scheduling decisions.
Noop is also noted for nearly zero CPU overhead.</p>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>In practice, the most useful thing you may want to do is to set Idle class (with <code>ionice(1)</code>) for batch tasks such as backuping,
thus reduce interference with simultaneously running online daemons.</p>
</div>
<div class="paragraph">
<p>It is also advisable to switch scheduler from default <code>cfq</code> to <code>noop</code> on SSD devices and RAIDs.
This may help if application fully saturates device IOPS, otherwise benefits are negligible.</p>
</div>
</div>
<div class="sect2">
<h3 id="_page_cache">Page cache</h3>
<div class="paragraph">
<p>Page cache provides transparent caching layer for block storage devices.
Logically, page cache is a pool of RAM pages with each page mapping some sector-aligned region of disk.
Each page may be either in clean or dirty state.
Clean state means that data in memory page and data in corresponding location of storage device are identical.
Dirty state means that data in memory page has newer version.
This is result of write buffering: when application calls <code>write()</code> then data from user-supplied buffer is copied into
page cache, page is marked as dirty and control returns to program.
Data is not actually written to disk at this moment, but OS pretends to application that as it was already written and will
even properly serve read requests from the same location.
That&#8217;s why it may seem that writes work almost instantaneously (in constrast to reads) even when huge volume of data is written
to slow device.
If power is lost at this moment, then data in storage may occur to be in some logically inconsistent state.
So, kernel constantly synchronizes data from dirty pages to disk and does so in background.
After page has been physically written to disk, it is still present in page cache, but now its state is clean.
If you leave system without active writes from processes, all dirty pages will converge to clean state typically in a matter
of seconds.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hlv-pagecache.svg" alt="hlv pagecache" width="60%">
</div>
<div class="title">Clean (green) and dirty (red) pages in page cache</div>
</div>
<div class="paragraph">
<p>Page cache is beneficial in multiple ways:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>It dramatically reduces read response time for frequently used locations of disk (so called <em>temporal locality</em>):
access to RAM is thousands of times faster than access to storage device.</p>
</li>
<li>
<p>It dramatically amortizes write response time due to write buffering.</p>
</li>
<li>
<p>If very frequent writes go to the same location (e.g. file metadata), then this also reduces wearing of SSDs.</p>
</li>
<li>
<p>Most of applications do not need to implement caching by themselves&#8201;&#8212;&#8201;it is already done transparently in kernel.
Only sophisticated database applications may want to perform caching explicitly, such as when special eviction policy
is required.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Quite common page cache is the largest consumer of RAM&#8201;&#8212;&#8201;all RAM not used directly by processes is available for cache.
But it has the lowest priority: if some process requests more memory, then page cache has to throw away some of its clean entries
to fulfill application demands.
Amount of RAM used currently for caching is possible to find out from <code>Cached:</code> record of <code>/proc/meminfo</code> or, more convenient,
from <code>top(1)</code> output.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hlv-top-cached.png" alt="hlv top cached" width="80%">
</div>
<div class="title"><code>top(1)</code> displaying that 80% of memory is occupied by page cache</div>
</div>
<div class="paragraph">
<p>Unit of page cache is single memory page, which is 4 KiB on x86.
When used to cache 4096n devices, single page maps natively single sector.
When used to cache 512n devices, single page maps 8 adjacent sectors aligned by 4 KiB.
Latter case raises minimal unit to avoid read-modify-write to 4 KiB.
Even if you attempt to write 512 bytes properly aligned to 512n device, then kernel will first read group of 8 adjacent
sectors from drive (unless it is already cached).</p>
</div>
<div class="paragraph">
<p>It is possible to bypass page cache by specifying <code>O_DIRECT</code> flag to <code>open(2)</code> but with some limitations.
Buffer passed to IO functions must be a multiple of logical sector size, the same is required for offset arguments.
If you need to write smaller unit of data, then you will have to implement read-modify-write by hand&#8201;&#8212;&#8201;kernel won&#8217;t do that
for you and will return <code>EINVAL</code> error instead.
This flag may be used to avoid cache pollution, if you don&#8217;t want for previously cached data to be evicted.</p>
</div>
<div class="paragraph">
<p>Another useful flag is <code>O_SYNC</code>&#8201;&#8212;&#8201;it blocks until data is actually written to storage medium.
This flag prohibits to buffer write requests in both kernel and storage device.
If <code>O_SYNC</code> is specified alone then, after <code>write()</code> returns, data is also present in page cache in clean state.
If it is specified in tandem with <code>O_DIRECT</code>, then no copy into page cache is made.
<code>O_SYNC</code> primary use case is to append data to write ahead logs, such as filesystem journals, in order to enforce
consistency guarantees.</p>
</div>
</div>
<div class="sect2">
<h3 id="_filesystems">Filesystems</h3>
<div class="paragraph">
<p>It should be obvious that mapping linearly addressable storage address space into tree-like hierarchy of files and directories
doesn&#8217;t come for free in terms of performance.
Filesystems differ in design and implementation, but typical ext*-family filesystem has following structure:</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hlv-ext.svg" alt="hlv ext" width="100%">
</div>
<div class="title">Filesystem layout</div>
</div>
<div class="paragraph">
<p>Basic low-level unit of storage allocation is block, which is usually 4 KiB.
Groups of blocks are split into four logical regions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>superblock&#8201;&#8212;&#8201;this is very small region storing core metainformation about filesystem: its settings, timestamps and state flags.
Contents of superblock may be dumped in human readable format with <code>tune2fs -l &lt;dev&gt;</code> (root privileges required).</p>
</li>
<li>
<p>inodes&#8201;&#8212;&#8201;similar to as superblock storing global metainformation about filesystem, inodes store metainformation about each
separate file: its type (regular file or directory), size, modification timestamp, owner and group identifiers, access rights,
extended attributes, and also points to blocks which store actual data.
Inodes are addressable by indices, <code>stat &lt;filename&gt;</code> may be used to print inode index and its well-known fields.
Total number of inodes is fixed during filesystem construction&#8201;&#8212;&#8201;this limits maximal allowed number of files and directories in
filesystem; <code>df -i</code> displays current consumption.</p>
</li>
<li>
<p>journal&#8201;&#8212;&#8201;small region that is used in order to be able to recover filesystem into consistent state in case of unexpected
outage.</p>
</li>
<li>
<p>data&#8201;&#8212;&#8201;all other space is occupied by data blocks.
Depending on type of file, data block may be either used exclusively to store user data (regular files) or may store
filename/inode pairs (directories).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Practical implementations are much more complex, striving to achieve optimal data packing and to minimize number of requests
to block device.
But even in simplified model presented above, number of requests for typical filesystem operation is large.
Let&#8217;s see, for example, what needs to be done to create new file <code>/a/b/c/d/file</code>:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>write intent to journal</p>
</li>
<li>
<p>write user supplied data into data block(s)</p>
</li>
<li>
<p>write metainformation into file inode</p>
</li>
<li>
<p>read root directory inode&#8201;&#8212;&#8201;to find its data block location</p>
</li>
<li>
<p>read root directory data block - to find dir <code>/a</code> inode</p>
</li>
<li>
<p>read dir <code>/a</code> inode&#8201;&#8212;&#8201;to find its data block locations</p>
</li>
<li>
<p>read dir <code>/a</code> data block&#8201;&#8212;&#8201;to find dir <code>/a/b</code> inode</p>
</li>
<li>
<p>&#8230;&#8203;</p>
</li>
<li>
<p>write updated version of dir <code>/a/b/c/d/</code> extended with file&#8217;s name and its inode index</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>With such large number of requests, performance can&#8217;t be good without significant optimizations.
Here is where page cache comes into action.
The same inodes and directories are usually accessed again and again.
This results in that their copies live constantly within page cache, thus amortizing total number of requests to constant number.
Append to journal is buffered and may be easily combined with other such appends from other IO requests.
The same is true for writes to adjacent inodes.
Last two requests have to be delivered to storage as separate requests, though.
If inodes and directories are not cached&#8201;&#8212;&#8201;this is the case with ultra large and deep directory trees&#8201;&#8212;&#8201;then all those
inner steps must be performed online, which cripples performance.</p>
</div>
<div class="paragraph">
<p>As with all previous levels, programmers have to deal with read-modify-write problem once again.
Luckily, filesystems are typically designed in such way that each block has one-to-one mapping to a group of one or more
adjacent sectors, and each block is used exclusively either to store actual data or to store filesystem internal structures,
but not both. As such, standard rule of avoiding RMW by making write requests by block boundary still works and is applicable
to each separate file.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hlv-rmw.svg" alt="hlv rmw" width="80%">
</div>
<div class="title">RMW when overwriting (top) and appending (bottom)</div>
</div>
<div class="paragraph">
<p>Another problem that comes with filesystems is <strong>internal fragmentation</strong>.
Because allocation happens in units of blocks, this results in that last block of file is only partially filled (unless file
size is a multiple of block size).
This severely reduces capacity efficiency for small files.
In extreme case, filling 4KiB-block filesystem with single byte files results in only <code>1/4096 = 0.02%</code> efficiency.
Some filesystems mitigate this problem by packing very small files together into single block, and when file is appended to,
it is reallocated into another place.</p>
</div>
<div class="paragraph">
<p>Other types of fragmentation are also possible but are hard to encounter.
If filesystem device is low on free space, or if some file is periodically appended to, then this may result in that there
are no free blocks immediately after last file block, and filesystem will have to use some distant blocks to fulfill next
<code>write()</code> call.
This results in <strong>file fragmentation</strong>.
Such situation is not good for sequential access because it requires sending separate requests instead of one,
which is particular bad with HDDs.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="hlv-file-fragmentation.svg" alt="hlv file fragmentation" width="80%">
</div>
<div class="title">File fragmentation</div>
</div>
<div class="paragraph">
<p>Image above demonstrates greatly exaggerated effect: in practice, file fragmentation is hard to come across due to clever
allocation algorithms employed by filesystems.
They scatter different files along available space rather than packing them densely in order to reduce chances
of future file fragmentation.
Also they may perform basic online defragmentation on demand by moving blocks of single file close together to provide better data
continuity.
Anyway, if you know file length in advance, then it is advisable to use <code>fallocate -l &lt;length&gt;</code>: it reserves enough blocks
for specified file length.
Providing information about file length to filesystem in advance allows filesystem to make optimal allocation decisions.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_tips_and_tricks">Tips and tricks</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_optimizations">Optimizations</h3>
<div class="sect3">
<h4 id="_disable_atime">Disable atime</h4>
<div class="paragraph">
<p>Traditional *nix filesystems store three timestamps along with each file (see <code>stat(2)</code> for details):</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">mtime</dt>
<dd>
<p>Modify time.
This timestamp is updated each time file is written to.</p>
</dd>
<dt class="hdlist1">ctime</dt>
<dd>
<p>Change time.
Updated each time file is written to or file&#8217;s metainformation is changed.
Latter is triggered, for example, by calling <code>chmod(1)</code>.</p>
</dd>
<dt class="hdlist1">atime</dt>
<dd>
<p>Access time.
Updated each time file is read from.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>We are interested in the last timestamp, <strong>atime</strong>.
It&#8217;s usefulness is questionable and negative performance effects are obvious.
With <code>atime</code> enabled, each read operation is automatically amended by an additional write, which is performed to
update <code>atime</code> timestamp.
This reduces IOPS performance if HDDs are used and facilitates premature wearing if SSDs are used.
So, if your programs do not use this timestamp on purpose, it is better to disable updates to it.
The standard way to disable it is to pass <code>noatime</code> option to the mount command via <code>/etc/fstab</code>.
It will disable atime updates for all files of given mount point.</p>
</div>
<div class="listingblock">
<div class="content">
<pre># &lt;file system&gt; &lt;mount point&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;
UUID=1d29600c-b536-41d7-962e-2bb24aec720c / ext4 noatime,errors=remount-ro 0 1</pre>
</div>
</div>
<div class="paragraph">
<p>It is also possible to disable <code>atime</code> updates programmatically by passing <code>O_NOATIME</code> flag to <code>open(2)</code>.
Reads issued to the file through created file descriptor won&#8217;t update atime.</p>
</div>
</div>
<div class="sect3">
<h4 id="_squeeze_all_effective_space_out_of_filesystem">Squeeze all effective space out of filesystem</h4>
<div class="paragraph">
<p>Suppose you have 2 TB disk drive and you create ext4 filesystem with default settings in it.
How much of this space will be available for storing data?
The answer is: 93%.
7%, or 140 GB, won&#8217;t be accessible to ordinary processes.
Luckily, we can increase effective space by tweaking filesystem settings.
Here is the distribution of space for ext4 filesystem with default settings created in 2 TB partition:</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-fs-capacity-overhead.svg" alt="tip fs capacity overhead" width="90%">
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Reserved blocks</dt>
<dd>
<p>By default, ext4 reserves 5% of capacity to be available exclusively for root user.
This means that all non-root processes together may occupy maximum of 95% of designed capacity.
Filesystem will return an error if one of them attempts to write beyond that value, even though <code>df</code> reports that there is
up to 5% of free space left at this point.
Remaining 5% may be used only by processes with root privileges.
Such limitation was implemented in order to make system administration more robust: if ordinary processes make disk full (95%),
system daemons including logging daemon will still be able to write to disk.
This gives system administrator some time to resolve the issue.</p>
<div class="paragraph">
<p>But reserving 5% of space is pointless if partition is used exclusively to store application data.
This is often the case with large storage systems, which intrinsically have a lot of data-only disk drives.
Reserved space percentage may be safely reduced to 0% for such systems.
It may be done during filesystem consturction by specifiying <code>-m &lt;pcs&gt;</code> option to <code>mkfs.ext4</code>.
In already existing filesystems it can be changed on the fly with <code>tune2fs -m &lt;pcs&gt;</code>.
Currently reserved capacity is reported by <code>tune2fs -l</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># tune2fs -l /dev/sda5
...
Block count:              36621056
Reserved block count:     1831052
...</pre>
</div>
</div>
</dd>
<dt class="hdlist1">Inode table</dt>
<dd>
<p>Maximal number of files and directories (inodes) is fixed when filesystem is created and can&#8217;t be changed later.
Ext4 reserves space for inode table, which occupies <img src="math-4ab1ce07079b9aa4.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.031ex;" alt="{\mathrm{InodeCount}}\times{\mathrm{InodeSize}}"/> bytes.
Inode size is printed by <code>tune2fs</code> and usually equals to 256 bytes.
By default, ext4 creates single inode per each 16 KiB of data.
This results in too many inodes being unused unless average file size is very small or directory hierarchy is very deep.
Current inode usage is printed by <code>df -i</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ df -i
Filesystem         Inodes   IUsed      IFree IUse% Mounted on
/dev/sda5         9158656  616765    8541891    7% /</pre>
</div>
</div>
<div class="paragraph">
<p>To reduce inode count, pass <code>-i &lt;count&gt;</code> to <code>mkfs.ext4</code>, where <code>&lt;count&gt;</code> is bytes/inode ratio.
Once filesystem is created, this value can&#8217;t be changed.</p>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p><strong>Example.</strong>
Consider 2 TB disk drive.
If we reduce reserved block count to 0% and inode count to be 1 per each 64 KiB of data, then we will get additional
<img src="math-fd79c79f533158be.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.359ex;" alt="2\;\mathrm{TB}\cdot(0.012+0.05)=124\;\mathrm{GB}"/>.
Thus, effective space is increased from 93.3% to 99.5%.</p>
</div>
</div>
<div class="sect3">
<h4 id="_use_tmpfs_or_ramfs_to_reduce_io">Use tmpfs or ramfs to reduce IO</h4>
<div class="paragraph">
<p>Some libraries are designed to process standalone files and do not provide API for reading from in-memory buffers.
Let&#8217;s say that you want to extract meta information from photoes which are stored somewhere in database rather than in files.
This database provides iterator-like access, and in order to use selected library,
you first need to write each photo into separate file,
then to pass file name to library function, and then to delete file.
Problem is that saving thousands and millions of photoes to filesystem would take immense amount of time because of IO.
Most likely it will take more time than even parsing photoes.</p>
</div>
<div class="paragraph">
<p>Solution is to write files not to a persistent filesystem, but to a in-memory filesystem: <code>tmpfs</code> or <code>ramfs</code>.
See corresponding sections of <code>mount(8)</code> for possible options.
You will still need to use above sequence of actions, but at least this approach will eliminate unnecessary disk IO.</p>
</div>
<div class="listingblock">
<div class="content">
<pre># mkdir -p /apps/tmp
# mount -t tmpfs -o size=$((128*1024*1024)),uid=appuser,gid=appuser none /apps/tmp
# df -h
Filesystem      Size  Used Avail Use% Mounted on
...
none            128M     0  128M   0% /apps/tmp
...</pre>
</div>
</div>
<div class="paragraph">
<p>Such approach is perfect for infrequent batch jobs, but is a bit dirty if you are planning on using it in the course
of standard dataflow.
In latter case it may be cheaper to spend some time adding support for in-memory buffers directly into the library.</p>
</div>
</div>
<div class="sect3">
<h4 id="_use_right_syscall_to_flush_buffers">Use right syscall to flush buffers</h4>
<div class="paragraph">
<p>When write request is made, it usually appears to be performed immediately.
This is the result of buffering modified data.
There are at least two places where data may be buffered: page cache (modified pages has &#8220;dirty&#8221; flag set) and storage device
itself.
Buffering makes possible number of optimizations to be employed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Call to <code>write()</code> may return immediately, while physical write continues to run in background along with the application.
Thus, programmer doesn&#8217;t need to resort to more complicated non-blocking IO.</p>
</li>
<li>
<p>Multiple write requests from user application may be grouped into single big one, which is cheaper than separate requests.</p>
</li>
<li>
<p>Requests may be reordered.
This is the case with hard disk drive&#8217;s internal scheduler: it may rearrange requests in order to minimize rotational latency
and to reduce distance actuator arm has to travel.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Because flushing is an important part of applications which have to guarantee consistency and persistence of written data,
there are a lot of different ways to flush buffered data characterized by different granularity levels:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>sync()</code>&#8201;&#8212;&#8201;this is global flush.
Guarantees that <em>all</em> outstanding modifications are written to <em>all</em> block devices.
Hence, it is the most expensive flushing syscall.
It is also available as command line utility <code>sync(1)</code> accessible to all users.</p>
</li>
<li>
<p><code>syncfs(fd)</code>&#8201;&#8212;&#8201;similar to <code>sync()</code> but flushes only block device to which <code>fd</code> belongs.</p>
</li>
<li>
<p><code>fsync(fd)</code>&#8201;&#8212;&#8201;flushes modifications of only single file.</p>
</li>
<li>
<p><code>fdatasync(fd)</code>&#8201;&#8212;&#8201;flushes modifications of only single file without flushing metadata.
Call to this function guarantees that modified contents is flushed, but doesn&#8217;t guarantee that file&#8217;s metadata is flushed
(such as mtime and ctime).</p>
</li>
<li>
<p><code>sync_file_range(fd, offset, nbytes, flags)</code>&#8201;&#8212;&#8201;flushes modifications of single file in given offset range.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Flushing buffers may be considered complementary to invalidating caches.
Invalidating caches frees memory occupied by non-dirty pages only, while syncing buffers forces dirty pages to be written onto
medium.
In order to free <em>whole</em> page cache, you first need to call <code>sync()</code>, thus ensuring that there are no pages in dirty state left,
and only then to drop caches.
In practice, syncing is omitted because number of dirty pages is usually very small&#8201;&#8212;&#8201;dirty pages are automatically synced after only couple of seconds after being &#8220;spoilt&#8221;.
Clean pages, in contrast, may occupy all available RAM.</p>
</div>
</div>
<div class="sect3">
<h4 id="allocate-separate-partition-hdd">Allocate separate partition for fast data (HDD)</h4>
<div class="paragraph">
<p>As described in HDD section, performance of sequential access depends on location of data in HDD: it worsens steadily with
offset growth.
Sometimes it is a good idea to create separate partitions for &#8220;slow&#8221; and &#8220;fast&#8221; data.
Example below demonstrates effect of partition location.
Two 10 GB partitions were created, one in the very beginning of HDD and another one in the very end of HDD.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-fast-slow-partitions.svg" alt="tip fast slow partitions" width="45%">
</div>
</div>
<div class="paragraph">
<p>Copying file to first partition takes only 35 seconds:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ time { cp data_file mnt1/; sync; }

real    0m35.338s
user    0m0.016s
sys     0m1.664s</pre>
</div>
</div>
<div class="paragraph">
<p>Copying the same file to second partition takes 51 sec&#8201;&#8212;&#8201;1.46 times slower:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ time { cp data_file mnt2/; sync; }

real    0m51.468s
user    0m0.016s
sys     0m1.592s</pre>
</div>
</div>
<div class="paragraph">
<p>Long time ago, when hard drives were much smaller in capacity and slower, it was common to create root and swap partitions
in the beginning of HDD to optimize system performance.
Nowadays, when swap area concept is extinct and drives are much faster, it is usually not worse to trade flexibility of single
large partition to only slight performance improvements of multi-partition design.
But still, sometimes it makes sense to differentiate data by performance requirements and lock different types of data
to separate partitions.
For example, someone may allocate special &#8220;fast&#8221; partition for writing log files in a very high-load server.</p>
</div>
<div class="paragraph">
<p>Note that SSDs do not suffer from performance unevenness: their performance is uniform along whole range of addresses.</p>
</div>
</div>
<div class="sect3">
<h4 id="_use_reasonable_read_buffer_size">Use reasonable read buffer size</h4>
<div class="paragraph">
<p>Proper selection of buffer size may dramatically increase or decrease application performance.
Reasons to select particular value depend on type of application and environment.
This section demonstrates how buffer size affects applications which read huge files and process them on the fly.
Typical examples of such applications include grepping logs with complex regexp, computing aggregate funcion over large CSV
file or verifying MD5 sum.
Crucial part here is that there are two and only two dependent workers: storage (blocking read mode) and CPU.</p>
</div>
<div class="paragraph">
<p>Chart below demonstrates how <code>bufsize</code> value influences working time of below code snippet.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="pygments highlight"><code data-lang="cpp"><table class="pyhltable"><tr><td class="linenos"><div class="linenodiv"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10</div></td><td class="code"><span></span><span class="tok-n">std</span><span class="tok-o">::</span><span class="tok-n">ifstream</span> <span class="tok-n">s</span><span class="tok-p">;</span>
<span class="tok-kt">char</span> <span class="tok-n">buf</span><span class="tok-p">[</span><span class="tok-n">bufsize</span><span class="tok-p">];</span>
<span class="tok-n">s</span><span class="tok-p">.</span><span class="tok-n">rdbuf</span><span class="tok-p">()</span><span class="tok-o">-&gt;</span><span class="tok-n">pubsetbuf</span><span class="tok-p">(</span><span class="tok-n">buf</span><span class="tok-p">,</span> <span class="tok-n">bufsize</span><span class="tok-p">);</span>
<span class="tok-n">s</span><span class="tok-p">.</span><span class="tok-n">open</span><span class="tok-p">(</span><span class="tok-n">filename</span><span class="tok-p">,</span> <span class="tok-n">std</span><span class="tok-o">::</span><span class="tok-n">ios_base</span><span class="tok-o">::</span><span class="tok-n">in</span><span class="tok-o">|</span><span class="tok-n">std</span><span class="tok-o">::</span><span class="tok-n">ios_base</span><span class="tok-o">::</span><span class="tok-n">binary</span><span class="tok-p">);</span>
<span class="tok-p">...</span>
<span class="tok-kt">char</span> <span class="tok-n">val</span><span class="tok-p">[</span><span class="tok-mi">8</span><span class="tok-p">];</span>
<span class="tok-k">while</span> <span class="tok-p">(</span><span class="tok-o">!</span><span class="tok-n">s</span><span class="tok-p">.</span><span class="tok-n">eof</span><span class="tok-p">())</span> <span class="tok-p">{</span>
  <span class="tok-n">s</span><span class="tok-p">.</span><span class="tok-n">read</span><span class="tok-p">(</span><span class="tok-n">val</span><span class="tok-p">,</span> <span class="tok-k">sizeof</span><span class="tok-p">(</span><span class="tok-n">val</span><span class="tok-p">));</span>
  <span class="tok-n">process</span><span class="tok-p">(</span><span class="tok-n">val</span><span class="tok-p">);</span>
<span class="tok-p">}</span>
</td></tr></table></code></pre>
</div>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-bufsize.svg" alt="tip bufsize" width="70%">
</div>
</div>
<div class="paragraph">
<p>If buffer size is very small then performance is poor because of extreme number of syscalls.
Each syscall has some constant-time overhead, which is unnoticable if number of syscalls is few, but which becomes a bottleneck
if number of syscalls is large.
IO and data processing do not contribute much to overall time in this case.</p>
</div>
<div class="paragraph">
<p>Region between ~1 KiB and 128 KiB is where performance is the best.
Optimality is achieved by effective use of read-ahead.
Application blocks for long time only during first call to <code>read()</code>.
After that, read-ahead is activated and it works simultaneously with application, while it processes previously read data.
When application calls <code>read()</code> again, requested data is already read into disk cache, and the only thing that needs to be
waited for is copying of data from kernel buffer into userspace buffer.
No storage IO is involved during <code>read()</code> syscalls.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-bufsize-optimal.svg" alt="tip bufsize optimal" width="80%">
</div>
</div>
<div class="paragraph">
<p>If buffer size is increased even further, performance degrades once again.
This happens when buffer size exceeds max read-ahead value (it was 512 byts/sector × 256 sectors in above test environment).
Read-ahead still works in this case, but when application requests kernel to <code>read()</code>, then data amount is kernel buffer is not
enough, so blocking IO takes place.
Technically, kernel may return immediately with only partially filled userspace buffer, but it&#8217;s up to kernel/filesystem to decide
when to do so or when to block for additional IO.
Furthermore, some multi-tiered IO libraries may call <code>read()</code> in loop until buffer is fully filled.
As such, it&#8217;s better not to rely on such subtleties.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-bufsize-toolarge.svg" alt="tip bufsize toolarge" width="100%">
</div>
</div>
<div class="paragraph">
<p>Default buffer size of most libraries is 8192 (e.g. libstdc&plus;&plus;'s <code>std::basic_streambuf</code>,
OpenJDK&#8217;s <code>java.io.BufferedReader</code>), which is pretty good choice for wide range of workloads and hardware.
Tuning buffer size and/or read-ahead settings may increase performance, but this needs to be done separately for each
environment.
Anyway, chart above strongly suggests not to make buffer size larger than read-ahead value.</p>
</div>
</div>
<div class="sect3">
<h4 id="_specify_large_block_size_to_code_dd_code">Specify large block size to <code>dd</code></h4>
<div class="paragraph">
<p>Copying with <code>dd</code> is another situation when buffer size selection is important.
In this case, there are two workers: storage (read mode) and storage (write mode).</p>
</div>
<div class="paragraph">
<p>If you forget to specify block size (<code>bs=</code>) when using <code>dd</code>, then it will use default block size, which is only 512 bytes.
This won&#8217;t create significant performance penalty on ordinary filesystems: read-ahead will ensure that no blocking
occurs during reads (to the extent limited by device speed), while buffering and grouping of writes will ensure that no
blocking occurs during writes.
But non-standard filesystems (e.g. distributed) may use synchronized IO implicitly meaning that each separate 512-byte
write syscall will issue single physical request and wait for its completion before returning control to <code>dd</code>.
Blocking writes will become a bottleneck.</p>
</div>
<div class="paragraph">
<p>To achieve reasonable performance in such situations, buffer size should be set to very large value (order of megabytes).
Ideally, read-ahead should also be increased up to the same value, but this is unfeasible if <code>dd</code> is used for one-time job.</p>
</div>
<div class="paragraph">
<p>To demonstrate the difference, external USB 2.0 drive was mounted with <code>-o sync</code> option.
Compare performance with default block size (tired to wait until full completion):</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ dd if=data_file of=mnt/dst_file
766213+0 records in
766213+0 records out
392301056 bytes (392 MB) copied, 1550.98 s, 253 kB/s</pre>
</div>
</div>
<div class="paragraph">
<p>... and with 4 MiB block size specified explicitly:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ dd if=data_file of=mnt/dst_file bs=$((4*1024*1024))
256+0 records in
256+0 records out
1073741824 bytes (1.1 GB) copied, 37.8691 s, 28.4 MB/s</pre>
</div>
</div>
<div class="paragraph">
<p>Test run with large buffer size demonstrated ~112x improvement over default value.
As it was figured out from kernel trace, requests to disk drive were issued by 240 sectors each (limitation of USB).</p>
</div>
</div>
<div class="sect3">
<h4 id="_pad_data_to_avoid_rmw">Pad data to avoid RMW</h4>
<div class="paragraph">
<p>Suppose that you are designing data storage containing fixed-length records, let&#8217;s say 4000 bytes each.
Entire storage file is large enough so that it doesn&#8217;t fit into disk cache.
For example, it might represent some huge hash table with open addressing.
Storage must support record overwriting (insert) among set of supported operations.
If you do this naively and allocate only 4000 bytes for each record, then you will stumble across read-modify-write.
Better solution is to allocate 4096 bytes for each record: 4000 first bytes are used for storing record
and remaining 96 bytes are filled with zeroes (padding).</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-padding.svg" alt="tip padding" width="100%">
</div>
</div>
<div class="paragraph">
<p>Because now each record is aligned by sector boundary, no RMW happens, which results in the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>better response time: each record insert is done by executing exactly one write operation to drive</p>
</li>
<li>
<p>as an immediate consequence, less wearing in case if SSD is used</p>
</li>
<li>
<p>you may easily switch to direct IO by appending <code>O_DIRECT</code> flag if you don&#8217;t want to pollute cache;
no additional coding is required</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Downside is reduced capacity efficiency because some space is wasted for paddings.</p>
</div>
<div class="paragraph">
<p>In the example below, 1M random records were overwritten inside 40 GB file.
Padding records resulted in 40% reduction in running time.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 50%;">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-right valign-top">Record size</th>
<th class="tableblock halign-right valign-top">&Sigma; pwrite()</th>
<th class="tableblock halign-right valign-top">Final fsync()</th>
<th class="tableblock halign-right valign-top">Total time</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">4000</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">124.4 s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">13.3 s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">137.7 s</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">4096</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">49.3 s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">33.0 s</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">82.3 s</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="_align_partitions_by_physical_sector_to_avoid_rmw">Align partitions by physical sector to avoid RMW</h4>
<div class="paragraph">
<p>Read-modify-write may also happen because of partitions not aligned by physical sector size.
It&#8217;s hard to misalign partitions in brand new device because partitioning software such as <code>fdisk(8)</code> uses kernel-provided
value for physical sector size and suggests all offset values to be multiple of it.
But it is easy to misalign partitions unintentionally when migrating from 512n device to 512e by making low-level copy
with <code>dd(1)</code>.
Performance degradation may be similar or even more severe than in previous section because now every write&#8201;&#8212;&#8201;even to file metadata&#8201;&#8212;&#8201;triggers RMW.
It is clearly seen in example below that <code>sda2</code> is misaligned because its first sector address 104859645 is not divisible by 8:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
...

Device     Boot     Start       End   Sectors   Size Id Type
/dev/sda1            2048 104859644 104857597    50G 83 Linux
/dev/sda2       104859645 468862127 364002483 173.6G 83 Linux</pre>
</div>
</div>
<div class="paragraph">
<p>It is advisable to always allocate partitions with offset and size values being multiples of 4096 even in 512n devices.
Also use <code>fdisk(1)</code> to check for alignment problems after low-level migrations: all partitions must have multiple-of-8
start sector addresses.</p>
</div>
</div>
<div class="sect3">
<h4 id="_use_idle_scheduling_class_for_maintenance_jobs">Use idle scheduling class for maintenance jobs</h4>
<div class="paragraph">
<p>Every server, even online one, requires periodical storage maintenance such as making backups or compressing log files.
If maintenance job is started without appropriate precautions, then chances that it will consume major part of disk time
are high due to batch nature of such jobs.
Result is severe reduction in performance of primary service for the duration of maintenance job run.
Example of such situation is demonstrated in image below.
Green line represents IOPS of online service, time flows left to right.
At some point maintenance job (red) was started: it worked for a brief moment only, but this was enough to halve IOPS of main
service.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-ionice-naive.svg" alt="tip ionice naive" width="90%">
</div>
<div class="title">Both server and maintainance job are in best-effort (default) IO class</div>
</div>
<div class="paragraph">
<p>Problem may be solved by starting maintainance job in the idle IO scheduling class: <code>ionice -c idle &lt;command&gt;</code>.
It takes more time to complete in this case but online service performance is nearly unaffected.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-ionice-fixed.svg" alt="tip ionice fixed" width="90%">
</div>
<div class="title">Maintainance job is in idle IO class</div>
</div>
<div class="paragraph">
<p>Note that such trick works only if using CFQ scheduler&#8201;&#8212;&#8201;other schedulers do not support priority classes.</p>
</div>
<div class="paragraph">
<p>Also beware that once submitted to scheduler, IO request is moved into the queue of corresponding IO class and remains there
until dispatched.
Even if you change IO class of the running process on the fly with <code>ionice -p &lt;pid&gt;</code>, then it will affect only new IO requests.
This normally doesn&#8217;t present problems for online services because they are designed to have large IOPS reserve,
that is, there are always time slots when disk is not used.
But if you start maintenance job in idle IO class in batch server where there is best-effort class process constantly occupying
disk time, then maintenance job will be blocked forever.
Requests from idle queue are dispatched only when there were no requests of higher classes (realtime and best-effort) for couple
hundreds of milliseconds.</p>
</div>
</div>
<div class="sect3">
<h4 id="_understand_performance_limitations_of_b_tree_indices">Understand performance limitations of B-tree indices</h4>
<div class="paragraph">
<p>Almost all SQL and no-SQL DBMS support two types of indices: hash and B-tree.
Hash indices are simple, but B-tree indices are much more complex.
Their performance limitations are poorly understood.</p>
</div>
<div class="paragraph">
<p>As an example, here is presented B-tree index built atop of single-character column.
Main storage is represented by the table on the right, it stores all rows one by one in no particular order.
B-tree index on the left is built atop of <code>chr</code> column and it maps characters (index keys) into file offsets inside main
storage (index values).
Index keys are sorted inside B-tree index and each of them additionally contains a pointer into main storage.
For convenience, only single pointer for <code>L</code> character is shown.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-btree.svg" alt="tip btree" width="100%">
</div>
</div>
<div class="paragraph">
<p>B-tree indices are used in two ways depending on query:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To search for single row by exact key (lookup)</p>
</li>
<li>
<p>To search for set of rows by range of keys (range scan)</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Key lookup</div>
<p>Consider following query: <code>SELECT * FROM table WHERE chr = "L"</code>.
When such query is issued, DBMS searches for given character in above B-tree index.
If it is found, then index node also contains address into main storage.
DBMS loads record by this address and returns it to requesting application.</p>
</div>
<div class="paragraph">
<p>Now let&#8217;s estimate performance.
To find single character in index, DBMS needs to traverse B-tree from root node down to the node
with desired column value.
This requires access to <img src="math-ebce6ee31a8c53b7.svg" class="inlinemath" style="height:1.938ex;vertical-align:-0.547ex;" alt="\log_{k}(N)"/> nodes, where <img src="math-8ce86a6ae65d3692.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.0ex;" alt="N"/> is total number of rows
in database and <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/>
is the number of column values stored in each index node (-arity).
Obvious enough, <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/> has determinant role in overall performance.
How large is <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/>?
Access time must be paid to access each separate node, which takes particularly long in case HDD
is used as storage device.
That&#8217;s why real-world DBMS use large-sized nodes, in order of tens and hundreds of kilobytes.
Such large node sizes result, in turn, in large <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/> values&#8201;&#8212;&#8201;hundreds and thousands&#8201;&#8212;&#8201;and still keep time it takes to load single node not greatly exceeding access time alone.
With such <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/> values, typical B-tree has only 3-4 levels even for databases with extremely
large number of records (hundreds of millions).
Combining with additional access to main storage, single lookup requires 4-5 storage accesses
in total.</p>
</div>
<div class="paragraph">
<p><strong>Example.</strong>
Suppose that we have a database with <img src="math-c1c7df530cb007d2.svg" class="inlinemath" style="height:1.797ex;vertical-align:-0.031ex;" alt="N=10^9"/> rows in it and we want to index 12 byte keys.
This may mean, in terms of SQL, indexing three INT columns simultaneously or single CHAR(12) column.
Pointers to both B-tree nodes and into main storage are 8 bytes each, no compression is used.
We decide to use 64 KiB nodes, resulting in that each node has capacity to store
<img src="math-7288b72ed6324f11.svg" class="inlinemath" style="height:4.141ex;vertical-align:-1.406ex;" alt="\frac{{64}\cdot{1024}}{12+8+8}\approx 2340"/> records.
This capacity is not necessarily used in full, but typical B-tree allocation algorithm rebalances
B-tree on each modification
in such way that each node is occupied by at least half of this capacity.
As such, each node stores at least
<img src="math-65af665fc1963f07.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.031ex;" alt="{2340}\cdot{0.5}=1170"/>
keys, and whole B-tree has height of only
<img src="math-b4a5f3f845dd8865.svg" class="inlinemath" style="height:2.047ex;vertical-align:-0.547ex;" alt="\lceil{\log_{1170} 1\,000\,000\,000}\rceil \approx \lceil{2.93}\rceil = 3"/>
levels.
If index is located in HDD with access time of 5 ms and average sustained speed of 150 MB/s, then
single node is read in
<img src="math-3f7dbeeb2492b773.svg" class="inlinemath" style="height:4.609ex;vertical-align:-1.875ex;" alt="5\,\mathrm{ms} + \frac{{64}\cdot{1024}\;\mathrm{B}}{150\,000\;\mathrm{B/ms}} \approx 5\,\mathrm{ms} + 0.43\,\mathrm{ms}"/>
and entire request is completed in
<img src="math-e00b56caf7765f49.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.359ex;" alt="5.43\;\mathrm{ms}\cdot{(3+1)}=21.72\;\mathrm{ms}."/></p>
</div>
<div class="paragraph">
<p>This figure is not final yet.
Caching&#8201;&#8212;&#8201;whether it is done implicitly by OS or explicitly by DBMS itself&#8201;&#8212;&#8201;also plays
important role.
Root node of B-tree is top candidate for caching because it is accessed during each request.
Nodes of second and further levels may also be cached, but this greatly depends on amount
of available RAM.
Different environments and applications may demonstrate cache hit rate for low-level nodes
anywhere between zero and one hundred percent.
It can be said that caching reduces total number of physical accesses to storage down to only
2-3 in average case.
However, translated to real-world time, performance is still very poor if HDD is used.
Request execution time and throughput in above example are limited to 10 ms and 100 RPS
(requests per second) respectively even with aggressive caching.
These figures may be improved by orders of magnitude by replacing HDD with SSD or by installing
plenty of RAM.
If second approach is chosen, then you want indices to be fully cached in RAM.
Disproportion between access times of HDD and RAM is so huge that even small percentage
of accesses (1%) going to HDD would jeopardize overall performance.</p>
</div>
<div class="paragraph">
<div class="title">Range scan</div>
<p>Performance is much better if B-tree index is used for searching keys in some range:
<code>SELECT * FROM table WHERE chr &gt;= "H" AND ch &lt;= "R"</code>.
Improvement is possible because keys are stored in sorted order in B-tree index.
Once first key in given range is found, DBMS traverses nodes one by one, returning all keys from these nodes.
For example, order of reads for above query would be M, DG, HKL, QT, NP, RS.
This makes number of storage accesses very attractive: <img src="math-2b6b5286b5dd7fcc.svg" class="inlinemath" style="height:3.938ex;vertical-align:-1.688ex;" alt="\frac{m}{k_\mathrm{min}}"/>, where <img src="math-62c66a7a5dd70c31.svg" class="inlinemath" style="height:0.906ex;vertical-align:-0.016ex;" alt="m"/> is the number
of rows matched and <img src="math-f07125fb65972607.svg" class="inlinemath" style="height:1.719ex;vertical-align:-0.313ex;" alt="k_{\mathrm{min}}"/> is minimal number of children node is allowed to have.</p>
</div>
<div class="paragraph">
<p>Single range scan query working in the context of previous example and returning 50K rows will need to make only
<img src="math-cf5234081cd7bfb7.svg" class="inlinemath" style="height:4.813ex;vertical-align:-1.906ex;" alt="{\left\lceil\frac{50\,000}{1170}\right\rceil}=43"/>
node accesses or, translated to query execution time,
<img src="math-a4b7bc1b01026aa3.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.031ex;" alt="{43}\cdot{5.43}\;\mathrm{ms}=234\;\mathrm{ms}"/>&#8201;&#8212;&#8201;very good timing for such heavy query.
If index contains copies of all requested fields, then request is complete at this point.
Otherwise, access to main storage is required for each matched row.
This makes overall performance poor once again: 43 accesses to index plus <img src="math-af92308b4d0fa668.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.031ex;" alt="50\,000"/> accesses to main storage.</p>
</div>
<div class="paragraph">
<div class="title">Bonus: searching for optimal node size</div>
<p>It is possible to build theoretical estimation of optimal B-tree node size for lookup operation.
Sum of random access timings and sum of linear read timings counteract each other, making <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/>
not too big, not too small.
If <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/> is very small <img src="math-8bf9007ec3afc538.svg" class="inlinemath" style="height:1.797ex;vertical-align:-0.391ex;" alt="(k \less 10),"/> tree height becomes very huge, resulting in long
lookup time due to large number of random accesses.
For extreme case of binary tree (<img src="math-4e5347e07e38c76e.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k=2"/>) and <img src="math-c1c7df530cb007d2.svg" class="inlinemath" style="height:1.797ex;vertical-align:-0.031ex;" alt="N=10^9"/>, there are 30 levels
in a tree requiring 30 distinct random accesses during single lookup.
Conversely, if <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/> is selected to be very large&#8201;&#8212;&#8201;up to available RAM size&#8201;&#8212;&#8201;then throughput
is the limiting factor: it takes considerable amount of time to read each single node into memory,
but only tiny fraction of it is required to make decision on what node has to be
accessed next (with binary search inside current node).
Optimal <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/> value lies somewhere between these two extreme cases.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s start with writing general formula for lookup time <img src="math-6faae11227201acd.svg" class="inlinemath" style="height:1.766ex;vertical-align:-0.359ex;" alt="T_{\mathrm{lu}}\left(k\right)"/>:</p>
</div>
<div class="imageblock text-indent">
<div class="content">
<img src="math-98b792934d48d503.svg" alt="T_{\mathrm{lu}}(k) \;=\; \log_{k}{N}\left(t_{\mathrm{ra}}+\frac{k\left(2d_{\mathrm{ptr}}+d_{\mathrm{fld}}\right)}{v}\right)" height="51">
</div>
</div>
<div class="paragraph">
<p>In the formula above,
<img src="math-373b6ec4d837bf0b.svg" class="inlinemath" style="height:2.0ex;vertical-align:-0.594ex;" alt="d_{\mathrm{ptr}}"/> denotes size of single pointer (typ. 8 bytes),
<img src="math-03bd2fb60a2d4023.svg" class="inlinemath" style="height:1.719ex;vertical-align:-0.313ex;" alt="d_{\mathrm{fld}}"/>&#8201;&#8212;&#8201;size of indexed field(s), which may range from 1 bit to hundreds
bytes (indexed field is a character or byte string),
<img src="math-4c94485e0c21ae6c.svg" class="inlinemath" style="height:0.906ex;vertical-align:-0.016ex;" alt="v"/> and <img src="math-1b88cc42001ad78c.svg" class="inlinemath" style="height:1.578ex;vertical-align:-0.313ex;" alt="t_{\mathrm{ra}}"/> are characteristics of storage device: average sustainted
throughput (bytes/second) and average random access time (seconds) respectively.
Following steps lead to optimial <img src="math-8254c329a92850f6.svg" class="inlinemath" style="height:1.422ex;vertical-align:-0.016ex;" alt="k"/> value <img src="math-940e6a915db09950.svg" class="inlinemath" style="height:2.0ex;vertical-align:-0.594ex;" alt="k_{\mathrm{opt}}"/>, when lookup time
<img src="math-e9332c0b0453a56e.svg" class="inlinemath" style="height:1.766ex;vertical-align:-0.359ex;" alt="T_{\mathrm{lu}}(k)"/> is minimal:</p>
</div>
<div class="imageblock text-indent">
<div class="content">
<img src="math-8dd4c22563ab7f9d.svg" alt="\frac{\mathrm{d}}{\mathrm{d}k}T_{\mathrm{lu}}\left(k\right) \;=\;
\cfrac{
  {\ln{N}\cfrac{2d_{\mathrm{ptr}}+d_{\mathrm{fld}}}{v}\ln{k}} \;-\;
  {\cfrac{\ln{N}\left(t_{\mathrm{ra}}+k\cfrac{2d_{\mathrm{ptr}}+d_{\mathrm{fld}}}{v}\right)}{k}}
  }{\ln^2{k}}
\;=\;0" height="90">
</div>
</div>
<div class="imageblock text-indent">
<div class="content">
<img src="math-dd15bb7175d8bbce.svg" alt="\Rightarrow
{k\frac{2d_{\mathrm{ptr}}+d_{\mathrm{fld}}}{v}\ln{k}} \;=\;
  {t_{\mathrm{ra}}+{k\frac{2d_{\mathrm{ptr}}+d_{\mathrm{fld}}}{v}}}" height="36">
</div>
</div>
<div class="imageblock text-indent">
<div class="content">
<img src="math-affd6ad84a8ccee4.svg" alt="\Rightarrow
k\left(\ln{k}-1\right) \;=\;
\underbrace{
  t_{\mathrm{ra}}\frac{v}{2d_{\mathrm{ptr}}+d_{\mathrm{fld}}}
  }_{\displaystyle{a}}" height="58">
</div>
</div>
<div class="imageblock text-indent">
<div class="content">
<img src="math-6e8c107e42e59335.svg" alt="\Rightarrow \ln{k} \;=\; 1+\frac{a}{k}" height="31">
</div>
</div>
<div class="imageblock text-indent">
<div class="content">
<img src="math-5ba500ed752f2f64.svg" alt="\Rightarrow
k \;=\; e^{1+\frac{a}{k}}" height="16">
</div>
</div>
<div class="imageblock text-indent">
<div class="content">
<img src="math-08014433a61e26a1.svg" alt="\Rightarrow
\frac{a}{e} \;=\; \frac{a}{k}e^{\frac{a}{k}}" height="31">
</div>
</div>
<div class="imageblock text-indent">
<div class="content">
<img src="math-dc30e7e2267e0f16.svg" alt="\Rightarrow
k \;=\; \frac{a}{\operatorname{LambertW}\displaystyle{\left(\frac{a}{e}\right)}}" height="49">
</div>
</div>
<div class="paragraph">
<p>Plugging in approximation for <img src="math-c0a4fc3b71cb8822.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.359ex;" alt="\operatorname{LambertW}(x)"/> function yields final solution:</p>
</div>
<div class="imageblock text-indent">
<div class="content">
<img src="math-619564f3bde21bd6.svg" alt="k_{\mathrm{opt}} \;\approx\; \frac{a}{\ln{\displaystyle{\frac{a}{e}}} \;-\; \ln{\ln{\displaystyle{\frac{a}{e}}}}}
\mathrm{,\quad where\; } a \;=\; t_{\mathrm{ra}}\frac{v}{2d_{\mathrm{ptr}}+d_{\mathrm{fld}}}" height="48">
</div>
</div>
<div class="paragraph">
<p>Notable fact is that optimal node size doesn&#8217;t depend on <img src="math-8ce86a6ae65d3692.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.0ex;" alt="N"/>&#8201;&#8212;&#8201;total number
of elements stored in index.
This is beneficial for developers because it is not uncommon that <img src="math-8ce86a6ae65d3692.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.0ex;" alt="N"/> is unknown in advance.
The only factors which must be taken in account are indexable field(s) size and charactersitics of
storage device.
Once optimal node size is computed, it will remain optimal for any <img src="math-8ce86a6ae65d3692.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.0ex;" alt="N"/>: no need to recompute
it and rebuild index even after dramatical increase or reduction in <img src="math-8b1663da576e3578.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.0ex;" alt="N."/>
(In fact, there is some implicit influence through <img src="math-1b88cc42001ad78c.svg" class="inlinemath" style="height:1.578ex;vertical-align:-0.313ex;" alt="t_{\mathrm{ra}}"/> if index
is located in HDD: larger <img src="math-8ce86a6ae65d3692.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.0ex;" alt="N"/> &#8658; larger index file size &#8658; longer avg seek &#8658;
longer <img src="math-1b88cc42001ad78c.svg" class="inlinemath" style="height:1.578ex;vertical-align:-0.313ex;" alt="t_{\mathrm{ra}}"/>).</p>
</div>
<div class="paragraph">
<p>Table below presents optimal values for storing B-tree index atop of 12-byte column,
pointer size is 8 byte long.
Because random access time differs by order of magnitude between HDD and SSD,
so does the optimal node size.
It is only two 4Kn sectors for SSD, but is an impressively large unit for HDD.
Also note that choosing SSD for storing B-tree indices provides its usual benefit: parallelism.
Multiple lookups may be issued simultaneously and will be completed in the nearly same amount of time
as it would take to perform single lookup, while HDD is strictly one lookup at a time device.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 60%;">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Device</th>
<th class="tableblock halign-right valign-top"><em>k</em><sub>opt</sub></th>
<th class="tableblock halign-right valign-top">Node size</th>
<th class="tableblock halign-right valign-top"><em>T</em><sub>lu</sub> (<em>N</em>=10<sup>9</sup>)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">HDD (5.0 ms, 150 MB/s)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">3839</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">107492 B</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">20.31 ms</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SSD (0.1 ms, 400 MB/s)</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">323</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">9044 B</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0.44 ms</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Keep in mind that there are a lot of subtleties affecting optimal node size:
device characterisitcs are not fixed values but only averages, tree height is not a real
number but an integer, top-level nodes most likely reside in cache, etc.
Values provided by above formula are only first-order theoretical estimations and must be
further tuned with live testing.</p>
</div>
</div>
<div class="sect3">
<h4 id="_sort_addresses_when_reading_in_bulk_hdd">Sort addresses when reading in bulk (HDD)</h4>
<div class="paragraph">
<p>Suppose that we need to read <img src="math-8ce86a6ae65d3692.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.0ex;" alt="N"/> records in bulk by their addresses.
Such task might be a part of standard dataflow in a database, when set of output records
is determined by one or more indices.
Consider following query: <code>SELECT * FROM goods WHERE price &gt;= 100 &amp;&amp; price &lt;= 500</code>.
Assuming that there is a B-tree index over <code>price</code> column, DBMS starts query execution
by looking into this index to retrieve list of record addresses matching given condition.
Now DBMS has to load actual records from main storage by these addresses.
In general case, loading starts even before all addresses are known.
For example, if addresses are slowly coming out one by one from a long-running join,
then it would be unfeasable to wait for the join to fully complete.
Loading may start as soon as address of first record becomes available.</p>
</div>
<div class="paragraph">
<p>Order in which main storage is accessed is the topic of particular intereset to us.
Incoming sequence of addresses has no particular order.
Two adjacent addresses may easily be located in opposite cylinders of a HDD.
If addresses are served in the same order as they are coming in, then each access to disk would
include random seek across 1/3 of file length on average.
And in worst case, when file is huge and low and high addressess are interleaved,
actuator arm has to constantly jump between inner and outer cylinders.
Seek portion of random access time may easily be improved with reordering.
Idea is to sort list of addresses, thus ensuring that disk will be accessed by addresses
in strictly ascending order.
Actuator arm will need to make only single radial pass in total in this case, from lowest to
highest address.
It&#8217;s like when you enter a supermarket with a long list of items you intend to buy and you know
their locations in advance, you subconciously sort this list to make your path as short as possible.
You don&#8217;t want to run all over the place for an hour with multiple returns to the same shelf.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-sort-reads-a.svg" alt="tip sort reads a" width="75%">
</div>
</div>
<div class="paragraph">
<p>Even if full sorting is not possible because not all addresses are immediately available,
some variation of partial ordering may be implemented.
For example, someone may process address groups of fixed length by passing them through
fixed-length buffer.
This is not so perfect as full sort, but anyway significantly reduces cumulative seek length.
Such technique is also beneficial to other methods because it allows to easily maintain sorting
constraints in cases when records must be returned in the same order as addresses are coming in.
If, for example, query is <code>SELECT * FROM goods WHERE price &gt;= 100 &amp;&amp; price &lt;= 500 SORT DESC</code>, then
DBMS would read B-tree index over <code>price</code> column in reverse, and addresses of matching records
would come out already in desired order.
Algorithm of partial sorting becomes like this:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>allocate buffer of fixed size</p>
</li>
<li>
<p>repeat:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>read addresses one by one into buffer until it is full</p>
</li>
<li>
<p>sort them but remember their original order</p>
</li>
<li>
<p>load records into memory</p>
</li>
<li>
<p>sort records according to the original order</p>
</li>
<li>
<p>output the records</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Further improvement is possible by observing the fact that reading addresses in descending order
is also as good as reading in ascending order.
As such, odd groups of addresses should be sorted ascendingly and even groups&#8201;&#8212;&#8201;descendingly.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-sort-reads-b.svg" alt="tip sort reads b" width="75%">
</div>
</div>
<div class="paragraph">
<p>Graph below demonstrates how running time of test program depends on reorder buffer size.
Test program consists of 1000 reads scattered uniformly along 70 GB file.
Horizontal axis is buffer size in number of entries, vertical axis&#8201;&#8212;&#8201;total running time
in seconds.
Buffer size of 1 is degenerate case when no actual reordering was done.
Buffer size of 1024 is when all of reads were loaded into buffer and reordered at once.
As you may see, even with moderate buffer size of 32 overall running time is reduced by 1/3.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-sort-reads.svg" alt="tip sort reads" width="80%">
</div>
</div>
<div class="paragraph">
<p>Please note that read reordering makes sense only for reads and synchronized writes.
Manual reordering of ordinal (non-synchronized) writes is of no use because it is already
implemented in scheduler.
Without having to guarantee that data is written before syscall returns, scheduler
is free to postpone actual write with purpose of reordering it with further requests.
Manual reordering also is of no use for solid state drives because they do not need to be seeked.
Their performance is good enough even without any external optimizations.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_ad_hoc_tasks">Ad-hoc tasks</h3>
<div class="sect3">
<h4 id="_how_to_identify_drive_model">How to identify drive model</h4>
<div class="paragraph">
<p>Quite often you need to know model and characteristics (HDD/SSD, sector sizes and the like) of installed drives.
If you have root access then this is easy: you may use <code>hdparm</code> or <code>smartctl</code>, both are able to request full identifying
information.
Non-priviliged users do not have access to such information, but still, there are ways to retrieve model string and then
to search for drive&#8217;s datasheet on the web.
All common methods and their limitations are listed below.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">hdparm</dt>
<dd>
<p>Root access required: <strong>yes</strong>.
<code>hdparm</code> is the most verbose tool.
It is able to request all information directly from drives with option <code>-I</code>.
This information includes model name, sector sizes, supported features, security status and the like&#8201;&#8212;&#8201;everything you may want to know about the drive.
Do not confuse options: <code>-i</code> will print only basic information, which is retrieved from kernel, rather than information
from drive directly.</p>
<div class="listingblock">
<div class="content">
<pre># /sbin/hdparm -I /dev/sda
...
Model Number:       ST9250410AS
...
CHS current addressable sectors:   16514064
LBA    user addressable sectors:  268435455
LBA48  user addressable sectors:  488397168
Logical/Physical Sector size:           512 bytes
device size with M = 1024*1024:      238475 MBytes
device size with M = 1000*1000:      250059 MBytes (250 GB)
cache/buffer size  = 16384 KBytes
Nominal Media Rotation Rate: 7200
...</pre>
</div>
</div>
</dd>
<dt class="hdlist1">smartctl</dt>
<dd>
<p>Root access required: <strong>yes</strong>.
<code>smartctl</code> comes as part of <code>smartmontools</code> package and is primarily designed to deal with SMART.
Its output is much limited to that of <code>hdparm</code>, but good thing is that it comes with internal database of drives.
Not only it prints model, but also it is able to decipher model string into vendor and family.</p>
<div class="listingblock">
<div class="content">
<pre># /usr/sbin/smartctl -i /dev/sda
...
Model Family:     Seagate Momentus 7200.4
Device Model:     ST9250410AS
Serial Number:    5VG6N3SS
User Capacity:    250,059,350,016 bytes [250 GB]
Sector Size:      512 bytes logical/physical
...</pre>
</div>
</div>
</dd>
<dt class="hdlist1">/dev/disk/by-id/</dt>
<dd>
<p>Root access required: <strong>no</strong>.
Block devices are listed in <code>/dev/disk</code> subdirectories multiple times by using different ways of classification.
In particular, directory <code>/dev/disk/by-id</code> lists drives by their IDs, which are created
by concatenating subsystem name and model string.
Note that this approach will get you only model string and nothing more&#8201;&#8212;&#8201;all other characteristics
must be searched on the web.</p>
<div class="listingblock">
<div class="content">
<pre>$ ls -l /dev/disk/by-id/
...
ata-ST9250410AS_5VG6N3SS -&gt; ../../sda
...</pre>
</div>
</div>
<div class="paragraph">
<p>Here, second part of drive id&#8201;&#8212;&#8201;<code>ST9250410AS</code>&#8201;&#8212;&#8201;is model string.
Searching this string on the web immediately reveals drive&#8217;s model name (<code>Seagate Momentus 7200.4</code>) and link to datasheet.</p>
</div>
</dd>
<dt class="hdlist1">dmesg</dt>
<dd>
<p>Root access required: <strong>no</strong>.
Search through <code>dmesg</code> output for strings like <code>ata</code> and <code>scsi</code>: drive model name appears in system messages during boot.</p>
<div class="listingblock">
<div class="content">
<pre>...
[    1.868816] ata3.00: ATA-8: ST9250410AS, D005SDM1, max UDMA/133
...
[    1.884623] scsi 2:0:0:0: Direct-Access     ATA      ST9250410AS      D005 PQ: 0 ANSI: 5
...</pre>
</div>
</div>
</dd>
<dt class="hdlist1">sysfs</dt>
<dd>
<p>Root access required: <strong>no</strong>.
Essential drive characteristics may be queried through sysfs.
First two commands return logical and physical sector sizes respectively.
Third command returns number of requests which may be issued simultaneously.
This is effective value computed as a minimum of corresponding values of drive itself and connecting interface.
For example, connecting drive through USB 2.0 external drive enclosure would reduce this number to 1 no matter how many
parallel requests disk drive supports itself.</p>
<div class="listingblock">
<div class="content">
<pre>$ cat /sys/class/block/sda/queue/logical_block_size
512
$ cat /sys/class/block/sda/queue/physical_block_size
512
$ cat /sys/class/block/sda/device/queue_depth
31</pre>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="_use_hard_links_to_modify_large_directory_structures">Use hard links to modify large directory structures</h4>
<div class="paragraph">
<p>Hard links are rarely used explicitly.
In practice, though, they are handy in making dangerous modifications to large datasets.
Suppose you have 3-level file storage with typical file location like <code>/storage/d/a/daf7e1a87eb8.jpg</code>.
Because of large number of files, you decide to change it to 4 levels.
E.g, previous location should be changed to <code>/storage_new/d/a/f/daf7e1a87eb8.jpg</code>.</p>
</div>
<div class="paragraph">
<p>Direct moving of files is one of ways to achieve this but is rather dangerous.
If you don&#8217;t want to lose files or mess with directory hierarchy by mistake (this may be particularly expensive in
live system), you&#8217;d better do thorough and time-consuming testing using artificial data set first.
Alternative way is to make full copy, but this requires plenty of free space and hours of IO time.</p>
</div>
<div class="paragraph">
<p>A better solution exists.
Instead of copying files, create <em>hard links</em> to existing files.
This is fast because no actual contents is copied and also is safe because original files are not modified.
After copy is made and new storage layout is verified, old storage may be safely removed with <code>rm -Rf /storage</code>.
Once again, this operation completes in no time.
Additionally, having files in both old and new formats at the same time may simplify backward compatability in software.</p>
</div>
</div>
<div class="sect3">
<h4 id="when-file-content-is-deleted">Understand when file content is deleted</h4>
<div class="paragraph">
<p>One common complaint from *nix users is that &#8220;removing files doesn&#8217;t release disk space&#8221;.
Such complaint usually comes after trying to delete very large log file.
Suppose that at some point, system monitoring says that disk space hit 100%, you log in to server and find out that
your application managed to create multi-hundered-gigabyte log file.
You remove this log file expecting that it will release disk space but this doesn&#8217;t actually happen.
What&#8217;s going on?</p>
</div>
<div class="paragraph">
<p>Source of the problem is poor understanding of *nix filesystem concepts.
In order for filesystem to physically remove file (declare its blocks as free), <em>both</em> of the following conditions must be met:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>There are no hard links (= file names) left referring to file contents</p>
</li>
<li>
<p>There are no open file descriptors left referring to this file</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>As such, by deleting file name you satisified (1) but not (2).
To truly delete file contents, you also need to restart application or to notify it somehow to reopen log file.
In case of standard system daemons, latter is typically accomplished by sending <code>SIGHUP</code> signal.
It is advisable to design new applications in the same manner: reopen log file with <code>O_WRONLY|O_APPEND|O_CREAT</code> on receiving
<code>SIGHUP</code> signal.</p>
</div>
</div>
<div class="sect3">
<h4 id="_recover_deleted_but_opened_file">Recover deleted but opened file</h4>
<div class="paragraph">
<p>Deletion conditions from previous section may be life-saving sometimes.
If you have mistakenly deleted file which is still held open by some process, you may easily recover it.
Locate deleted file by analyzing contents of <code>ls -l /src/&lt;pid&gt;/fd/</code> where &lt;pid&gt; is PID of process that holds file descriptor
to deleted file.
Recover file by copying it from <code>/src/&lt;pid&gt;/fd/&lt;fd&gt;</code> to the destination of your choice.</p>
</div>
<div class="paragraph">
<p>If you was unfortunate and terminated application before recovery, then file contents is removed by filesystem.
Only low-level filesystem recovery tool may help you in this case (or may not).</p>
</div>
</div>
<div class="sect3">
<h4 id="_prefer_code_tail_f_code_to_code_tail_f_code">Prefer <code>tail -F</code> to <code>tail -f</code></h4>
<div class="paragraph">
<p><code>tail -f</code> is often used to read log files in realtime.
Suppose you run a webserver.
One evening you detect that there is malicious activity going on from single IP address.
You decide to extract all further requests which will be sent from this particular IP address
into separate file with purpose to analyze it the next day in the morning.
In order to so, you decide to use <code>tail</code> with option <code>-f</code>.
Instead of just printing 10 last lines of file, it makes tail to <em>follow</em> file as new data arrives.
Thus, you expect that all the data since moment tail is started will be fed into <code>grep</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># tail -f /var/log/webserver.log | grep '46.17.98.22' &gt; evidence.log</pre>
</div>
</div>
<div class="paragraph">
<p>Problem here is that when logrotate script comes, it will create new empty <code>webserver.log</code> file.
But <code>tail</code>, being not aware of log rotating, will still be waiting for data to arrive
to old version of <code>webserver.log</code>.
When you return in the morning, <code>evidence.log</code> will lack most of expected records.
In the worst case, if logs are rotated on hourly basis and only couple of recent files
are preserved, there even will be nowhere to recover desired data from.</p>
</div>
<div class="paragraph">
<p>Another, even more severe problem, is that you may potentially run out of disk space.
Consider following configuration: 20 GB partition is used exclusively for storing log files
(for example, because you blindly followed advice on <a href="#allocate-separate-partition-hdd">allocating separate partition</a>),
single log file occupies about 8 GB, and logratate is configured to store only two log files.
Such configuration is fine with only two files (16 GB total), but fails when you start <code>tail -f</code> and keep it
running for prolonged amount of time.
Because <code>tail -f</code> still holds file in opened state, filesystem is not
<a href="#when-file-content-is-deleted">allowed</a> to delete file&#8217;s content until <code>tail</code> is terminated.
As such, you need enough space to store 3 files simultaneously: two "normals" and one "shadow".
Because <img src="math-aed279e9eb8396b6.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="{{3}\times{8\,\mathrm{GB}}} \gtr {20\,\mathrm{GB}},"/>
you will run out of space.</p>
</div>
<div class="paragraph">
<p>Solution to both problems is to use &#8220;-F&#8221; option.
Instead of holding file opened forever, this option will make tail to try to reopen file
by its name if original file is gone even for a brief moment of time,
which is exactly the case with how log rotating is done:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># mv webserver.log webserver.log.1 &amp;&amp; touch webserver.log &amp;&amp; kill -HUP 42424</pre>
</div>
</div>
<div class="paragraph">
<p>Corresponding output of <code>tail -F</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>46.17.98.22 - - [05/Sep/2016:14:48:35 +0200] "GET /script HTTP/1.1" 404 168 "-" "Python-urllib/2.7"
46.17.98.22 - - [05/Sep/2016:14:48:35 +0200] "GET /jenkins/script HTTP/1.1" 301 184 "-" "Python-urllib/2.7"
tail: ‘/var/log/webserver.log’ has become inaccessible: No such file or directory
tail: ‘/var/log/webserver.log’ has appeared;  following end of new file
46.17.98.22 - - [05/Sep/2016:14:48:36 +0200] "GET /login HTTP/1.1" 301 184 "-" "Python-urllib/2.7"
46.17.98.22 - - [05/Sep/2016:14:48:46 +0200] "GET /jmx-console HTTP/1.1" 301 184 "-" "Python-urllib/2.7"</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_do_not_underestimate_copy_time">Do not underestimate copy time</h4>
<div class="paragraph">
<p>People usually make unrealistical assumptions on how long it would take to make full copy of disk drive
(this is true for both HDDs and SSDs).
Full copies are commonly produced during backup procedures or during RAID reconstruction.
Table below lists estimated time to make full copy depending on drive capacity and max sustained read/write speed.
Average speed is assumed to be 1.3 times lower than max sustained speed.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 80%;">
<colgroup>
<col style="width: 12.5%;">
<col style="width: 12.5%;">
<col style="width: 12.5%;">
<col style="width: 12.5%;">
<col style="width: 12.5%;">
<col style="width: 12.5%;">
<col style="width: 12.5%;">
<col style="width: 12.5%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-center valign-top">max sust</th>
<th class="tableblock halign-center valign-top">avg sust</th>
<th class="tableblock halign-center valign-top">1 TB</th>
<th class="tableblock halign-center valign-top">2 TB</th>
<th class="tableblock halign-center valign-top">4 TB</th>
<th class="tableblock halign-center valign-top">6 TB</th>
<th class="tableblock halign-center valign-top">8 TB</th>
<th class="tableblock halign-center valign-top">10 TB</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">50 MB/s</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">38 MB/s</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">7h:18m</p></td>
<td class="tableblock halign-center valign-top"></td>
<td class="tableblock halign-center valign-top"></td>
<td class="tableblock halign-center valign-top"></td>
<td class="tableblock halign-center valign-top"></td>
<td class="tableblock halign-center valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">100 MB/s</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">76 MB/s</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">3h:39m</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">7h:18m</p></td>
<td class="tableblock halign-center valign-top"></td>
<td class="tableblock halign-center valign-top"></td>
<td class="tableblock halign-center valign-top"></td>
<td class="tableblock halign-center valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">200 MB/s</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">153 MB/s</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">1h:48m</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">3h:37m</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">7h:15m</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">10h:53m</p></td>
<td class="tableblock halign-center valign-top"></td>
<td class="tableblock halign-center valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">400 MB/s</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">307 MB/s</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0h:54m</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">1h:48m</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">3h:37m</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">5h:25m</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">7h:14m</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">9h:02m</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Takes too long, doesn&#8217;t it?
More than that, values above are achieved only under perfect conditions: source and destination drives
are separate entities and low-level copy is made
(copying raw device with <code>dd</code> rather than copying files separately with <code>cp</code>).
If source and destination are located in the same drive (different partitions or different files),
then it would take twice as much time.
And if per-file copy is made (<code>cp</code>) instead of low-level copy (<code>dd</code>), and filesystem contains predominantly
small files, then this may take the eternity to complete.</p>
</div>
</div>
<div class="sect3">
<h4 id="_dealing_with_text_file_busy_error">Dealing with &#8220;text file busy&#8221; error</h4>
<div class="paragraph">
<p>If you try to copy (<code>cp</code>) file over another one which is a running executable, you will get <code>ETXTBSY</code> error (&#8220;Text file busy&#8221;).
Most often such error occurs during deployment of server software, when new version of binary replaces current version, which is
still running.
Source of such error is that linux prohibits writes to running executables.
This makes sense because contents of executable is mmap()-ped into RAM.
Thus, by copying you modify executable on the fly, which definitely won&#8217;t lead to anything good.</p>
</div>
<div class="paragraph">
<p>In order to avoid this problem, you need to either stop running application before copy is made or to use move command (<code>mv</code>)
instead of copy (<code>cp</code>).
Latter creates brand new file (with another inode), while old file contents still continues to exist in filesystem until
all applications which hold it open are closed (see section &#8220;Understand when file contents is deleted&#8221;).</p>
</div>
</div>
<div class="sect3">
<h4 id="_create_extent_files_instantly_with_code_fallocate_code">Create extent files instantly with <code>fallocate</code></h4>
<div class="paragraph">
<p>Sometimes you need to create extent files of fixed lengths.
Canonical way is to use <code>dd(1)</code> to copy desired number of bytes from <code>/dev/zero</code> or <code>/dev/urandom</code> but this is too slow.
Luckily, modern filesystems (ext4, XFS) support allocating of file data blocks without actually filling them with data.
Allocating is done from command shell with <code>fallocate(1)</code> or programmatically with <code>fallocate(2)</code>.
In example below, 10 GB file is created:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ fallocate -l $((10*1000*1000*1000)) extent.0
$ du -hs extent.0
9.4G    extent.0</pre>
</div>
</div>
<div class="paragraph">
<p>This works instantly and resulting file is just an ordinary plain file without any strangenesses.</p>
</div>
<div class="paragraph">
<p>Note that another utility&#8201;&#8212;&#8201;<code>truncate(1)</code>&#8201;&#8212;&#8201;is not the same.
The only thing that it does is extending or shrinking file length attribute without actually allocating blocks.
As such, you may even &#8220;create&#8221; file with length exceeding available free space (&#8220;sparse file&#8221;).
Trying to fill such file with data will result in no free space error at some point.
Most likely this is <em>not</em> you want.</p>
</div>
<div class="paragraph">
<p>Another word of warning is that <code>fallocate</code> is not suitable for reducing number of free pages in brand new or extensively
TRIMmed SSDs before running performance tests of drives themselves or of applications.
No actual data is sent to device and, hence, no flash memory pages become occupied.
The only way to achieve this is to fill SSD with some random data, for example from <code>/dev/urandom</code>
(<code>/dev/zero</code> is not suitable either because SSDs may compress data on the fly).</p>
</div>
<div class="paragraph">
<p>Generalizing, above commands may be ordered into four levels:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><code>truncate</code>&#8201;&#8212;&#8201;modifies file length only (instant)</p>
</li>
<li>
<p><code>fallocate</code>&#8201;&#8212;&#8201;modifies file length and reserves data blocks but doesn&#8217;t fill them with data (instant)</p>
</li>
<li>
<p>write from <code>/dev/zero</code>&#8201;&#8212;&#8201;modifes file length, reserves blocks and fills them with zeroes but doesn&#8217;t necessarily
occupy expected number of SSD pages because zeroes are highly compressible (slow)</p>
</li>
<li>
<p>write from <code>/dev/urandom</code>&#8201;&#8212;&#8201;modifes file length, reserves blocks and writes supplied data (very slow)</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_use_progress_meters">Use progress meters</h4>
<div class="paragraph">
<p>Because filesystem commands may take long time to complete, it is always advisable to track their progress.
Almost all command line tools dealing with file operations provide way to display progress meters.
Even if tool in use doesn&#8217;t provide such functionality or if you forgot to specify progress option, linux exposures basic
per-file progress through <code>/proc</code>.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">rsync</dt>
<dd>
<p><code>rsync</code> has <code>--progress</code> option specifically intended for progress monitoring:</p>
<div class="listingblock">
<div class="content">
<pre>$ rsync -av --progress geodb-bak.tar.gz /backups/
sending incremental file list
geodb-bak.tar.gz
    388,071,424  42%  370.06MB/s    0:00:15</pre>
</div>
</div>
</dd>
<dt class="hdlist1">dd</dt>
<dd>
<p><code>dd</code> doesn&#8217;t have any progress option.
But it outputs current progress on receiving SIGUSR1 signal.
Unfortunately, there is no simple way to bind key combination to SIGUSR1, so the only reasonable way is to send signal
from another tty (or, alternatively, by putting <code>dd</code> to background):</p>
<div class="listingblock">
<div class="content">
<pre>kill -SIGUSR1 &lt;pid&gt;</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ dd if=geodb-bak.tar.gz of=geodb-bak-current.tar.gz
13517137+0 records in
13517137+0 records out
13517137 bytes (14 MB) copied, 15.1775 s, 891 kB/s</pre>
</div>
</div>
</dd>
<dt class="hdlist1">cp</dt>
<dd>
<p><code>cp</code> doesn&#8217;t have progress meter option either.
On BSD systems <code>cp</code> outputs its progress on receiving SIGINFO.
Sending this signal is mapped to &lt;Ctrl&gt;+&lt;T&gt; by default.</p>
</dd>
<dt class="hdlist1">pv</dt>
<dd>
<p><code>pv</code> utility may be handy when writing interactive bash/grep/sed/awk one-liners to process large volumes of data.
It is distributed as standalone <code>pv</code> package.
<code>pv</code> works either as a replacement for <code>cat</code> (e.g. <code>pv access.log | ...</code>)
or as a pipe connector (<code>cat access.log | pv | grep '/index.html' | wc -l</code>).
It displays progress meter in both ways.
First approach is preferrable because it shows ETA, while pipe connector variant is not able to do so for obvious reasons.</p>
<div class="listingblock">
<div class="content">
<pre>$ cat ~/geodb.tar.gz | pv | wc -l
1.06GiB 0:00:07 [ 340MiB/s] [    &lt;=&gt;                                                          ]</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ pv ~/geodb.tar.gz | wc -l
1.31GiB 0:00:01 [ 831MiB/s] [==============================&gt;                  ] 62% ETA 0:00:01</pre>
</div>
</div>
</dd>
<dt class="hdlist1">/proc/&lt;pid&gt;/fdinfo</dt>
<dd>
<p>Linux exposures basic IO statistics for each given file descriptor through <code>/proc/&lt;pid&gt;/fdinfo</code>.
Suppose you have a process that sequentially reads <code>/var/logs/my.log</code> but doesn&#8217;t show any progress.
It is already being running for some time and you want to know how much data it has processed so far.
Steps to perform:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Find out PID of process under investigation</p>
</li>
<li>
<p>Find out file descriptor of required file with <code>ls -l /proc/&lt;pid&gt;/fd</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ ls -l /proc/11872/fd
total 0
lr-x------ 1 user user 64 Jul 16 01:44 0 -&gt; /var/logs/my.log
lrwx------ 1 user user 64 Jul 16 01:44 1 -&gt; /dev/pts/5
lrwx------ 1 user user 64 Jul 16 01:44 2 -&gt; /dev/pts/5</pre>
</div>
</div>
</li>
<li>
<p>Print contents of <code>/proc/&lt;pid&gt;/fdinfo/&lt;fd&gt;</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat /proc/11872/fdinfo/0
pos:    14578072
flags:  0100000
mnt_id: 19</pre>
</div>
</div>
<div class="paragraph">
<p><code>pos</code> field gives current offset in given file.
Of course, this works only if process uses basic IO syscalls <code>read(2)</code> and <code>write(2)</code>.
If it uses syscalls which act on file position in non-trivial way (<code>pread(2)</code>, <code>lseek(2)</code>) then this method is of no use.</p>
</div>
</li>
</ol>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="_how_to_resize_ext4">How to resize ext4</h4>
<div class="paragraph">
<p>This section describes how to resize filesystem manually without using high-level tools.
Only <code>fdisk</code> and ext4 utilities are required.
Two operations are described: how to expand FS to the right (simple) and how to shrink FS to the left (a bit trickier).</p>
</div>
<div class="paragraph">
<div class="title">Expanding filesystem to the right</div>
<p>Suppose that you have free space (probably by previously deleting some partition) and now you want to expand existing
partition <code>sda2</code> with existing filesystem in it:</p>
</div>
<div class="imageblock" style="text-align: left">
<div class="content">
<img src="tip-ext4-enlarge.svg" alt="tip ext4 enlarge" width="50%">
</div>
</div>
<div class="paragraph">
<p>Steps to perform:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Unmount <code>/dev/sda2</code></p>
</li>
<li>
<p>Modify partition table using <code>fdisk</code>:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Remember start position of partition #2</p>
</li>
<li>
<p>Delete partition #2</p>
</li>
<li>
<p>Create partition #2 with start position as in 2.a and end position as desired</p>
</li>
<li>
<p>(Don&#8217;t forget to set boot flag if it is required)</p>
</li>
<li>
<p>Save partition table</p>
</li>
</ol>
</div>
</li>
<li>
<p>Call <code>resize2fs</code> without size argument.
It will detect that capacity of underlying partition is larger than FS uses and will automatically expand FS to occupy
whole partition.</p>
<div class="listingblock">
<div class="content">
<pre># resize2fs /dev/sda2
resize2fs 1.42.5 (29-Jul-2012)
Resizing the filesystem on /dev/sda2 to 39072470 (4k) blocks.
The filesystem on /dev/sda2 is now 39072470 blocks long.</pre>
</div>
</div>
</li>
<li>
<p>Mount <code>/dev/sda2</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Shrinking filesystem to the left</div>
<p>Shrinking is a bit more complex&#8201;&#8212;&#8201;some math must be performed to compute proper partition size.
Furthermore, different tools report numeric values using different units.</p>
</div>
<div class="imageblock" style="text-align: left">
<div class="content">
<img src="tip-ext4-shrink.svg" alt="tip ext4 shrink" width="50%">
</div>
</div>
<div class="paragraph">
<p>Steps to perform:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Unmount <code>/dev/sda2</code></p>
</li>
<li>
<p>Call <code>tune2fs</code> to detect current block count FS uses.
Also remember FS block size:</p>
<div class="listingblock">
<div class="content">
<pre># tunefs -l /dev/sda2
...
Block count:        39072470
...
Block size:         4096
...</pre>
</div>
</div>
</li>
<li>
<p>Call <code>resize2fs</code> with new (reduced) block count size:</p>
<div class="listingblock">
<div class="content">
<pre># resize2fs /dev/sda2 25000000
resize2fs 1.42.5 (29-Jul-2012)
Resizing the filesystem on /dev/sda2 to 25000000 (4k) blocks.
The filesystem on /dev/sda2 is now 25000000 blocks long.</pre>
</div>
</div>
</li>
<li>
<p>Now modify partition table using <code>fdisk</code>.
According to previous steps, we need partition of at least <code>25,000,000 × 4096</code> bytes.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Remember Start of partition #2</p>
</li>
<li>
<p>Delete partition #2</p>
</li>
<li>
<p>Create partition #2 with Start position as in 4.a and such End position that Blocks is enough to accommodate FS.
Note that Start and End are both inclusive.
Aslo note that Blocks count, Start/End positions and tune2fs block size are all given in different units
(e.g. Blocks are 1k long, Start/End are 512 bytes long and tune2fs blocks are 4k long).</p>
</li>
<li>
<p>(Don&#8217;t forget to set boot flag if it is required)</p>
</li>
<li>
<p>Save partition table</p>
</li>
</ol>
</div>
</li>
<li>
<p>Mount /dev/sda2</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_invalidate_page_cache_before_running_performance_tests">Invalidate page cache before running performance tests</h4>
<div class="paragraph">
<p>Invalidating page cache is crucial step when testing performance of
storage-based applications by using limited amount of requests.
Forgetting to invalidate page cache may lead to unrealistically good performance that will diminish in production environment.
Consider that you need to test database application that is supposed to handle 1 TB local storage and
system has only 128 GiB of RAM available for page cache.
Most likely, your test requests do not cover full range of 1 TB.
Even more likely, covered range fits in under 128 GiB.
This will result in that only first test run will actually issue requests to storage, and all further test runs will use
data from page cache.
Thus, you will see unrealistically good results.
After deploying to production environment, when live requests appear, their range won&#8217;t fit into page cache anymore and
performance will be much lower than observed during testing.</p>
</div>
<div class="paragraph">
<p>In order to invalidate cache, you need to write value <code>3</code> into <code>/proc/sys/vm/drop_caches</code> (root privileges required):</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sudo bash -c "echo 3 &gt; /proc/sys/vm/drop_caches"</pre>
</div>
</div>
<div class="paragraph">
<p>Writing other values (<code>1</code> or <code>2</code>) invalidates objects of only specified type&#8201;&#8212;&#8201;see <code>proc(5)</code> for detailed information.
Note that invalidating cache is global operation&#8201;&#8212;&#8201;you can&#8217;t invalidate page cache for only some block device or a file.
Also note that this doesn&#8217;t invalidate dirty (modified) pages.</p>
</div>
<div class="paragraph">
<p>By using invalidate cache functionality, each single test run becomes like this:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>invalidate page cache</p>
</li>
<li>
<p>(optional) run some fraction of requests in order to &#8220;preheat&#8221; cache to achieve production-like cache hit rate</p>
</li>
<li>
<p>run actual performance test</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_monitoring_io_kernel_interface">Monitoring IO: kernel interface</h4>
<div class="paragraph">
<p>After demonstrating all the subtleties involved in working with storage subsystem, it is obvious that we need means of
monitoring IO activity.
Linux kernel provides a variety of virtual files which may be used to monitor different performance characteristics.
These files may be parsed manually (for example, to feed data into monitoring service) or accessed in human-readable format
with command line utilities such as <code>iostat</code>.
Even if you choose to use latter approach, it is advisable to understand meaning of metrics provided with these files
because their semantics is not always obvious.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">/proc/diskstats</dt>
<dd>
<p>Cumulative system-wide IO statistics is published by Linux kernel into <code>/proc/diskstats</code>.
Each line in this file corresponds to single block device and is easily parseable with awk or similar tool.
This file provides per block device information only&#8201;&#8212;&#8201;there is no way to get per-process distribution from it.
As such, this file is best used for continuous monitoring of hardware usage.
Typical output and its brief explanation are presented below.
Full description of all fields is documented in linux kernel in <code>Documentation/iostats.txt</code>.
The same statistics may also be obtained by reading per-device files <code>/sys/class/block/&lt;dev&gt;/stat</code>.</p>
<div class="listingblock">
<div class="content">
<pre>   8       0 sda 16661 508173 4198672 121760 0 0 0 0 0 62624 121760
   8      16 sdb 4855 7363 303789 79652 380 394 7225 18844 0 36208 98496</pre>
</div>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1428%;">
<col style="width: 7.1436%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-right valign-top">&nbsp;</th>
<th class="tableblock halign-right valign-top">&nbsp;</th>
<th class="tableblock halign-left valign-top">dev</th>
<th class="tableblock halign-center valign-top">reads, done</th>
<th class="tableblock halign-center valign-top">reads, merges</th>
<th class="tableblock halign-center valign-top">reads, sectors</th>
<th class="tableblock halign-center valign-top">reads, millis</th>
<th class="tableblock halign-center valign-top">writes, done</th>
<th class="tableblock halign-right valign-top">writes, merges</th>
<th class="tableblock halign-center valign-top">writes, sectors</th>
<th class="tableblock halign-center valign-top">writes, millis</th>
<th class="tableblock halign-center valign-top">ops in progress</th>
<th class="tableblock halign-center valign-top">io, millis</th>
<th class="tableblock halign-center valign-top">io, weighted, millis</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">8</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">sda</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">16661</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">508173</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4198672</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">121760</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">62624</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">121760</p></td>
</tr>
<tr>
<td class="tableblock halign-right valign-top"><p class="tableblock">8</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">16</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">sdb</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">4855</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">7363</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">303789</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">79652</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">380</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">394</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">7225</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">18844</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">0</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">36208</p></td>
<td class="tableblock halign-right valign-top"><p class="tableblock">98496</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p><strong>Reads, done</strong> and <strong>writes, done</strong> count physical requests to block device.
Note that large sequential requests from user applications may be split into smaller physical requests because interfaces
have limit on number of sectors carried in single physical request.
Opposite is also true: if application issues sequence of small writes to adjacent sectors, then write combining may happen
inside IO scheduler, and all these requests will be issued as single large physical request.
Thus, these fields are not to be used to count number of logical requests.
In particular, these fields do not say anything about sequential vs random pattern: value may be equally high for both patterns.
Average size per physical request is a much better metric:
<img src="math-72cb6a43a9978cbe.svg" class="inlinemath" style="height:4.141ex;vertical-align:-1.391ex;" alt="\frac{{\mathrm{SectorsRead}}\times{\mathrm{SectorSize}}}{\mathrm{ReadsDone}}"/>.
If this value is order of hundreds of kilobytes, then IO is purely sequential.</p>
</div>
<div class="paragraph">
<p><strong>Reads, merges</strong> and <strong>writes, merges</strong> fields are not very useful.
They refer to situation when Linux observes two separate requests to ajacent sectors and merges these requests together
into single physical IO request.
If sequential access is used, this value will be large and it is OK.
This is because of block IO architecture: userspace request is split and passed to IO scheduler, which combines them again.</p>
</div>
<div class="paragraph">
<p><strong>Reads, sectors</strong> and <strong>writes, sectors</strong> may be used to compute number of bytes read or written.
Note that because read-modify-write, logical write operation may trigger physical <em>read</em>.
As such, don&#8217;t be frustrated if you see that high-level tools such as <code>iostat</code> report number of bytes read plus written
much higher that you anticipated.</p>
</div>
<div class="paragraph">
<p><strong>Reads, millis</strong> and <strong>writes, millis</strong> count total number of milliseconds it took for IO operations
of respective type to complete.
These values are computed as follows: for each operation, its time is logged and is added to this counter.
Because IO requests may be sent to device in parallel, these counters may be much larger than wall time.
Contrary, <strong>io, millis</strong> counts wall time device was busy.
You may want to use former or latter depending on what you want to measure.</p>
</div>
</dd>
<dt class="hdlist1">/proc/&lt;pid&gt;/io</dt>
<dd>
<p>Cumulative per-process and per-thread IO statistics is available using <code>/proc/&lt;pid&gt;/io</code> and <code>/proc/&lt;pid&gt;/task/&lt;tid&gt;/io</code> files.
<code>rchar</code> and <code>wchar</code> count number of bytes application requested to read and write by various IO syscalls,
while <code>read_bytes</code> and <code>write_bytes</code> are number of bytes <em>physically</em> requested to/from storage device.
Ratio of <code>rchar</code> to <code>read_bytes</code> may be used to monitor cache efficiency.</p>
<div class="listingblock">
<div class="content">
<pre>rchar: 4452
wchar: 2013266335
syscr: 9
syscw: 13
read_bytes: 65536
write_bytes: 2013286400
cancelled_write_bytes: 0</pre>
</div>
</div>
</dd>
<dt class="hdlist1">/proc/&lt;pid&gt;/fdinfo/&lt;fd&gt;</dt>
<dd>
<p>There is no means of getting per-file statistics.
Nearest thing you can do is to look into <code>/proc/&lt;pid&gt;/fdinfo/&lt;fd&gt;</code> files&#8201;&#8212;&#8201;they contain <code>pos</code> field, which is current
file position.
It may be used as ad-hoc method to figure out progress of IO-heavy applications which read or write files sequentially by
using basic <code>read()</code> or <code>write()</code> syscalls.
For example, if application is reading and processing some large file on fly, then you may compare <code>pos</code> field with
actual file length to compute ETA.
Otherwise, if application uses more advanced syscalls such as <code>pread()</code>, <code>pwrite()</code> or modifies file position explicitly with
<code>lseek()</code>, then these files are useless.</p>
<div class="listingblock">
<div class="content">
<pre>$ cat /proc/6114/fdinfo/21
pos:    5787796
flags:  02100000
mnt_id: 19</pre>
</div>
</div>
</dd>
<dt class="hdlist1">ftrace</dt>
<dd>
<p>Separate physical requests to block device may be monitored by using Linux function tracer (ftrace).
This method requires root privileges.
You will need to mount debugfs if it is not mounted yet and then to enable <code>block_rq_issue</code> and <code>block_rq_complete</code> events:</p>
<div class="listingblock">
<div class="content">
<pre># mount -t debugfs none /sys/kernel/debug
# echo 1 &gt; /sys/kernel/debug/tracing/events/block/block_rq_issue/enable
# echo 1 &gt; /sys/kernel/debug/tracing/events/block/block_rq_complete/enable</pre>
</div>
</div>
<div class="paragraph">
<p>Now each time physical request is sent or is completed, trace file will contain single line describing request.
<code>8,0</code> in below example are major/minor block device numbers, <code>WS</code> is operation flags (W&#8201;&#8212;&#8201;write, R&#8201;&#8212;&#8201;read, F&#8201;&#8212;&#8201;flush,
M&#8201;&#8212;&#8201;metadata), next goes sector address plus number of sectors.
Last field is application name that triggered this event.</p>
</div>
<div class="listingblock">
<div class="content">
<pre># cat /sys/kernel/debug/tracing/trace
...
   drvperf-13634 [003] d... 34630.349502: block_rq_issue: 8,0 WS 0 () 425040352 + 8 [drvperf]
    &lt;idle&gt;-0     [000] ..s. 34630.349547: block_rq_complete: 8,0 WS () 425040352 + 8 [0]
...</pre>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="_monitoring_io_command_line_utilities">Monitoring IO: command line utilities</h4>
<div class="dlist">
<dl>
<dt class="hdlist1">iostat</dt>
<dd>
<p><code>iostat(1)</code> utility is part of <code>sysstat</code> package and provides user-friendly interface to <code>/proc/diskstats</code>.
It comes out handy for monitoring disk activity interactively.
If it is run as <code>iostat -m 1 /dev/sd*</code>, it will print disk statistics each second.
<code>tps</code> field is &#8220;transfers per second&#8221; and is counted as number of <code>reads_done</code> plus <code>writes_done</code> per second
and should be interpreted with care because of reasons described above (this is number of <em>physical</em> and not logical requests).</p>
<div class="listingblock">
<div class="content">
<pre>avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           1.02   41.84   11.22   45.92    0.00    0.00

Device:            tps    MB_read/s    MB_wrtn/s    MB_read    MB_wrtn
sda             395.92        49.49         0.00         48          0
sda1            395.92        49.49         0.00         48          0
sdb              10.20         0.18         0.03          0          0
sdb1             10.20         0.18         0.03          0          0
sdb2              0.00         0.00         0.00          0          0</pre>
</div>
</div>
</dd>
<dt class="hdlist1">iosnoop</dt>
<dd>
<p>iosnoop is simple command line utility written in bash.
It provides human readable interactive interface to ftrace.
Distributed as part of <code>perf-tools-unstable</code> package.
Requires superuser privileges.
Each line corresponds to single physical request.
May be a good choice if you want to use ftrace but don&#8217;t want to parse its output manually.</p>
<div class="listingblock">
<div class="content">
<pre># iosnoop
COMM             PID    TYPE DEV      BLOCK        BYTES     LATms
...
drvperf          15539  WS   8,0      425778664    4096       0.09
jbd2/sda5-168    168    WS   8,0      340339232    12288      0.10
&lt;idle&gt;           0      WS   8,0      340339256    4096       0.09
drvperf          15539  WS   8,0      425651976    4096       0.08
jbd2/sda5-168    168    WS   8,0      340339264    12288      0.07
...</pre>
</div>
</div>
</dd>
<dt class="hdlist1">iotop</dt>
<dd>
<p>iotop is a simple top-like monitoring tool written in python.
Requires superuser privileges.
Displays basic per-process statistics, which is updated each second.
May be used instead of parsing <code>/proc/&lt;pid&gt;/io</code> files manually.</p>
<div class="listingblock">
<div class="content">
<pre># iotop
Total DISK READ :       0.00 B/s | Total DISK WRITE :       2.09 M/s
Actual DISK READ:       0.00 B/s | Actual DISK WRITE:       8.76 M/s
  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND
  168 be/3 root        0.00 B/s   49.40 K/s  0.00 % 78.75 % [jbd2/sda5-8]
22552 be/4 root        0.00 B/s    2.04 M/s  0.00 %  4.29 % ./drvperf rndwrite extent
    1 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % init
...</pre>
</div>
</div>
</dd>
<dt class="hdlist1">atop/atopsar</dt>
<dd>
<p>May be considered as very extended version of <code>top</code> and as a &#8220;swiss knife&#8221; of system monitoring.
Requires superuser privileges to retrieve IO stats (otherwise it will start but won&#8217;t print IO stats).
Depending on what arguments are used, it may display different metrics.
If it is started as <code>atop -d 1</code>, it will display basic per-process disk activity, which is refreshed each second.
This mode may be used instead of parsing <code>/proc/&lt;pid&gt;/io</code> files manually.
It is also possible to get batch-style interval per-disk measurements from <code>/proc/diskstats</code> by running <code>atopsar</code>.
Output below is produced with <code>atopsar -d 1</code>:</p>
<div class="listingblock">
<div class="content">
<pre>...
01:19:34  disk           busy read/s KB/read  writ/s KB/writ avque avserv _dsk_
01:19:35  sda              0%    0.0     0.0     0.0     0.0   0.0   0.00 ms
          sdb              5%    3.0    37.3     0.0     0.0   2.7  17.33 ms
01:19:36
01:19:37
01:19:38  sdb              4%    0.0     0.0     3.0    24.0   1.0  14.67 ms
01:19:39  sda             15%   29.7   132.3     0.0     0.0   1.4   4.93 ms
01:19:40  sda            104%  390.6   128.7     0.0     0.0   1.9   2.66 ms
01:19:41  sda            103%  373.2   128.7     0.0     0.0   1.9   2.75 ms
01:19:42  sda            103%  382.5   128.7     0.0     0.0   1.9   2.70 ms
...</pre>
</div>
</div>
<div class="paragraph">
<p>The good thing about atop/atopsar is that they may be used to collect statistics into log files periodically, e.g. each 5 seconds.
These log files may be analyzing later, by moving &#8220;in time&#8221; one probe backwards or forwards.
See <a href="http://www.atoptool.nl/index.php">atop home page</a> for more info.</p>
</div>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_miscellaneous">Miscellaneous</h3>
<div class="sect3">
<h4 id="_distinguish_between_binary_and_decimal_units">Distinguish between binary and decimal units</h4>
<div class="paragraph">
<p>Coexistance of two different units&#8201;&#8212;&#8201;binary and decimal&#8201;&#8212;&#8201;is a constant source of confusion.
It may even create severe capacity misplanning problem when absolute values involved are high.
Chart below displays relative error if decimal unit was confused with binary one by mistake.
For example, if you have 1 TB drive (decimal), then its binary value is only 0.91 TiB&#8201;&#8212;&#8201;9% is &#8220;lost&#8221;.</p>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="tip-units.svg" alt="tip units" width="70%">
</div>
<div class="title">Discrepancy between binary and decimal units</div>
</div>
<div class="paragraph">
<p>Even if you can&#8217;t influence other people, you can at least stick with following notation yourself to avoid confusion.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Binary units</dt>
<dd>
<div class="paragraph">
<p>When referring to units in contexts when memory organization is important, use binary units, preferably with -bi- prefixes:
kibi-/mebi-/gibi- (KiB/MiB/GiB).
Typically, memory organization is important when dealing with low-level: memory addressing, buffer sizes, alignments and the like.
Examples:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>&#8220;buffer has size of 2 MiB&#8221; (binary)</p>
</li>
<li>
<p>&#8220;this field must be aligned by 16 KiB&#8221; (binary)</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Decimal units</dt>
<dd>
<div class="paragraph">
<p>On the other hand, when thinking it terms of high level, decimal units are better.
High level includes capacity and throughput of storages, file lengths and the like.
Decimal units are usually denoted by standard SI abbreviations: KB/MB/GB.
Examples:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>&#8220;this HDD has size 2 TB&#8221; (decimal)</p>
</li>
<li>
<p>&#8220;storage throughput is 5.5 GB/sec&#8221; (decimal)</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>In general, you want use decimals by default, and only when dealing with low-level things, you switch to binary units.
Good thing about decimal units is that they may be naturally used in arithmetical expressions
in the same way as all other metric units are used (length, weight, etc).
Compare: [1 GiB + 200 MiB = 1224 MiB] (hard) vs [1 GB + 200 MB = 1.2 GB] (easy).
Another pro-decimal argument is that marketers almost always use decimal units in order to make
an impression of capacities and speeds being better than they actually are.
Thus, by using decimals by default you won&#8217;t stumble into situation of bying not enough hardware.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_appendix_drvperf">Appendix: drvperf</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Performance measurements for this article were carried out by using custom benchmarking tool&#8201;&#8212;&#8201;<strong>drvperf</strong>.
It includes number of separate tests, some of which are common for testing storage subsystem
(namely, sequential and random access patterns), others are more specific and non-standard
(searching for optimal concurrency factor in SSD and statistically detecting mechanical characteristics of HDD).
Drvperf is written in C and works exclusively in Linux environment.
Drvperf requires root access to unlock full feature set,
but basic tests may be run by non-priveleged user by specifying large regular file as target block device.
Needless to say, results would be slightly shifted in this case due to the influence of filesystem layer.
Source code along with man page can be downloaded here: <a href="https://github.com/andreigudkov/drvperf">drvperf</a>.
Following paragraphs briefly describe methology of each test.</p>
</div>
<div class="paragraph">
<p>Sequential access performance is measured by pair of tests, <strong>seqread</strong> and <strong>seqwrite</strong>,
read and write respectively.
Idea is simple: split LBA address space into one hundred even chunks and make large sequential
IO operation inside each of them.
Final result is displayed in MB/s as triplet of values: minimal, maximal and average speed.
These three values are nearly identical when SSDs are tested, which proves their sequential
performance uniformity.
However, HDDs demonstrate min and max speeds differing by up to 50%,
with best speed at the beginning of address space and worst speed at the end of it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[Typical HDD]     Sequential read, each probe 134,217,728 bytes long:
                    min speed 22.21 MB/sec (at offset 159,907,659,264)
                    avg speed 37.96 MB/sec
                    max speed 51.33 MB/sec (at offset 11,431,329,280)

[Typical SSD]     Sequential read, each probe 134,217,728 bytes long:
                    min speed 363.13 MB/sec (at offset 116,591,763,456)
                    avg speed 395.21 MB/sec
                    max speed 420.78 MB/sec (at offset 137,765,863,424)</pre>
</div>
</div>
<div class="paragraph">
<p>Random access performance is measured by <strong>rndread</strong> and <strong>rndwrite</strong> tests.
Large number of small sized requests is made at randomly generated positions.
Requests may be issued in serial manner or concurrently, this depends on test settings.
Result is the throughput expressed in IOPS and also latency metrics:
average and percentiles (90%, 98%).
Random access is the field where SSDs outperform HDDs by orders of magnitude:
reads are faster, writes are faster, and also multiple reads and writes may be
executed simultaneously with large concurrency factor.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[Typical HDD]     Random read, each probe 512 bytes, alignment 512
                    Latency:
                      avg   17.958 ms, stddev 5.286 ms
                      p90   24.700 ms (90% of probes were faster)
                    Throughput:
                      overall             56 IOPS

[Typical SSD]     Random read, each probe 4096 bytes, alignment 4096
                    Latency:
                      avg  0.129 ms, stddev 0.047 ms
                      p90  0.157 ms (90% of probes were faster)
                    Throughput:
                      overall             7665 IOPS</pre>
</div>
</div>
<div class="paragraph">
<p>To simplify searching for optimal SSD concurrency factor, <strong>cncread</strong> and <strong>cncwrite</strong> tests were created.
Large RAIDs may also act as test species.
Idea of the test is to gradually increase number of parallel requests while keeping close eye
on actual throughput (IOPS) and latency.
By making concurrency factor higher, throughput increases but latency becomes poorer because of multiple
requests going into the same flash chip/LUN.
There is some point of saturation after which concurrency factor grows slowly but latency degrades very fast.
Concurrency test identifies this point by maximizing next formula, which is a weighted harmonic mean
with much higher weight given to latency:
<img src="math-46f3fae92944f151.svg" class="inlinemath" style="height:4.953ex;vertical-align:-2.219ex;" alt="\frac{1}{\frac{\alpha}{T} + \frac{1-\alpha}{L}} = \frac{{T}\cdot{L}}{{\alpha}{L} + (1-\alpha)T}"/>.
Here, <img src="math-e632b7095b0bf32c.svg" class="inlinemath" style="height:1.359ex;vertical-align:-0.0ex;" alt="T"/> and <img src="math-72dfcfb0c470ac25.svg" class="inlinemath" style="height:1.375ex;vertical-align:-0.0ex;" alt="L"/> are normalized throughput and reciprocal latency respectively, <img src="math-3caf51b954340e52.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.031ex;" alt="\alpha = 0.8"/>.
In the end, test prints optimal concurrency factor along with observed throughput speedup and latency.
The same concurrency factor is not suitable for all types of applications, i.e. low-latency applications
would impose tough restrictions on latency even at the expense of severely suboptimal throughput.
On the contrary, large database used mainly to upload data in offline may do not care about latency at all&#8201;&#8212;&#8201;all it needs is maximal possible throughput and volume per dollar.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Concurrency read test:
  optimal concurrency  13
Throughput (at --concurrent=13):
  total    61964 IOPS
  speedup  8.6</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Fullstroke</strong> is one of the two tests to detect mechanical characteristics of hard disk drives.
Its goal is to statistically find out fullstroke seek time and rotational speed (RPM).
Test makes a lot of long jumps between low and high addresses, which makes HDD to move actuator arm
between inner and outer cylinders respectively.
Any such single jump consists of fullstroke time plus some fraction of single rotation:
<img src="math-3316fc76915acd3f.svg" class="inlinemath" style="height:1.75ex;vertical-align:-0.391ex;" alt="T_{\mathrm{fullstroke}}+\alpha\,{T_{\mathrm{rotation}}},\;\alpha\in[0.0\,..\,1.0"/>].
Searching for minimum value among all possible jumps leads to fullstroke time alone (<img src="math-7551150ae37ab8e7.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.031ex;" alt="\alpha=0.0"/>),
while maximum value is the sum of fullstroke time plus full rotation time (<img src="math-3862bf034f323840.svg" class="inlinemath" style="height:1.391ex;vertical-align:-0.031ex;" alt="\alpha=1.0"/>).</p>
</div>
<div class="paragraph">
<p>While above algorithm seems easy at first glance, implementing it in practice is much more trickier.
Outliers pose a serious problem to automatical detection of true min and max values: intra-HDD caching makes
some of the probes to be completed unrealistically fast, without invlolving any mechanical operations at all.
Similarly, various negative factors delay execution of some probes for prolonged amount of time,
such as simultaneous system activity.
Finally, <img src="math-4893e9df8b5496eb.svg" class="inlinemath" style="height:0.922ex;vertical-align:-0.016ex;" alt="\alpha"/> doesn&#8217;t always have true uniform distribution between 0.0 and 1.0.
Drvperf employs statistical techniques to solve these problems: MAD (median absolute deviation) is used
to filter out outliers and bootstrapped Anderson-Darling test is used to detect evident errors.
Such combination proved to work well with all tested devices.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Fullstroke test results:
  fullstroke seek time  22.051 ms
       revolution time  10.666 ms
      rotational speed  5625 RPM</pre>
</div>
</div>
<div class="paragraph">
<p>Similar, <strong>tracktotrack</strong> test searches for track-to-track seek time.
The only difference is that fullstroke test jumps between lowest and highest addresses,
while tracktotrack test makes jumps as small as possible, barely enough to force track-to-track seeks.
Extra complexity arises from the fact that different HDDs have different intra-track density
and different ZBR configurations.
As such, test starts with running preliminary iterative test to determine largest cylinder size in bytes.
This is the minimal jump delta to be sure that track-to-track seek will take place.
Second half of the test is identical to fullstroke test: series of jumps is made,
fastest jump equals to track-to-track seek time, slowest jump equals to track-to-track seek time
plus one full rotation.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Tracktotrack test results (with --maxcylsize=524288 bytes):
  tracktotrack seek time: 1.500 ms
         revolution time: 11.179 ms
        rotational speed: 5367 RPM</pre>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2018-08-23 12:12:21 UTC
</div>
</div>
</body>
</html>